{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Tune LightGBM with FLAML Library\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "FLAML is a Python library (https://github.com/microsoft/FLAML) designed to automatically produce accurate machine learning models \n",
    "with low computational cost. It is fast and economical. The simple and lightweight design makes it easy \n",
    "to use and extend, such as adding new learners. FLAML can \n",
    "- serve as an economical AutoML engine,\n",
    "- be used as a fast hyperparameter tuning tool, or \n",
    "- be embedded in self-tuning software that requires low latency & resource in repetitive\n",
    "   tuning tasks.\n",
    "\n",
    "In this notebook, we demonstrate how to use FLAML library to tune hyperparameters of LightGBM with a regression example.\n",
    "\n",
    "FLAML requires `Python>=3.7`. To run this notebook example, please install flaml with the `notebook` option:\n",
    "```bash\n",
    "pip install flaml[notebook]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flaml[notebook]==1.0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Regression Example\n",
    "### Load data and preprocess\n",
    "\n",
    "Download [houses dataset](https://www.openml.org/d/537) from OpenML. The task is to predict median price of the house in the region based on demographic composition and a state of housing market in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from ./openml_ds537.pkl\n",
      "Dataset name: houses\n",
      "X_train.shape: (15480, 8), y_train.shape: (15480,);\n",
      "X_test.shape: (5160, 8), y_test.shape: (5160,)\n"
     ]
    }
   ],
   "source": [
    "from flaml.data import load_openml_dataset\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id=537, data_dir='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run FLAML\n",
    "In the FLAML automl run configuration, users can specify the task type, time budget, error metric, learner list, whether to subsample, resampling strategy type, and so on. All these arguments have default values which will be used if users do not provide them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' import AutoML class from flaml package '''\n",
    "from flaml import AutoML\n",
    "automl = AutoML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"time_budget\": 240,  # total running time in seconds\n",
    "    \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['lgbm'],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'regression',  # task type    \n",
    "    \"log_file_name\": 'houses_experiment.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 08-19 14:26:03] {2540} INFO - task = regression\n",
      "[flaml.automl: 08-19 14:26:03] {2542} INFO - Data split method: uniform\n",
      "[flaml.automl: 08-19 14:26:03] {2545} INFO - Evaluation method: cv\n",
      "[flaml.automl: 08-19 14:26:03] {2664} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl: 08-19 14:26:03] {2806} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl: 08-19 14:26:03] {3108} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:04] {3241} INFO - Estimated sufficient time budget=2673s. Estimated necessary time budget=3s.\n",
      "[flaml.automl: 08-19 14:26:04] {3288} INFO -  at 0.4s,\testimator lgbm's best error=0.7383,\tbest estimator lgbm's best error=0.7383\n",
      "[flaml.automl: 08-19 14:26:04] {3108} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:04] {3288} INFO -  at 0.6s,\testimator lgbm's best error=0.7383,\tbest estimator lgbm's best error=0.7383\n",
      "[flaml.automl: 08-19 14:26:04] {3108} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:04] {3288} INFO -  at 1.0s,\testimator lgbm's best error=0.3267,\tbest estimator lgbm's best error=0.3267\n",
      "[flaml.automl: 08-19 14:26:04] {3108} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:05] {3288} INFO -  at 1.9s,\testimator lgbm's best error=0.1882,\tbest estimator lgbm's best error=0.1882\n",
      "[flaml.automl: 08-19 14:26:05] {3108} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:05] {3288} INFO -  at 2.2s,\testimator lgbm's best error=0.1882,\tbest estimator lgbm's best error=0.1882\n",
      "[flaml.automl: 08-19 14:26:05] {3108} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:09] {3288} INFO -  at 5.9s,\testimator lgbm's best error=0.1882,\tbest estimator lgbm's best error=0.1882\n",
      "[flaml.automl: 08-19 14:26:09] {3108} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:09] {3288} INFO -  at 6.1s,\testimator lgbm's best error=0.1882,\tbest estimator lgbm's best error=0.1882\n",
      "[flaml.automl: 08-19 14:26:09] {3108} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:10] {3288} INFO -  at 6.7s,\testimator lgbm's best error=0.1882,\tbest estimator lgbm's best error=0.1882\n",
      "[flaml.automl: 08-19 14:26:10] {3108} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:11] {3288} INFO -  at 7.4s,\testimator lgbm's best error=0.1882,\tbest estimator lgbm's best error=0.1882\n",
      "[flaml.automl: 08-19 14:26:11] {3108} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:11] {3288} INFO -  at 7.6s,\testimator lgbm's best error=0.1882,\tbest estimator lgbm's best error=0.1882\n",
      "[flaml.automl: 08-19 14:26:11] {3108} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:14] {3288} INFO -  at 10.5s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:26:14] {3108} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:15] {3288} INFO -  at 11.3s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:26:15] {3108} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:31] {3288} INFO -  at 27.4s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:26:31] {3108} INFO - iteration 13, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:32] {3288} INFO -  at 28.7s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:26:32] {3108} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:42] {3288} INFO -  at 38.8s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:26:42] {3108} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:51] {3288} INFO -  at 48.0s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:26:51] {3108} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl: 08-19 14:26:52] {3288} INFO -  at 49.2s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:26:52] {3108} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:16] {3288} INFO -  at 72.4s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:27:16] {3108} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:16] {3288} INFO -  at 72.9s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:27:16] {3108} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:18] {3288} INFO -  at 74.4s,\testimator lgbm's best error=0.1744,\tbest estimator lgbm's best error=0.1744\n",
      "[flaml.automl: 08-19 14:27:18] {3108} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:30] {3288} INFO -  at 86.5s,\testimator lgbm's best error=0.1738,\tbest estimator lgbm's best error=0.1738\n",
      "[flaml.automl: 08-19 14:27:30] {3108} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:37] {3288} INFO -  at 93.7s,\testimator lgbm's best error=0.1738,\tbest estimator lgbm's best error=0.1738\n",
      "[flaml.automl: 08-19 14:27:37] {3108} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:40] {3288} INFO -  at 96.9s,\testimator lgbm's best error=0.1738,\tbest estimator lgbm's best error=0.1738\n",
      "[flaml.automl: 08-19 14:27:40] {3108} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:57] {3288} INFO -  at 113.7s,\testimator lgbm's best error=0.1738,\tbest estimator lgbm's best error=0.1738\n",
      "[flaml.automl: 08-19 14:27:57] {3108} INFO - iteration 24, current learner lgbm\n",
      "[flaml.automl: 08-19 14:27:58] {3288} INFO -  at 114.7s,\testimator lgbm's best error=0.1738,\tbest estimator lgbm's best error=0.1738\n",
      "[flaml.automl: 08-19 14:27:58] {3108} INFO - iteration 25, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:00] {3288} INFO -  at 116.7s,\testimator lgbm's best error=0.1738,\tbest estimator lgbm's best error=0.1738\n",
      "[flaml.automl: 08-19 14:28:00] {3108} INFO - iteration 26, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:03] {3288} INFO -  at 119.6s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:03] {3108} INFO - iteration 27, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:07] {3288} INFO -  at 123.3s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:07] {3108} INFO - iteration 28, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:09] {3288} INFO -  at 125.5s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:09] {3108} INFO - iteration 29, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:11] {3288} INFO -  at 127.8s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:11] {3108} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:16] {3288} INFO -  at 133.2s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:16] {3108} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:24] {3288} INFO -  at 141.1s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:24] {3108} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:25] {3288} INFO -  at 142.2s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:25] {3108} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:43] {3288} INFO -  at 160.2s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:43] {3108} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:44] {3288} INFO -  at 160.9s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:44] {3108} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:49] {3288} INFO -  at 165.5s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:49] {3108} INFO - iteration 36, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:50] {3288} INFO -  at 167.0s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:50] {3108} INFO - iteration 37, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:53] {3288} INFO -  at 169.7s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:53] {3108} INFO - iteration 38, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:57] {3288} INFO -  at 173.3s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:57] {3108} INFO - iteration 39, current learner lgbm\n",
      "[flaml.automl: 08-19 14:28:58] {3288} INFO -  at 175.0s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:28:58] {3108} INFO - iteration 40, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:04] {3288} INFO -  at 180.8s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:29:04] {3108} INFO - iteration 41, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:06] {3288} INFO -  at 182.8s,\testimator lgbm's best error=0.1608,\tbest estimator lgbm's best error=0.1608\n",
      "[flaml.automl: 08-19 14:29:06] {3108} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:10] {3288} INFO -  at 186.8s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:10] {3108} INFO - iteration 43, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:11] {3288} INFO -  at 187.6s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:11] {3108} INFO - iteration 44, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:22] {3288} INFO -  at 198.6s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:22] {3108} INFO - iteration 45, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:24] {3288} INFO -  at 201.2s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:24] {3108} INFO - iteration 46, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:29] {3288} INFO -  at 205.7s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:29] {3108} INFO - iteration 47, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:30] {3288} INFO -  at 207.0s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:30] {3108} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:39] {3288} INFO -  at 215.6s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:39] {3108} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:40] {3288} INFO -  at 216.8s,\testimator lgbm's best error=0.1605,\tbest estimator lgbm's best error=0.1605\n",
      "[flaml.automl: 08-19 14:29:40] {3108} INFO - iteration 50, current learner lgbm\n",
      "[flaml.automl: 08-19 14:29:53] {3288} INFO -  at 230.1s,\testimator lgbm's best error=0.1564,\tbest estimator lgbm's best error=0.1564\n",
      "[flaml.automl: 08-19 14:29:56] {3552} INFO - retrain lgbm for 2.8s\n",
      "[flaml.automl: 08-19 14:29:56] {3559} INFO - retrained model: LGBMRegressor(colsample_bytree=0.6884091116362046,\n",
      "              learning_rate=0.0825101833775657, max_bin=1023,\n",
      "              min_child_samples=15, n_estimators=436, num_leaves=46,\n",
      "              reg_alpha=0.0010949400705571237, reg_lambda=0.004934208563558304,\n",
      "              verbose=-1)\n",
      "[flaml.automl: 08-19 14:29:56] {2837} INFO - fit succeeded\n",
      "[flaml.automl: 08-19 14:29:56] {2838} INFO - Time taken to find the best model: 230.08108830451965\n",
      "[flaml.automl: 08-19 14:29:56] {2849} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    }
   ],
   "source": [
    "'''The main flaml automl API'''\n",
    "automl.fit(X_train=X_train, y_train=y_train, **settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best model and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparmeter config: {'n_estimators': 436, 'num_leaves': 46, 'min_child_samples': 15, 'learning_rate': 0.0825101833775657, 'log_max_bin': 10, 'colsample_bytree': 0.6884091116362046, 'reg_alpha': 0.0010949400705571237, 'reg_lambda': 0.004934208563558304}\n",
      "Best r2 on validation data: 0.8436\n",
      "Training duration of best run: 2.842 s\n"
     ]
    }
   ],
   "source": [
    "''' retrieve best config'''\n",
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.6884091116362046,\n",
       "              learning_rate=0.0825101833775657, max_bin=1023,\n",
       "              min_child_samples=15, n_estimators=436, num_leaves=46,\n",
       "              reg_alpha=0.0010949400705571237, reg_lambda=0.004934208563558304,\n",
       "              verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.6884091116362046,\n",
       "              learning_rate=0.0825101833775657, max_bin=1023,\n",
       "              min_child_samples=15, n_estimators=436, num_leaves=46,\n",
       "              reg_alpha=0.0010949400705571237, reg_lambda=0.004934208563558304,\n",
       "              verbose=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.6884091116362046,\n",
       "              learning_rate=0.0825101833775657, max_bin=1023,\n",
       "              min_child_samples=15, n_estimators=436, num_leaves=46,\n",
       "              reg_alpha=0.0010949400705571237, reg_lambda=0.004934208563558304,\n",
       "              verbose=-1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 8 artists>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.barh(automl.feature_names_in_, automl.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "''' pickle and save the automl object '''\n",
    "import pickle\n",
    "with open('automl.pkl', 'wb') as f:\n",
    "    pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels [154950.89603668 261662.95147298 147753.80526837 ... 223816.69491088\n",
      " 204988.34248538 279060.41682988]\n",
      "True labels 14740    136900.0\n",
      "10101    241300.0\n",
      "20566    200700.0\n",
      "2670      72500.0\n",
      "15709    460000.0\n",
      "           ...   \n",
      "13132    121200.0\n",
      "8228     137500.0\n",
      "3948     160900.0\n",
      "8522     227300.0\n",
      "16798    265600.0\n",
      "Name: median_house_value, Length: 5160, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "''' compute predictions of testing dataset ''' \n",
    "y_pred = automl.predict(X_test)\n",
    "print('Predicted labels', y_pred)\n",
    "print('True labels', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.8504619441564182\n",
      "mse = 1976669769.3583572\n",
      "mae = 29365.09842559213\n"
     ]
    }
   ],
   "source": [
    "''' compute different metric values on testing dataset'''\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))\n",
    "print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))\n",
    "print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Current Learner': 'lgbm', 'Current Sample': 15480, 'Current Hyper-parameters': {'n_estimators': 4, 'num_leaves': 4, 'min_child_samples': 20, 'learning_rate': 0.09999999999999995, 'log_max_bin': 8, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.0}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 4, 'num_leaves': 4, 'min_child_samples': 20, 'learning_rate': 0.09999999999999995, 'log_max_bin': 8, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.0}}\n",
      "{'Current Learner': 'lgbm', 'Current Sample': 15480, 'Current Hyper-parameters': {'n_estimators': 22, 'num_leaves': 4, 'min_child_samples': 18, 'learning_rate': 0.2293009676418639, 'log_max_bin': 9, 'colsample_bytree': 0.9086551727646448, 'reg_alpha': 0.0015561782752413472, 'reg_lambda': 0.33127416269768944}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 22, 'num_leaves': 4, 'min_child_samples': 18, 'learning_rate': 0.2293009676418639, 'log_max_bin': 9, 'colsample_bytree': 0.9086551727646448, 'reg_alpha': 0.0015561782752413472, 'reg_lambda': 0.33127416269768944}}\n",
      "{'Current Learner': 'lgbm', 'Current Sample': 15480, 'Current Hyper-parameters': {'n_estimators': 28, 'num_leaves': 20, 'min_child_samples': 17, 'learning_rate': 0.32352862101602586, 'log_max_bin': 10, 'colsample_bytree': 0.8801327898366843, 'reg_alpha': 0.004475520554844502, 'reg_lambda': 0.033081571878574946}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 28, 'num_leaves': 20, 'min_child_samples': 17, 'learning_rate': 0.32352862101602586, 'log_max_bin': 10, 'colsample_bytree': 0.8801327898366843, 'reg_alpha': 0.004475520554844502, 'reg_lambda': 0.033081571878574946}}\n",
      "{'Current Learner': 'lgbm', 'Current Sample': 15480, 'Current Hyper-parameters': {'n_estimators': 44, 'num_leaves': 81, 'min_child_samples': 29, 'learning_rate': 0.26477481203117526, 'log_max_bin': 10, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.028486834222229064}, 'Best Learner': 'lgbm', 'Best Hyper-parameters': {'n_estimators': 44, 'num_leaves': 81, 'min_child_samples': 29, 'learning_rate': 0.26477481203117526, 'log_max_bin': 10, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.028486834222229064}}\n"
     ]
    }
   ],
   "source": [
    "from flaml.data import get_output_from_log\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename=settings['log_file_name'], time_budget=60)\n",
    "\n",
    "for config in config_history:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation r2')\n",
    "plt.scatter(time_history, 1 - np.array(valid_loss_history))\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with alternatives\n",
    "\n",
    "### FLAML's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flaml (4min) r2 = 0.8504619441564182\n"
     ]
    }
   ],
   "source": [
    "print('flaml (4min) r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "lgbm = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default lgbm r2 = 0.8296179648694404\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgbm.predict(X_test)\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('default lgbm r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna LightGBM Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna==2.8.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (2.8.0)\n",
      "Requirement already satisfied: alembic in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (1.8.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (1.4.40)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (1.23.2)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (0.8.2)\n",
      "Requirement already satisfied: colorlog in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (6.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (21.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (4.64.0)\n",
      "Requirement already satisfied: cliff in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (4.0.0)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from optuna==2.8.0) (1.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from packaging>=20.0->optuna==2.8.0) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna==2.8.0) (1.1.2)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from alembic->optuna==2.8.0) (5.9.0)\n",
      "Requirement already satisfied: Mako in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from alembic->optuna==2.8.0) (1.2.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from alembic->optuna==2.8.0) (4.12.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cliff->optuna==2.8.0) (3.3.0)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cliff->optuna==2.8.0) (2.4.2)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cliff->optuna==2.8.0) (6.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cliff->optuna==2.8.0) (0.5.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cliff->optuna==2.8.0) (4.0.0)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna==2.8.0) (22.1.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna==2.8.0) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna==2.8.0) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from importlib-metadata->alembic->optuna==2.8.0) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from stevedore>=2.0.1->cliff->optuna==2.8.0) (5.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/ec2-user/miniconda3/envs/flaml/lib/python3.8/site-packages (from Mako->alembic->optuna==2.8.0) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, val_x, train_y, val_y = train_test_split(X_train, y_train, test_size=0.1)\n",
    "import optuna.integration.lightgbm as lgb\n",
    "dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "dval = lgb.Dataset(val_x, label=val_y)\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"regression\",\n",
    "    \"verbosity\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-19 14:30:01,711]\u001b[0m A new study created in memory with name: no-name-8fbdb818-9837-4f94-8f57-c393fbe4a3af\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: feature_pre_filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-19 14:30:04,871]\u001b[0m Trial 0 finished with value: 2147291179.0395293 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 2147291179.0395293.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:07,408]\u001b[0m Trial 1 finished with value: 2027103695.9770539 and parameters: {'feature_fraction': 0.8}. Best is trial 1 with value: 2027103695.9770539.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:09,854]\u001b[0m Trial 2 finished with value: 2125123735.6499698 and parameters: {'feature_fraction': 0.6}. Best is trial 1 with value: 2027103695.9770539.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:12,380]\u001b[0m Trial 3 finished with value: 2027103695.9770539 and parameters: {'feature_fraction': 0.7}. Best is trial 1 with value: 2027103695.9770539.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:14,887]\u001b[0m Trial 4 finished with value: 2221252362.078979 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 2027103695.9770539.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:17,552]\u001b[0m Trial 5 finished with value: 2113023040.7493408 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 2027103695.9770539.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:19,726]\u001b[0m Trial 6 finished with value: 2129729447.7232711 and parameters: {'feature_fraction': 0.4}. Best is trial 1 with value: 2027103695.9770539.\u001b[0m\n",
      "feature_fraction, val_score: 2027103695.977054: 100%|##########| 7/7 [00:18<00:00,  2.57s/it]\n",
      "\u001b[32m[I 2022-08-19 14:30:26,884]\u001b[0m Trial 7 finished with value: 2153102051.571428 and parameters: {'num_leaves': 117}. Best is trial 7 with value: 2153102051.571428.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:28,102]\u001b[0m Trial 8 finished with value: 2122208920.607172 and parameters: {'num_leaves': 8}. Best is trial 8 with value: 2122208920.607172.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:31,374]\u001b[0m Trial 9 finished with value: 2107492681.7831585 and parameters: {'num_leaves': 46}. Best is trial 9 with value: 2107492681.7831585.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:34,591]\u001b[0m Trial 10 finished with value: 2067234636.6746926 and parameters: {'num_leaves': 41}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:42,706]\u001b[0m Trial 11 finished with value: 2157723341.991555 and parameters: {'num_leaves': 137}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:48,812]\u001b[0m Trial 12 finished with value: 2144153512.8728046 and parameters: {'num_leaves': 99}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:30:56,943]\u001b[0m Trial 13 finished with value: 2171424776.230104 and parameters: {'num_leaves': 131}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:03,487]\u001b[0m Trial 14 finished with value: 2094075525.1291854 and parameters: {'num_leaves': 85}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:10,285]\u001b[0m Trial 15 finished with value: 2128002934.2535832 and parameters: {'num_leaves': 113}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:15,355]\u001b[0m Trial 16 finished with value: 2095047632.8808584 and parameters: {'num_leaves': 79}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:27,953]\u001b[0m Trial 17 finished with value: 2195951756.433248 and parameters: {'num_leaves': 217}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:29,353]\u001b[0m Trial 18 finished with value: 2112841591.9662855 and parameters: {'num_leaves': 11}. Best is trial 10 with value: 2067234636.6746926.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:32,540]\u001b[0m Trial 19 finished with value: 2054387824.9535184 and parameters: {'num_leaves': 40}. Best is trial 19 with value: 2054387824.9535184.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:35,905]\u001b[0m Trial 20 finished with value: 2112828433.244751 and parameters: {'num_leaves': 47}. Best is trial 19 with value: 2054387824.9535184.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:47,473]\u001b[0m Trial 21 finished with value: 2201708147.602069 and parameters: {'num_leaves': 197}. Best is trial 19 with value: 2054387824.9535184.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:31:51,015]\u001b[0m Trial 22 finished with value: 2107492681.7831585 and parameters: {'num_leaves': 46}. Best is trial 19 with value: 2054387824.9535184.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:00,809]\u001b[0m Trial 23 finished with value: 2202402054.990555 and parameters: {'num_leaves': 166}. Best is trial 19 with value: 2054387824.9535184.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:02,856]\u001b[0m Trial 24 finished with value: 2023373826.6930401 and parameters: {'num_leaves': 22}. Best is trial 24 with value: 2023373826.6930401.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:04,485]\u001b[0m Trial 25 finished with value: 2094906957.590643 and parameters: {'num_leaves': 14}. Best is trial 24 with value: 2023373826.6930401.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:08,928]\u001b[0m Trial 26 finished with value: 2094584175.3524451 and parameters: {'num_leaves': 69}. Best is trial 24 with value: 2023373826.6930401.\u001b[0m\n",
      "num_leaves, val_score: 2023373826.693040: 100%|##########| 20/20 [01:49<00:00,  5.46s/it]\n",
      "\u001b[32m[I 2022-08-19 14:32:11,869]\u001b[0m Trial 27 finished with value: 2062465082.5876758 and parameters: {'bagging_fraction': 0.8976779191246269, 'bagging_freq': 1}. Best is trial 27 with value: 2062465082.5876758.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:14,581]\u001b[0m Trial 28 finished with value: 2024570073.8714666 and parameters: {'bagging_fraction': 0.7102750973814858, 'bagging_freq': 1}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:16,990]\u001b[0m Trial 29 finished with value: 2063551295.3287122 and parameters: {'bagging_fraction': 0.9567296421940273, 'bagging_freq': 7}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:19,896]\u001b[0m Trial 30 finished with value: 2062360224.5246172 and parameters: {'bagging_fraction': 0.9279811226569815, 'bagging_freq': 1}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:22,453]\u001b[0m Trial 31 finished with value: 2116145745.7056575 and parameters: {'bagging_fraction': 0.7432759374876607, 'bagging_freq': 7}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:25,034]\u001b[0m Trial 32 finished with value: 2040270267.2437565 and parameters: {'bagging_fraction': 0.9807690972204939, 'bagging_freq': 1}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:28,086]\u001b[0m Trial 33 finished with value: 2114099028.7517028 and parameters: {'bagging_fraction': 0.5764668687539389, 'bagging_freq': 1}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:30,599]\u001b[0m Trial 34 finished with value: 2093869660.0814357 and parameters: {'bagging_fraction': 0.8256408911097234, 'bagging_freq': 4}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:33,419]\u001b[0m Trial 35 finished with value: 2096225764.4084606 and parameters: {'bagging_fraction': 0.7014324834651005, 'bagging_freq': 6}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:36,134]\u001b[0m Trial 36 finished with value: 2138003596.945032 and parameters: {'bagging_fraction': 0.5583833201893088, 'bagging_freq': 1}. Best is trial 28 with value: 2024570073.8714666.\u001b[0m\n",
      "bagging, val_score: 2023373826.693040: 100%|##########| 10/10 [00:27<00:00,  2.72s/it]\n",
      "\u001b[32m[I 2022-08-19 14:32:38,140]\u001b[0m Trial 37 finished with value: 2023373826.6930401 and parameters: {'feature_fraction': 0.784}. Best is trial 37 with value: 2023373826.6930401.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:40,304]\u001b[0m Trial 38 finished with value: 2083536584.4856184 and parameters: {'feature_fraction': 0.88}. Best is trial 37 with value: 2023373826.6930401.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:42,328]\u001b[0m Trial 39 finished with value: 2023373826.6930401 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 37 with value: 2023373826.6930401.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:44,712]\u001b[0m Trial 40 finished with value: 2083536584.4856184 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 37 with value: 2023373826.6930401.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:46,816]\u001b[0m Trial 41 finished with value: 2083536584.4856184 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 37 with value: 2023373826.6930401.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:48,816]\u001b[0m Trial 42 finished with value: 2023373826.6930401 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 2023373826.6930401.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 2023373826.693040: 100%|##########| 6/6 [00:12<00:00,  2.11s/it]\n",
      "\u001b[32m[I 2022-08-19 14:32:51,122]\u001b[0m Trial 43 finished with value: 2087832009.9671495 and parameters: {'lambda_l1': 0.0017867049293892648, 'lambda_l2': 0.32011646374496444}. Best is trial 43 with value: 2087832009.9671495.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:53,141]\u001b[0m Trial 44 finished with value: 2044807356.45883 and parameters: {'lambda_l1': 6.860653712080662e-07, 'lambda_l2': 0.0016056009907102376}. Best is trial 44 with value: 2044807356.45883.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:55,148]\u001b[0m Trial 45 finished with value: 2023373817.2811556 and parameters: {'lambda_l1': 7.651469441589495e-07, 'lambda_l2': 3.7311099083828933e-06}. Best is trial 45 with value: 2023373817.2811556.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:57,647]\u001b[0m Trial 46 finished with value: 2023373823.594674 and parameters: {'lambda_l1': 1.0074416455744479e-07, 'lambda_l2': 1.079984795578841e-06}. Best is trial 45 with value: 2023373817.2811556.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:32:59,660]\u001b[0m Trial 47 finished with value: 2103655024.7329082 and parameters: {'lambda_l1': 1.688741753972323e-06, 'lambda_l2': 0.03367314166459796}. Best is trial 45 with value: 2023373817.2811556.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:01,719]\u001b[0m Trial 48 finished with value: 2023373823.3175213 and parameters: {'lambda_l1': 0.0005137351285796685, 'lambda_l2': 1.1187239378777308e-06}. Best is trial 45 with value: 2023373817.2811556.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:03,742]\u001b[0m Trial 49 finished with value: 2012825784.4799635 and parameters: {'lambda_l1': 1.7459219041300027, 'lambda_l2': 0.0004574117691883087}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:05,947]\u001b[0m Trial 50 finished with value: 2038532846.6222024 and parameters: {'lambda_l1': 2.0348207668205354e-06, 'lambda_l2': 0.5293555562360815}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:07,973]\u001b[0m Trial 51 finished with value: 2023373826.413511 and parameters: {'lambda_l1': 0.00020763533400612973, 'lambda_l2': 2.566151760122251e-08}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:10,005]\u001b[0m Trial 52 finished with value: 2023373823.33296 and parameters: {'lambda_l1': 6.745890495777255e-07, 'lambda_l2': 1.1991970123223683e-06}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:12,279]\u001b[0m Trial 53 finished with value: 2023372294.7715 and parameters: {'lambda_l1': 3.593697172378387, 'lambda_l2': 0.0002591304319388369}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:14,346]\u001b[0m Trial 54 finished with value: 2023371544.5906973 and parameters: {'lambda_l1': 7.4113049281674215, 'lambda_l2': 0.00018294403132597484}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:16,330]\u001b[0m Trial 55 finished with value: 2023371842.3254004 and parameters: {'lambda_l1': 5.490566006139139, 'lambda_l2': 0.00025341764436301605}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:18,576]\u001b[0m Trial 56 finished with value: 2071097519.12333 and parameters: {'lambda_l1': 0.14014972912947152, 'lambda_l2': 0.005915365751493022}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:20,614]\u001b[0m Trial 57 finished with value: 2023373741.6397815 and parameters: {'lambda_l1': 0.14465994205029584, 'lambda_l2': 1.961596202474825e-05}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:22,740]\u001b[0m Trial 58 finished with value: 2081961023.8220637 and parameters: {'lambda_l1': 0.2516652387896391, 'lambda_l2': 8.514455836746762}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:24,996]\u001b[0m Trial 59 finished with value: 2023371395.535413 and parameters: {'lambda_l1': 9.442157905673197, 'lambda_l2': 4.218635546011494e-05}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:26,977]\u001b[0m Trial 60 finished with value: 2023373718.794164 and parameters: {'lambda_l1': 0.007946156620961086, 'lambda_l2': 4.226531347735691e-05}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:29,014]\u001b[0m Trial 61 finished with value: 2023373700.9890735 and parameters: {'lambda_l1': 0.506601942498153, 'lambda_l2': 4.518709657991422e-08}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:31,321]\u001b[0m Trial 62 finished with value: 2053983580.5720456 and parameters: {'lambda_l1': 0.023605851475522598, 'lambda_l2': 0.0026100050609328034}. Best is trial 49 with value: 2012825784.4799635.\u001b[0m\n",
      "regularization_factors, val_score: 2012825784.479964: 100%|##########| 20/20 [00:42<00:00,  2.12s/it]\n",
      "\u001b[32m[I 2022-08-19 14:33:33,870]\u001b[0m Trial 63 finished with value: 2063537073.7149749 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 2063537073.7149749.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:36,243]\u001b[0m Trial 64 finished with value: 2052314263.0161862 and parameters: {'min_child_samples': 50}. Best is trial 64 with value: 2052314263.0161862.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:38,339]\u001b[0m Trial 65 finished with value: 2074078016.4231691 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 2052314263.0161862.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:40,305]\u001b[0m Trial 66 finished with value: 2055138857.2610078 and parameters: {'min_child_samples': 5}. Best is trial 64 with value: 2052314263.0161862.\u001b[0m\n",
      "\u001b[32m[I 2022-08-19 14:33:42,603]\u001b[0m Trial 67 finished with value: 2062122570.7408898 and parameters: {'min_child_samples': 10}. Best is trial 64 with value: 2052314263.0161862.\u001b[0m\n",
      "min_data_in_leaf, val_score: 2012825784.479964: 100%|##########| 5/5 [00:11<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 39s, sys: 9.03 s, total: 6min 48s\n",
      "Wall time: 3min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], verbose_eval=10000)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna LightGBM Tuner r2 = 0.8444598010690063\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('Optuna LightGBM Tuner r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add a customized LightGBM learner in FLAML\n",
    "The native API of LightGBM allows one to specify a custom objective function in the model constructor. You can easily enable it by adding a customized LightGBM learner in FLAML. In the following example, we show how to add such a customized LightGBM learner with a custom objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a customized LightGBM learner with a custom objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "''' define your customized objective function '''\n",
    "def my_loss_obj(y_true, y_pred):\n",
    "    c = 0.5\n",
    "    residual = y_pred - y_true\n",
    "    grad = c * residual /(np.abs(residual) + c)\n",
    "    hess = c ** 2 / (np.abs(residual) + c) ** 2\n",
    "    # rmse grad and hess\n",
    "    grad_rmse = residual\n",
    "    hess_rmse = 1.0\n",
    "    \n",
    "    # mae grad and hess\n",
    "    grad_mae = np.array(residual)\n",
    "    grad_mae[grad_mae > 0] = 1.\n",
    "    grad_mae[grad_mae <= 0] = -1.\n",
    "    hess_mae = 1.0\n",
    "\n",
    "    coef = [0.4, 0.3, 0.3]\n",
    "    return coef[0] * grad + coef[1] * grad_rmse + coef[2] * grad_mae, \\\n",
    "        coef[0] * hess + coef[1] * hess_rmse + coef[2] * hess_mae\n",
    "\n",
    "\n",
    "from flaml.model import LGBMEstimator\n",
    "\n",
    "''' create a customized LightGBM learner class with your objective function '''\n",
    "class MyLGBM(LGBMEstimator):\n",
    "    '''LGBMEstimator with my_loss_obj as the objective function\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **config):\n",
    "        super().__init__(objective=my_loss_obj, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the customized learner in FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 08-19 14:33:43] {2540} INFO - task = regression\n",
      "[flaml.automl: 08-19 14:33:43] {2542} INFO - Data split method: uniform\n",
      "[flaml.automl: 08-19 14:33:43] {2545} INFO - Evaluation method: cv\n",
      "[flaml.automl: 08-19 14:33:43] {2664} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl: 08-19 14:33:43] {2806} INFO - List of ML learners in AutoML Run: ['my_lgbm']\n",
      "[flaml.automl: 08-19 14:33:43] {3108} INFO - iteration 0, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:43] {3241} INFO - Estimated sufficient time budget=3513s. Estimated necessary time budget=4s.\n",
      "[flaml.automl: 08-19 14:33:43] {3288} INFO -  at 0.4s,\testimator my_lgbm's best error=2.9883,\tbest estimator my_lgbm's best error=2.9883\n",
      "[flaml.automl: 08-19 14:33:43] {3108} INFO - iteration 1, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:43] {3288} INFO -  at 0.6s,\testimator my_lgbm's best error=2.9883,\tbest estimator my_lgbm's best error=2.9883\n",
      "[flaml.automl: 08-19 14:33:43] {3108} INFO - iteration 2, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:43] {3288} INFO -  at 0.8s,\testimator my_lgbm's best error=1.7086,\tbest estimator my_lgbm's best error=1.7086\n",
      "[flaml.automl: 08-19 14:33:43] {3108} INFO - iteration 3, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:44] {3288} INFO -  at 1.0s,\testimator my_lgbm's best error=0.3574,\tbest estimator my_lgbm's best error=0.3574\n",
      "[flaml.automl: 08-19 14:33:44] {3108} INFO - iteration 4, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:44] {3288} INFO -  at 1.2s,\testimator my_lgbm's best error=0.3574,\tbest estimator my_lgbm's best error=0.3574\n",
      "[flaml.automl: 08-19 14:33:44] {3108} INFO - iteration 5, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:44] {3288} INFO -  at 1.5s,\testimator my_lgbm's best error=0.2977,\tbest estimator my_lgbm's best error=0.2977\n",
      "[flaml.automl: 08-19 14:33:44] {3108} INFO - iteration 6, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:44] {3288} INFO -  at 1.7s,\testimator my_lgbm's best error=0.2977,\tbest estimator my_lgbm's best error=0.2977\n",
      "[flaml.automl: 08-19 14:33:44] {3108} INFO - iteration 7, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:45] {3288} INFO -  at 1.9s,\testimator my_lgbm's best error=0.2977,\tbest estimator my_lgbm's best error=0.2977\n",
      "[flaml.automl: 08-19 14:33:45] {3108} INFO - iteration 8, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:45] {3288} INFO -  at 2.7s,\testimator my_lgbm's best error=0.2754,\tbest estimator my_lgbm's best error=0.2754\n",
      "[flaml.automl: 08-19 14:33:45] {3108} INFO - iteration 9, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:46] {3288} INFO -  at 3.0s,\testimator my_lgbm's best error=0.2754,\tbest estimator my_lgbm's best error=0.2754\n",
      "[flaml.automl: 08-19 14:33:46] {3108} INFO - iteration 10, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:48] {3288} INFO -  at 4.8s,\testimator my_lgbm's best error=0.1838,\tbest estimator my_lgbm's best error=0.1838\n",
      "[flaml.automl: 08-19 14:33:48] {3108} INFO - iteration 11, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:50] {3288} INFO -  at 7.6s,\testimator my_lgbm's best error=0.1838,\tbest estimator my_lgbm's best error=0.1838\n",
      "[flaml.automl: 08-19 14:33:50] {3108} INFO - iteration 12, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:52] {3288} INFO -  at 9.0s,\testimator my_lgbm's best error=0.1838,\tbest estimator my_lgbm's best error=0.1838\n",
      "[flaml.automl: 08-19 14:33:52] {3108} INFO - iteration 13, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:54] {3288} INFO -  at 11.3s,\testimator my_lgbm's best error=0.1838,\tbest estimator my_lgbm's best error=0.1838\n",
      "[flaml.automl: 08-19 14:33:54] {3108} INFO - iteration 14, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:56] {3288} INFO -  at 13.2s,\testimator my_lgbm's best error=0.1838,\tbest estimator my_lgbm's best error=0.1838\n",
      "[flaml.automl: 08-19 14:33:56] {3108} INFO - iteration 15, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:33:59] {3288} INFO -  at 15.9s,\testimator my_lgbm's best error=0.1793,\tbest estimator my_lgbm's best error=0.1793\n",
      "[flaml.automl: 08-19 14:33:59] {3108} INFO - iteration 16, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:34:01] {3288} INFO -  at 17.8s,\testimator my_lgbm's best error=0.1793,\tbest estimator my_lgbm's best error=0.1793\n",
      "[flaml.automl: 08-19 14:34:01] {3108} INFO - iteration 17, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:34:11] {3288} INFO -  at 28.7s,\testimator my_lgbm's best error=0.1763,\tbest estimator my_lgbm's best error=0.1763\n",
      "[flaml.automl: 08-19 14:34:11] {3108} INFO - iteration 18, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:34:14] {3288} INFO -  at 31.6s,\testimator my_lgbm's best error=0.1763,\tbest estimator my_lgbm's best error=0.1763\n",
      "[flaml.automl: 08-19 14:34:14] {3108} INFO - iteration 19, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:34:18] {3288} INFO -  at 34.9s,\testimator my_lgbm's best error=0.1763,\tbest estimator my_lgbm's best error=0.1763\n",
      "[flaml.automl: 08-19 14:34:18] {3108} INFO - iteration 20, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:34:40] {3288} INFO -  at 57.2s,\testimator my_lgbm's best error=0.1763,\tbest estimator my_lgbm's best error=0.1763\n",
      "[flaml.automl: 08-19 14:34:40] {3108} INFO - iteration 21, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:34:51] {3288} INFO -  at 67.9s,\testimator my_lgbm's best error=0.1763,\tbest estimator my_lgbm's best error=0.1763\n",
      "[flaml.automl: 08-19 14:34:51] {3108} INFO - iteration 22, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:34:57] {3288} INFO -  at 74.2s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:34:57] {3108} INFO - iteration 23, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:35:02] {3288} INFO -  at 79.4s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:35:02] {3108} INFO - iteration 24, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:35:12] {3288} INFO -  at 89.4s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:35:12] {3108} INFO - iteration 25, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:35:39] {3288} INFO -  at 116.7s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:35:39] {3108} INFO - iteration 26, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:35:41] {3288} INFO -  at 118.7s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:35:41] {3108} INFO - iteration 27, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:35:57] {3288} INFO -  at 133.9s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:35:57] {3108} INFO - iteration 28, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:36:00] {3288} INFO -  at 137.1s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:36:00] {3108} INFO - iteration 29, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:36:01] {3288} INFO -  at 138.4s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:36:01] {3108} INFO - iteration 30, current learner my_lgbm\n",
      "[flaml.automl: 08-19 14:36:13] {3288} INFO -  at 150.4s,\testimator my_lgbm's best error=0.1686,\tbest estimator my_lgbm's best error=0.1686\n",
      "[flaml.automl: 08-19 14:36:15] {3552} INFO - retrain my_lgbm for 1.4s\n",
      "[flaml.automl: 08-19 14:36:15] {3559} INFO - retrained model: LGBMRegressor(colsample_bytree=0.783676729687123,\n",
      "              learning_rate=0.49086790129318314, max_bin=511,\n",
      "              min_child_samples=5, n_estimators=40, num_leaves=472,\n",
      "              objective=<function my_loss_obj at 0x7fa2189a49d0>,\n",
      "              reg_alpha=0.0009765625, reg_lambda=0.003042181059805583,\n",
      "              verbose=-1)\n",
      "[flaml.automl: 08-19 14:36:15] {2837} INFO - fit succeeded\n",
      "[flaml.automl: 08-19 14:36:15] {2838} INFO - Time taken to find the best model: 74.16670298576355\n"
     ]
    }
   ],
   "source": [
    "automl = AutoML()\n",
    "automl.add_learner(learner_name='my_lgbm', learner_class=MyLGBM)\n",
    "settings = {\n",
    "    \"time_budget\": 150,  # total running time in seconds\n",
    "    \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "    \"estimator_list\": ['my_lgbm',],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'regression',  # task type    \n",
    "    \"log_file_name\": 'houses_experiment_my_lgbm.log',  # flaml log file\n",
    "}\n",
    "automl.fit(X_train=X_train, y_train=y_train, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparmeter config: {'n_estimators': 40, 'num_leaves': 472, 'min_child_samples': 5, 'learning_rate': 0.49086790129318314, 'log_max_bin': 9, 'colsample_bytree': 0.783676729687123, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003042181059805583}\n",
      "Best r2 on validation data: 0.8314\n",
      "Training duration of best run: 1.436 s\n",
      "Predicted labels [132791.8974753  261783.72528027 157936.02135606 ... 231511.42570721\n",
      " 220549.58812948 263519.27074828]\n",
      "True labels 14740    136900.0\n",
      "10101    241300.0\n",
      "20566    200700.0\n",
      "2670      72500.0\n",
      "15709    460000.0\n",
      "           ...   \n",
      "13132    121200.0\n",
      "8228     137500.0\n",
      "3948     160900.0\n",
      "8522     227300.0\n",
      "16798    265600.0\n",
      "Name: median_house_value, Length: 5160, dtype: float64\n",
      "r2 = 0.8362103987498559\n",
      "mse = 2165053915.5402246\n",
      "mae = 30417.01739126795\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
    "\n",
    "y_pred = automl.predict(X_test)\n",
    "print('Predicted labels', y_pred)\n",
    "print('True labels', y_test)\n",
    "\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))\n",
    "print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))\n",
    "print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize learners via custom_hp in FLAML\n",
    "\n",
    "You can provide a 'custom_hp' as a shortcut to override the original search spaces of the built-in learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 08-19 14:36:15] {2540} INFO - task = regression\n",
      "[flaml.automl: 08-19 14:36:15] {2542} INFO - Data split method: uniform\n",
      "[flaml.automl: 08-19 14:36:15] {2545} INFO - Evaluation method: cv\n",
      "[flaml.automl: 08-19 14:36:15] {2664} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl: 08-19 14:36:15] {2806} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl: 08-19 14:36:15] {3108} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:15] {3241} INFO - Estimated sufficient time budget=1919s. Estimated necessary time budget=16s.\n",
      "[flaml.automl: 08-19 14:36:15] {3288} INFO -  at 0.3s,\testimator lgbm's best error=0.7383,\tbest estimator lgbm's best error=0.7383\n",
      "[flaml.automl: 08-19 14:36:15] {3108} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:15] {3288} INFO -  at 0.4s,\testimator lgbm's best error=0.7383,\tbest estimator lgbm's best error=0.7383\n",
      "[flaml.automl: 08-19 14:36:15] {3108} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:16] {3288} INFO -  at 0.8s,\testimator lgbm's best error=0.5506,\tbest estimator lgbm's best error=0.5506\n",
      "[flaml.automl: 08-19 14:36:16] {3108} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:16] {3288} INFO -  at 1.0s,\testimator xgboost's best error=1.8235,\tbest estimator lgbm's best error=0.5506\n",
      "[flaml.automl: 08-19 14:36:16] {3108} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:16] {3288} INFO -  at 1.2s,\testimator lgbm's best error=0.3916,\tbest estimator lgbm's best error=0.3916\n",
      "[flaml.automl: 08-19 14:36:16] {3108} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:16] {3288} INFO -  at 1.3s,\testimator lgbm's best error=0.3916,\tbest estimator lgbm's best error=0.3916\n",
      "[flaml.automl: 08-19 14:36:16] {3108} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:16] {3288} INFO -  at 1.5s,\testimator lgbm's best error=0.3916,\tbest estimator lgbm's best error=0.3916\n",
      "[flaml.automl: 08-19 14:36:16] {3108} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:17] {3288} INFO -  at 1.7s,\testimator lgbm's best error=0.3031,\tbest estimator lgbm's best error=0.3031\n",
      "[flaml.automl: 08-19 14:36:17] {3108} INFO - iteration 8, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:17] {3288} INFO -  at 1.8s,\testimator extra_tree's best error=0.6071,\tbest estimator lgbm's best error=0.3031\n",
      "[flaml.automl: 08-19 14:36:17] {3108} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:17] {3288} INFO -  at 2.1s,\testimator xgboost's best error=1.8235,\tbest estimator lgbm's best error=0.3031\n",
      "[flaml.automl: 08-19 14:36:17] {3108} INFO - iteration 10, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:17] {3288} INFO -  at 2.3s,\testimator extra_tree's best error=0.4393,\tbest estimator lgbm's best error=0.3031\n",
      "[flaml.automl: 08-19 14:36:17] {3108} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:18] {3288} INFO -  at 2.8s,\testimator xgboost's best error=0.6769,\tbest estimator lgbm's best error=0.3031\n",
      "[flaml.automl: 08-19 14:36:18] {3108} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:18] {3288} INFO -  at 2.9s,\testimator extra_tree's best error=0.4393,\tbest estimator lgbm's best error=0.3031\n",
      "[flaml.automl: 08-19 14:36:18] {3108} INFO - iteration 13, current learner rf\n",
      "[flaml.automl: 08-19 14:36:19] {3288} INFO -  at 4.2s,\testimator rf's best error=0.2488,\tbest estimator rf's best error=0.2488\n",
      "[flaml.automl: 08-19 14:36:19] {3108} INFO - iteration 14, current learner rf\n",
      "[flaml.automl: 08-19 14:36:20] {3288} INFO -  at 5.2s,\testimator rf's best error=0.2424,\tbest estimator rf's best error=0.2424\n",
      "[flaml.automl: 08-19 14:36:20] {3108} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:20] {3288} INFO -  at 5.5s,\testimator xgboost's best error=0.3620,\tbest estimator rf's best error=0.2424\n",
      "[flaml.automl: 08-19 14:36:20] {3108} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:21] {3288} INFO -  at 6.0s,\testimator lgbm's best error=0.2631,\tbest estimator rf's best error=0.2424\n",
      "[flaml.automl: 08-19 14:36:21] {3108} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:21] {3288} INFO -  at 6.3s,\testimator extra_tree's best error=0.3922,\tbest estimator rf's best error=0.2424\n",
      "[flaml.automl: 08-19 14:36:21] {3108} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:21] {3288} INFO -  at 6.5s,\testimator lgbm's best error=0.2631,\tbest estimator rf's best error=0.2424\n",
      "[flaml.automl: 08-19 14:36:21] {3108} INFO - iteration 19, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:22] {3288} INFO -  at 6.8s,\testimator xgboost's best error=0.3620,\tbest estimator rf's best error=0.2424\n",
      "[flaml.automl: 08-19 14:36:22] {3108} INFO - iteration 20, current learner rf\n",
      "[flaml.automl: 08-19 14:36:24] {3288} INFO -  at 8.9s,\testimator rf's best error=0.2205,\tbest estimator rf's best error=0.2205\n",
      "[flaml.automl: 08-19 14:36:24] {3108} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:24] {3288} INFO -  at 9.1s,\testimator xgboost's best error=0.3620,\tbest estimator rf's best error=0.2205\n",
      "[flaml.automl: 08-19 14:36:24] {3108} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:24] {3288} INFO -  at 9.4s,\testimator lgbm's best error=0.2185,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:24] {3108} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:24] {3288} INFO -  at 9.6s,\testimator lgbm's best error=0.2185,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:24] {3108} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:25] {3288} INFO -  at 10.0s,\testimator xgboost's best error=0.3364,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:25] {3108} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:25] {3288} INFO -  at 10.3s,\testimator extra_tree's best error=0.3230,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:25] {3108} INFO - iteration 26, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:25] {3288} INFO -  at 10.5s,\testimator lgbm's best error=0.2185,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:25] {3108} INFO - iteration 27, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:26] {3288} INFO -  at 10.8s,\testimator xgboost's best error=0.3364,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:26] {3108} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:26] {3288} INFO -  at 11.1s,\testimator extra_tree's best error=0.3230,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:26] {3108} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:26] {3288} INFO -  at 11.4s,\testimator extra_tree's best error=0.2858,\tbest estimator lgbm's best error=0.2185\n",
      "[flaml.automl: 08-19 14:36:26] {3108} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:27] {3288} INFO -  at 12.1s,\testimator lgbm's best error=0.1966,\tbest estimator lgbm's best error=0.1966\n",
      "[flaml.automl: 08-19 14:36:27] {3108} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:28] {3288} INFO -  at 12.8s,\testimator lgbm's best error=0.1966,\tbest estimator lgbm's best error=0.1966\n",
      "[flaml.automl: 08-19 14:36:28] {3108} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:29] {3288} INFO -  at 13.8s,\testimator lgbm's best error=0.1892,\tbest estimator lgbm's best error=0.1892\n",
      "[flaml.automl: 08-19 14:36:29] {3108} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:29] {3288} INFO -  at 14.4s,\testimator extra_tree's best error=0.2570,\tbest estimator lgbm's best error=0.1892\n",
      "[flaml.automl: 08-19 14:36:29] {3108} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:30] {3288} INFO -  at 15.0s,\testimator lgbm's best error=0.1892,\tbest estimator lgbm's best error=0.1892\n",
      "[flaml.automl: 08-19 14:36:30] {3108} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:32] {3288} INFO -  at 17.1s,\testimator lgbm's best error=0.1762,\tbest estimator lgbm's best error=0.1762\n",
      "[flaml.automl: 08-19 14:36:32] {3108} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:32] {3288} INFO -  at 17.5s,\testimator extra_tree's best error=0.2570,\tbest estimator lgbm's best error=0.1762\n",
      "[flaml.automl: 08-19 14:36:32] {3108} INFO - iteration 37, current learner rf\n",
      "[flaml.automl: 08-19 14:36:35] {3288} INFO -  at 20.6s,\testimator rf's best error=0.2043,\tbest estimator lgbm's best error=0.1762\n",
      "[flaml.automl: 08-19 14:36:35] {3108} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:36] {3288} INFO -  at 20.9s,\testimator xgboost's best error=0.3364,\tbest estimator lgbm's best error=0.1762\n",
      "[flaml.automl: 08-19 14:36:36] {3108} INFO - iteration 39, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:37] {3288} INFO -  at 21.9s,\testimator lgbm's best error=0.1762,\tbest estimator lgbm's best error=0.1762\n",
      "[flaml.automl: 08-19 14:36:37] {3108} INFO - iteration 40, current learner xgboost\n",
      "[flaml.automl: 08-19 14:36:37] {3288} INFO -  at 22.3s,\testimator xgboost's best error=0.2738,\tbest estimator lgbm's best error=0.1762\n",
      "[flaml.automl: 08-19 14:36:37] {3108} INFO - iteration 41, current learner extra_tree\n",
      "[flaml.automl: 08-19 14:36:38] {3288} INFO -  at 23.4s,\testimator extra_tree's best error=0.2570,\tbest estimator lgbm's best error=0.1762\n",
      "[flaml.automl: 08-19 14:36:38] {3108} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl: 08-19 14:36:43] {3288} INFO -  at 28.5s,\testimator lgbm's best error=0.1648,\tbest estimator lgbm's best error=0.1648\n",
      "[flaml.automl: 08-19 14:36:43] {3108} INFO - iteration 43, current learner catboost\n",
      "[flaml.automl: 08-19 14:37:12] {3288} INFO -  at 57.1s,\testimator catboost's best error=0.1768,\tbest estimator lgbm's best error=0.1648\n",
      "[flaml.automl: 08-19 14:37:12] {3108} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl: 08-19 14:37:16] {3288} INFO -  at 60.9s,\testimator xgboost's best error=0.2738,\tbest estimator lgbm's best error=0.1648\n",
      "[flaml.automl: 08-19 14:37:27] {3552} INFO - retrain lgbm for 11.2s\n",
      "[flaml.automl: 08-19 14:37:27] {3559} INFO - retrained model: LGBMRegressor(colsample_bytree=0.4280572638757817,\n",
      "              learning_rate=0.12229148765139466, max_bin=255,\n",
      "              min_child_samples=45, n_estimators=378, num_leaves=30,\n",
      "              reg_alpha=0.0009765625, reg_lambda=18.717310934365923,\n",
      "              subsample=0.8687573632569908, subsample_freq=1, verbose=-1)\n",
      "[flaml.automl: 08-19 14:37:27] {2837} INFO - fit succeeded\n",
      "[flaml.automl: 08-19 14:37:27] {2838} INFO - Time taken to find the best model: 28.469687938690186\n"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "automl = AutoML()\n",
    "custom_hp = {\n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": {\n",
    "            \"domain\": tune.lograndint(lower=4, upper=100),\n",
    "            \"low_cost_init_value\": 4,\n",
    "        },\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"max_leaves\": {\n",
    "            \"domain\": None,  # disable search\n",
    "        },\n",
    "    },\n",
    "    \"lgbm\": {\n",
    "        \"subsample\": {\n",
    "            \"domain\": tune.uniform(lower=0.1, upper=1.0),\n",
    "            \"init_value\": 1.0,\n",
    "        },\n",
    "        \"subsample_freq\": {\n",
    "            \"domain\": 1,  # subsample_freq must > 0 to enable subsample\n",
    "        },\n",
    "    },\n",
    "}\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # total running time in seconds\n",
    "    \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "    \"custom_hp\": custom_hp,\n",
    "    \"task\": 'regression',  # task type    \n",
    "    \"log_file_name\": 'houses_experiment_my_lgbm.log',  # flaml log file\n",
    "}\n",
    "automl.fit(X_train=X_train, y_train=y_train, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparmeter config: {'n_estimators': 378, 'num_leaves': 30, 'min_child_samples': 45, 'learning_rate': 0.12229148765139466, 'log_max_bin': 8, 'colsample_bytree': 0.4280572638757817, 'reg_alpha': 0.0009765625, 'reg_lambda': 18.717310934365923, 'subsample': 0.8687573632569908, 'subsample_freq': 1}\n",
      "Best r2 on validation data: 0.8352\n",
      "Training duration of best run: 11.24 s\n",
      "Predicted labels [149787.03152099 262888.75538702 154935.80727812 ... 214050.57916561\n",
      " 226633.52713256 261029.05148998]\n",
      "True labels 14740    136900.0\n",
      "10101    241300.0\n",
      "20566    200700.0\n",
      "2670      72500.0\n",
      "15709    460000.0\n",
      "           ...   \n",
      "13132    121200.0\n",
      "8228     137500.0\n",
      "3948     160900.0\n",
      "8522     227300.0\n",
      "16798    265600.0\n",
      "Name: median_house_value, Length: 5160, dtype: float64\n",
      "r2 = 0.8412520803443934\n",
      "mse = 2098410414.4031174\n",
      "mae = 31073.08166515394\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
    "\n",
    "y_pred = automl.predict(X_test)\n",
    "print('Predicted labels', y_pred)\n",
    "print('True labels', y_test)\n",
    "\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))\n",
    "print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))\n",
    "print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
