{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook includes toy examples to demonstrate how to tune User Defined Functions with `flaml.tune`.\n",
    "\n",
    "FLAML requires `Python>=3.7`. To run this notebook example, please install flaml with the `notebook` option:\n",
    "```bash\n",
    "pip install flaml[notebook]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flaml[notebook]\n",
    "# from v0.6.6, catboost is made an optional dependency to build conda package.\n",
    "# to install catboost without installing the notebook option, you can run:\n",
    "# %pip install flaml[catboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tuning procedure\n",
    "## 1. A basic tuning example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set a search space'''\n",
    "from flaml import tune\n",
    "config_search_space = {\n",
    "    \"x\": tune.lograndint(lower=1, upper=100000),\n",
    "    \"y\": tune.randint(lower=1, upper=100000)\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a evaluation function'''\n",
    "import time\n",
    "def evaluate_config(config: dict):\n",
    "    \"\"\"evaluate a hyperparameter configuration\"\"\"\n",
    "    score = (config[\"x\"] - 85000) ** 2 - config[\"x\"] / config[\"y\"]\n",
    "    # usually the evaluation takes an non-neglible cost\n",
    "    # and the cost could be related to certain hyperparameters\n",
    "    # here we simulate this cost by calling the time.sleep() function\n",
    "    # here we assume the cost is proportional to x\n",
    "    faked_evaluation_cost = config[\"x\"] / 100000\n",
    "    time.sleep(faked_evaluation_cost)\n",
    "    # we can return a single float as a score on the input config:\n",
    "    # return score\n",
    "    # or, we can return a dictionary that maps metric name to metric value:\n",
    "    return {\"score\": score, \"evaluation_cost\": faked_evaluation_cost, \"constraint_metric\": config[\"x\"] * config[\"y\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:50:56,441]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 1 config: {'x': 3, 'y': 13184}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 2 config: {'x': 6134, 'y': 2076}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 3 config: {'x': 1143, 'y': 74880}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 4 config: {'x': 5539, 'y': 1}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 5 config: {'x': 6793, 'y': 16190}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 6 config: {'x': 220, 'y': 22480}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 7 config: {'x': 6, 'y': 76053}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 8 config: {'x': 4, 'y': 8834}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 9 config: {'x': 2148, 'y': 95339}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 10 config: {'x': 1, 'y': 51219}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 11 config: {'x': 10155, 'y': 61252}\n",
      "[flaml.tune.tune: 08-15 01:50:56] {506} INFO - trial 12 config: {'x': 51672, 'y': 61799}\n",
      "[flaml.tune.tune: 08-15 01:50:57] {506} INFO - trial 13 config: {'x': 18407, 'y': 72736}\n",
      "[flaml.tune.tune: 08-15 01:50:57] {506} INFO - trial 14 config: {'x': 99999, 'y': 50862}\n",
      "[flaml.tune.tune: 08-15 01:50:58] {506} INFO - trial 15 config: {'x': 2, 'y': 372}\n",
      "[flaml.tune.tune: 08-15 01:50:58] {506} INFO - trial 16 config: {'x': 99999, 'y': 39100}\n",
      "[flaml.tune.tune: 08-15 01:50:59] {506} INFO - trial 17 config: {'x': 40494, 'y': 50862}\n",
      "[flaml.tune.tune: 08-15 01:50:59] {506} INFO - trial 18 config: {'x': 60615, 'y': 25643}\n",
      "[flaml.tune.tune: 08-15 01:51:00] {506} INFO - trial 19 config: {'x': 99999, 'y': 52557}\n",
      "[flaml.tune.tune: 08-15 01:51:01] {506} INFO - trial 20 config: {'x': 3350, 'y': 29188}\n",
      "[flaml.tune.tune: 08-15 01:51:01] {506} INFO - trial 21 config: {'x': 6, 'y': 25996}\n",
      "[flaml.tune.tune: 08-15 01:51:01] {506} INFO - trial 22 config: {'x': 36654, 'y': 71457}\n",
      "[flaml.tune.tune: 08-15 01:51:01] {506} INFO - trial 23 config: {'x': 376, 'y': 14217}\n",
      "[flaml.tune.tune: 08-15 01:51:01] {506} INFO - trial 24 config: {'x': 99999, 'y': 64368}\n",
      "[flaml.tune.tune: 08-15 01:51:02] {506} INFO - trial 25 config: {'x': 51439, 'y': 97709}\n",
      "[flaml.tune.tune: 08-15 01:51:03] {506} INFO - trial 26 config: {'x': 24442, 'y': 71457}\n",
      "[flaml.tune.tune: 08-15 01:51:03] {506} INFO - trial 27 config: {'x': 60949, 'y': 50896}\n",
      "[flaml.tune.tune: 08-15 01:51:04] {506} INFO - trial 28 config: {'x': 73238, 'y': 99999}\n",
      "[flaml.tune.tune: 08-15 01:51:05] {506} INFO - trial 29 config: {'x': 51439, 'y': 86194}\n",
      "[flaml.tune.tune: 08-15 01:51:05] {506} INFO - trial 30 config: {'x': 99999, 'y': 77840}\n"
     ]
    }
   ],
   "source": [
    "''''Performs tuning'''\n",
    "# require: pip install flaml[blendsearch]\n",
    "analysis = tune.run(\n",
    "    evaluate_config,  # the function to evaluate a config\n",
    "    config=config_search_space,  # the search space defined\n",
    "    metric=\"score\",\n",
    "    mode=\"min\",  # the optimization mode, \"min\" or \"max\"\n",
    "    num_samples=-1,  # the maximal number of configs to try, -1 means infinite\n",
    "    time_budget_s=10,  # the time budget in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 138344643.26761267, 'evaluation_cost': 0.73238, 'constraint_metric': 7323726762, 'training_iteration': 0, 'config': {'x': 73238, 'y': 99999}, 'config/x': 73238, 'config/y': 99999, 'experiment_tag': 'exp', 'time_total_s': 0.7344884872436523}\n"
     ]
    }
   ],
   "source": [
    "'''Investigate results'''\n",
    "print(analysis.best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(analysis.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical search space \n",
    "Hierarchical search space is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set a hierarchical search space'''\n",
    "from flaml import tune\n",
    "gbtree_hp_space = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"n_estimators\": tune.lograndint(lower=4, upper=64),\n",
    "        \"max_leaves\": tune.lograndint(lower=4, upper=64),\n",
    "        \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),\n",
    "    }\n",
    "gblinear_hp_space = {\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"lambda\": tune.uniform(0, 1),\n",
    "    \"alpha\": tune.loguniform(0.0001, 1),\n",
    "}\n",
    "\n",
    "full_space = {\n",
    "    \"xgb_config\": tune.choice([gbtree_hp_space, gblinear_hp_space]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a evaluation function'''\n",
    "import xgboost as xgb\n",
    "def xgb_obj(X_train, X_test, y_train, y_test, config):\n",
    "    config = config[\"xgb_config\"]\n",
    "    params = config2params(config)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    booster_type = config.get(\"booster\")\n",
    "\n",
    "    if booster_type == \"gblinear\":\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "        )\n",
    "    else:\n",
    "        _n_estimators = params.pop(\"n_estimators\")\n",
    "        model = xgb.train(params, dtrain, _n_estimators)\n",
    "\n",
    "    # get validation loss\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_test_predict = model.predict(dtest)\n",
    "    test_loss = 1.0 - r2_score(y_test, y_test_predict)\n",
    "    return {\"loss\": test_loss}\n",
    "\n",
    "def config2params(config: dict) -> dict:\n",
    "    params = config.copy()\n",
    "    max_depth = params[\"max_depth\"] = params.get(\"max_depth\", 0)\n",
    "    if max_depth == 0:\n",
    "        params[\"grow_policy\"] = params.get(\"grow_policy\", \"lossguide\")\n",
    "        params[\"tree_method\"] = params.get(\"tree_method\", \"hist\")\n",
    "    # params[\"booster\"] = params.get(\"booster\", \"gbtree\")\n",
    "    params[\"use_label_encoder\"] = params.get(\"use_label_encoder\", False)\n",
    "    if \"n_jobs\" in config:\n",
    "        params[\"nthread\"] = params.pop(\"n_jobs\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-15 01:51:08,271]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 08-15 01:51:08] {506} INFO - trial 1 config: {'xgb_config': {'booster': 'gblinear', 'lambda': 0.6472660813321921, 'alpha': 0.0028264214081400044}}\n",
      "[flaml.tune.tune: 08-15 01:51:08] {506} INFO - trial 2 config: {'xgb_config': {'n_estimators': 22, 'max_leaves': 31, 'learning_rate': 0.0309282737630552, 'booster': 'gbtree'}}\n",
      "[flaml.tune.tune: 08-15 01:51:08] {506} INFO - trial 3 config: {'xgb_config': {'n_estimators': 32, 'max_leaves': 6, 'learning_rate': 0.0018014797394283806, 'booster': 'gbtree'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from ./openml_ds537.pkl\n",
      "Dataset name: houses\n",
      "X_train.shape: (15480, 8), y_train.shape: (15480,);\n",
      "X_test.shape: (5160, 8), y_test.shape: (5160,)\n",
      "[01:51:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { grow_policy, max_depth, tree_method, use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:08] {506} INFO - trial 4 config: {'xgb_config': {'lambda': 0.003948266327914451, 'alpha': 0.011188427539040417, 'booster': 'gblinear'}}\n",
      "[flaml.tune.tune: 08-15 01:51:08] {506} INFO - trial 5 config: {'xgb_config': {'n_estimators': 28, 'max_leaves': 8, 'learning_rate': 0.5655557791092936, 'booster': 'gbtree'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[01:51:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { grow_policy, max_depth, tree_method, use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "analysis {'bb341b44': {'loss': 0.573919882488896, 'training_iteration': 0, 'config': {'xgb_config': {'booster': 'gblinear', 'lambda': 0.6472660813321921, 'alpha': 0.0028264214081400044}}, 'config/xgb_config': {'booster': 'gblinear', 'lambda': 0.6472660813321921, 'alpha': 0.0028264214081400044}, 'experiment_tag': 'exp', 'time_total_s': 0.025920391082763672}, 'bb387e14': {'loss': 1.291006987963745, 'training_iteration': 0, 'config': {'xgb_config': {'n_estimators': 22, 'max_leaves': 31, 'learning_rate': 0.0309282737630552, 'booster': 'gbtree'}}, 'config/xgb_config': {'n_estimators': 22, 'max_leaves': 31, 'learning_rate': 0.0309282737630552, 'booster': 'gbtree'}, 'experiment_tag': 'exp', 'time_total_s': 0.15230369567871094}, 'bb5000d4': {'loss': 3.781676744261694, 'training_iteration': 0, 'config': {'xgb_config': {'n_estimators': 32, 'max_leaves': 6, 'learning_rate': 0.0018014797394283806, 'booster': 'gbtree'}}, 'config/xgb_config': {'n_estimators': 32, 'max_leaves': 6, 'learning_rate': 0.0018014797394283806, 'booster': 'gbtree'}, 'experiment_tag': 'exp', 'time_total_s': 0.0894002914428711}, 'bb5de690': {'loss': 0.5613721871648404, 'training_iteration': 0, 'config': {'xgb_config': {'lambda': 0.003948266327914451, 'alpha': 0.011188427539040417, 'booster': 'gblinear'}}, 'config/xgb_config': {'lambda': 0.003948266327914451, 'alpha': 0.011188427539040417, 'booster': 'gblinear'}, 'experiment_tag': 'exp', 'time_total_s': 0.02099776268005371}, 'bb6164d2': {'loss': 0.21745848280234414, 'training_iteration': 0, 'config': {'xgb_config': {'n_estimators': 28, 'max_leaves': 8, 'learning_rate': 0.5655557791092936, 'booster': 'gbtree'}}, 'config/xgb_config': {'n_estimators': 28, 'max_leaves': 8, 'learning_rate': 0.5655557791092936, 'booster': 'gbtree'}, 'experiment_tag': 'exp', 'time_total_s': 0.08208870887756348}}\n"
     ]
    }
   ],
   "source": [
    "'''Tune xgb_obj with configs from the hierarchical search space'''\n",
    "from flaml.data import load_openml_dataset\n",
    "from functools import partial\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(\n",
    "    dataset_id=537, data_dir=\"./\"\n",
    ")\n",
    "analysis = tune.run(\n",
    "    partial(xgb_obj, X_train, X_test, y_train, y_test),\n",
    "    config=full_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=5,\n",
    ")\n",
    "print(\"analysis\", analysis.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tuning Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constraints on the tuning\n",
    "\n",
    "1. A user can specify constraints on the configurations to be satisfied via the argument `config_constraints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:51:09,993]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 08-15 01:51:10] {506} INFO - trial 1 config: {'width': 1, 'height': 132, 'length': 647}\n",
      "[flaml.tune.tune: 08-15 01:51:10] {506} INFO - trial 2 config: {'width': 2, 'height': 760, 'length': 169}\n",
      "[flaml.tune.tune: 08-15 01:51:10] {506} INFO - trial 3 config: {'width': 1, 'height': 685, 'length': 953}\n",
      "[flaml.tune.tune: 08-15 01:51:10] {506} INFO - trial 4 config: {'width': 1, 'height': 512, 'length': 812}\n",
      "[flaml.tune.tune: 08-15 01:51:10] {506} INFO - trial 5 config: {'width': 1, 'height': 373, 'length': 674}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'volume': 85404, 'training_iteration': 0, 'config': {'width': 1, 'height': 132, 'length': 647}, 'config/width': 1, 'config/height': 132, 'config/length': 647, 'experiment_tag': 'exp', 'time_total_s': 0.006061077117919922}\n"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "def area(config):\n",
    "    return config[\"width\"] * config[\"length\"]\n",
    "\n",
    "cube_search_space = {\n",
    "    \"width\": tune.lograndint(lower=1, upper=1000),\n",
    "    \"height\": tune.randint(lower=1, upper=1000),\n",
    "    \"length\": tune.randint(lower=1, upper=1000),\n",
    "}\n",
    "\n",
    "def cube_volume(config: dict):\n",
    "    \"\"\"evaluate a hyperparameter configuration\"\"\"\n",
    "    score = config[\"width\"] * config[\"height\"] * config[\"length\"]\n",
    "    return {\"volume\": score}\n",
    "\n",
    "analysis = tune.run(evaluation_function=cube_volume,\n",
    "         mode=\"min\",\n",
    "         metric=\"volume\",\n",
    "         config=cube_search_space,\n",
    "         config_constraints=[(area, \"<=\", 1000)],\n",
    "         num_samples=5,\n",
    "        )\n",
    "print(analysis.best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  You can also specify a list of metric constraints to be satisfied via the argument `metric_constraints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:51:11,386]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 08-15 01:51:11] {506} INFO - trial 1 config: {'x': 3, 'y': 13184}\n",
      "[flaml.tune.tune: 08-15 01:51:11] {506} INFO - trial 2 config: {'x': 6134, 'y': 2076}\n",
      "[flaml.tune.tune: 08-15 01:51:11] {506} INFO - trial 3 config: {'x': 1143, 'y': 74880}\n",
      "[flaml.tune.tune: 08-15 01:51:11] {506} INFO - trial 4 config: {'x': 5539, 'y': 1}\n",
      "[flaml.tune.tune: 08-15 01:51:11] {506} INFO - trial 5 config: {'x': 6793, 'y': 16190}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flaml.tune.tune.ExperimentAnalysis at 0x7f066a190a00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flaml import tune\n",
    "tune.run(evaluation_function=evaluate_config,\n",
    "         mode=\"min\",\n",
    "         metric=\"score\",\n",
    "         config=config_search_space,\n",
    "         metric_constraints=[(\"evaluation_cost\", \"<=\", 0.1)],\n",
    "         num_samples=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config_constraints vs metric_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:51:12,028]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 08-15 01:51:12] {506} INFO - trial 1 config: {'n_estimators': 39, 'max_leaves': 9, 'learning_rate': 0.08672915197219133}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from ./openml_ds537.pkl\n",
      "Dataset name: houses\n",
      "X_train.shape: (15480, 8), y_train.shape: (15480,);\n",
      "X_test.shape: (5160, 8), y_test.shape: (5160,)\n",
      "[01:51:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:12] {506} INFO - trial 2 config: {'n_estimators': 7, 'max_leaves': 5, 'learning_rate': 0.6111947006764871}\n",
      "Received additional result for trial bda97d2e, but it already finished. Result: {'loss': 0.3552178044511305, 'training_cost': 0.029860258102416992, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 5, 'learning_rate': 0.6111947006764871}, 'config/n_estimators': 7, 'config/max_leaves': 5, 'config/learning_rate': 0.6111947006764871, 'experiment_tag': 'exp', 'time_total_s': 0.037056922912597656, 'loss_lagrange': 0.3552178044511305}\n",
      "Received additional completion for trial bda97d2e, but it already finished. Result: {'loss': 0.3552178044511305, 'training_cost': 0.029860258102416992, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 5, 'learning_rate': 0.6111947006764871}, 'config/n_estimators': 7, 'config/max_leaves': 5, 'config/learning_rate': 0.6111947006764871, 'experiment_tag': 'exp', 'time_total_s': 0.037056922912597656, 'loss_lagrange': 0.3552178044511305}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:13] {506} INFO - trial 3 config: {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}\n",
      "Received additional result for trial be41014e, but it already finished. Result: {'loss': 0.8238569381974518, 'training_cost': 0.028520584106445312, 'training_iteration': 0, 'config': {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}, 'config/n_estimators': 6, 'config/max_leaves': 5, 'config/learning_rate': 0.18074443349590638, 'experiment_tag': 'exp', 'time_total_s': 0.03594350814819336, 'loss_lagrange': 0.8238569381974518}\n",
      "Received additional completion for trial be41014e, but it already finished. Result: {'loss': 0.8238569381974518, 'training_cost': 0.028520584106445312, 'training_iteration': 0, 'config': {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}, 'config/n_estimators': 6, 'config/max_leaves': 5, 'config/learning_rate': 0.18074443349590638, 'experiment_tag': 'exp', 'time_total_s': 0.03594350814819336, 'loss_lagrange': 0.8238569381974518}\n",
      "[flaml.tune.tune: 08-15 01:51:13] {506} INFO - trial 4 config: {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.0050708287994836255}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:13] {506} INFO - trial 5 config: {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}\n",
      "Received additional result for trial be74d082, but it already finished. Result: {'loss': 0.8420746064984069, 'training_cost': 0.02660083770751953, 'training_iteration': 0, 'config': {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}, 'config/n_estimators': 4, 'config/max_leaves': 8, 'config/learning_rate': 0.24184523333348865, 'experiment_tag': 'exp', 'time_total_s': 0.033454179763793945, 'loss_lagrange': 0.8420746064984069}\n",
      "Received additional completion for trial be74d082, but it already finished. Result: {'loss': 0.8420746064984069, 'training_cost': 0.02660083770751953, 'training_iteration': 0, 'config': {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}, 'config/n_estimators': 4, 'config/max_leaves': 8, 'config/learning_rate': 0.24184523333348865, 'experiment_tag': 'exp', 'time_total_s': 0.033454179763793945, 'loss_lagrange': 0.8420746064984069}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 6 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.6006787986201269}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 7 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5793237833265791}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 8 config: {'n_estimators': 8, 'max_leaves': 5, 'learning_rate': 0.6139350165706452}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 9 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5823407285827188}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 10 config: {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.6153425666105765}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 11 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.0058671903833274665}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 12 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005472055643063294}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 13 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005482932727232923}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 14 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.00451003340688015}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 15 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.004559264656132888}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 16 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005491692419583242}\n",
      "[flaml.tune.tune: 08-15 01:51:14] {506} INFO - trial 17 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005400819560603306}\n",
      "[flaml.tune.tune: 08-15 01:51:15] {506} INFO - trial 18 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.0056842152617478025}\n",
      "[flaml.tune.tune: 08-15 01:51:15] {506} INFO - trial 19 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005370232296413302}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[01:51:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 08-15 01:51:15] {506} INFO - trial 20 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005273310814672741}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "analysis {'bd7170b4': {'loss': 0.2932019932272869, 'training_cost': 0.30817413330078125, 'training_iteration': 0, 'config': {'n_estimators': 39, 'max_leaves': 9, 'learning_rate': 0.08672915197219133}, 'config/n_estimators': 39, 'config/max_leaves': 9, 'config/learning_rate': 0.08672915197219133, 'experiment_tag': 'exp', 'time_total_s': 0.32521629333496094, 'loss_lagrange': 0.2932019932272869}, 'bda97d2e': {'loss': 0.3552178044511305, 'training_cost': 0.029860258102416992, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 5, 'learning_rate': 0.6111947006764871}, 'config/n_estimators': 7, 'config/max_leaves': 5, 'config/learning_rate': 0.6111947006764871, 'experiment_tag': 'exp', 'time_total_s': 0.037056922912597656, 'loss_lagrange': 0.3552178044511305}, 'be41014e': {'loss': 0.8238569381974518, 'training_cost': 0.028520584106445312, 'training_iteration': 0, 'config': {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}, 'config/n_estimators': 6, 'config/max_leaves': 5, 'config/learning_rate': 0.18074443349590638, 'experiment_tag': 'exp', 'time_total_s': 0.03594350814819336, 'loss_lagrange': 0.8238569381974518}, 'be510cec': {'loss': 3.9320434124731167, 'training_cost': 0.0350644588470459, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.0050708287994836255}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/learning_rate': 0.0050708287994836255, 'experiment_tag': 'exp', 'time_total_s': 0.04178285598754883, 'loss_lagrange': 3.9320434124731167}, 'be74d082': {'loss': 0.8420746064984069, 'training_cost': 0.02660083770751953, 'training_iteration': 0, 'config': {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}, 'config/n_estimators': 4, 'config/max_leaves': 8, 'config/learning_rate': 0.24184523333348865, 'experiment_tag': 'exp', 'time_total_s': 0.033454179763793945, 'loss_lagrange': 0.8420746064984069}, 'beacd63a': {'loss': 0.3562011380932295, 'training_cost': 0.035257816314697266, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.6006787986201269}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.6006787986201269, 'experiment_tag': 'exp', 'time_total_s': 0.042139291763305664, 'loss_lagrange': 0.3562011380932295}, 'beb4d880': {'loss': 0.3557370996374538, 'training_cost': 0.03376269340515137, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5793237833265791}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.5793237833265791, 'experiment_tag': 'exp', 'time_total_s': 0.041426897048950195, 'loss_lagrange': 0.3557370996374538}, 'bebe8b0a': {'loss': 0.3186249025051877, 'training_cost': 0.035384416580200195, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 5, 'learning_rate': 0.6139350165706452}, 'config/n_estimators': 8, 'config/max_leaves': 5, 'config/learning_rate': 0.6139350165706452, 'experiment_tag': 'exp', 'time_total_s': 0.04307675361633301, 'loss_lagrange': 0.3186249025051877}, 'bec6c48c': {'loss': 0.3555395919501293, 'training_cost': 0.029999732971191406, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5823407285827188}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.5823407285827188, 'experiment_tag': 'exp', 'time_total_s': 0.038771867752075195, 'loss_lagrange': 0.3555395919501293}, 'bece6246': {'loss': 0.3736141330578333, 'training_cost': 0.028629064559936523, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.6153425666105765}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/learning_rate': 0.6153425666105765, 'experiment_tag': 'exp', 'time_total_s': 0.03590655326843262, 'loss_lagrange': 0.3736141330578333}, 'bed5a72c': {'loss': 3.7776502666152, 'training_cost': 0.31342244148254395, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.0058671903833274665}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.0058671903833274665, 'experiment_tag': 'exp', 'time_total_s': 0.3202662467956543, 'loss_lagrange': 3.7776502666152}, 'bf089e52': {'loss': 3.80352379937711, 'training_cost': 0.03818225860595703, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005472055643063294}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.005472055643063294, 'experiment_tag': 'exp', 'time_total_s': 0.045723915100097656, 'loss_lagrange': 3.80352379937711}, 'bf11522c': {'loss': 3.802808953642905, 'training_cost': 0.036554813385009766, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005482932727232923}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.005482932727232923, 'experiment_tag': 'exp', 'time_total_s': 0.04422879219055176, 'loss_lagrange': 3.802808953642905}, 'bf19c3f8': {'loss': 3.8673322906832537, 'training_cost': 0.03828239440917969, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.00451003340688015}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.00451003340688015, 'experiment_tag': 'exp', 'time_total_s': 0.04530167579650879, 'loss_lagrange': 3.8673322906832537}, 'bf22658a': {'loss': 3.925433802238373, 'training_cost': 0.030550003051757812, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.004559264656132888}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.004559264656132888, 'experiment_tag': 'exp', 'time_total_s': 0.039549827575683594, 'loss_lagrange': 3.925433802238373}, 'bf2a7da6': {'loss': 3.875029299919108, 'training_cost': 0.03931617736816406, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005491692419583242}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005491692419583242, 'experiment_tag': 'exp', 'time_total_s': 0.049437522888183594, 'loss_lagrange': 3.875029299919108}, 'bf33e922': {'loss': 3.8799155655214927, 'training_cost': 0.030360937118530273, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005400819560603306}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005400819560603306, 'experiment_tag': 'exp', 'time_total_s': 0.03807973861694336, 'loss_lagrange': 3.8799155655214927}, 'bf3b7b38': {'loss': 3.864699473593886, 'training_cost': 0.03027176856994629, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.0056842152617478025}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.0056842152617478025, 'experiment_tag': 'exp', 'time_total_s': 0.038275957107543945, 'loss_lagrange': 3.864699473593886}, 'bf431384': {'loss': 3.8815617684345805, 'training_cost': 0.030115365982055664, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005370232296413302}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005370232296413302, 'experiment_tag': 'exp', 'time_total_s': 0.03780245780944824, 'loss_lagrange': 3.8815617684345805}, 'bf4aa18a': {'loss': 3.8867830670109877, 'training_cost': 0.030040502548217773, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005273310814672741}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005273310814672741, 'experiment_tag': 'exp', 'time_total_s': 0.044951438903808594, 'loss_lagrange': 3.8867830670109877}}\n"
     ]
    }
   ],
   "source": [
    "'''Write a evaluation function'''\n",
    "import xgboost as xgb\n",
    "from flaml import tune\n",
    "import time\n",
    "def xgb_simple_obj(X_train, X_test, y_train, y_test, config):\n",
    "    params = config2params(config)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    start_time = time.time()\n",
    "    _n_estimators = params.pop(\"n_estimators\")\n",
    "    model = xgb.train(params, dtrain, _n_estimators)\n",
    "    end_time = time.time()\n",
    "    # get validation loss\n",
    "    from sklearn.metrics import r2_score\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_test_predict = model.predict(dtest)\n",
    "    test_loss = 1.0 - r2_score(y_test, y_test_predict)\n",
    "    return {\"loss\": test_loss, \"training_cost\": end_time-start_time}\n",
    "\n",
    "def config2params(config: dict) -> dict:\n",
    "    params = config.copy()\n",
    "    max_depth = params[\"max_depth\"] = params.get(\"max_depth\", 0)\n",
    "    if max_depth == 0:\n",
    "        params[\"grow_policy\"] = params.get(\"grow_policy\", \"lossguide\")\n",
    "        params[\"tree_method\"] = params.get(\"tree_method\", \"hist\")\n",
    "    # params[\"booster\"] = params.get(\"booster\", \"gbtree\")\n",
    "    params[\"use_label_encoder\"] = params.get(\"use_label_encoder\", False)\n",
    "    if \"n_jobs\" in config:\n",
    "        params[\"nthread\"] = params.pop(\"n_jobs\")\n",
    "    return params\n",
    "\n",
    "def my_model_size(config):\n",
    "    return config[\"n_estimators\"] * config[\"max_leaves\"]\n",
    "\n",
    "'''Tune xgb_obj with configs from the hierarchical search space'''\n",
    "from flaml.data import load_openml_dataset\n",
    "from functools import partial\n",
    "\n",
    "xgb_space = {\n",
    "     \"n_estimators\": tune.randint(lower=4, upper=64),\n",
    "      \"max_leaves\": tune.randint(lower=4, upper=64),\n",
    "      \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),\n",
    "}\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(\n",
    "    dataset_id=537, data_dir=\"./\"\n",
    ")\n",
    "analysis = tune.run(\n",
    "    partial(xgb_simple_obj, X_train, X_test, y_train, y_test),\n",
    "    config=xgb_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    config_constraints = [(my_model_size, \"<=\", 40)],\n",
    "    metric_constraints = [(\"training_cost\", \"<=\", 1)],\n",
    "    num_samples=20,\n",
    ")\n",
    "print(\"analysis\", analysis.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flaml[ray] in /home/ec2-user/tutorial/FLAML (1.0.9)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.21.4)\n",
      "Requirement already satisfied: lightgbm>=2.3.1 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (3.3.1)\n",
      "Requirement already satisfied: xgboost>=0.90 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.5.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (0.24.2)\n",
      "Requirement already satisfied: ray[tune]~=1.13 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.13.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from lightgbm>=2.3.1->flaml[ray]) (0.37.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from pandas>=1.1.4->flaml[ray]) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from pandas>=1.1.4->flaml[ray]) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->flaml[ray]) (1.15.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (3.4.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (3.19.1)\n",
      "Requirement already satisfied: virtualenv in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (20.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (5.3)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (8.0.3)\n",
      "Requirement already satisfied: frozenlist in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.2.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (21.2.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (2.26.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (4.2.1)\n",
      "Requirement already satisfied: aiosignal in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.2.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.0.3)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.42.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (0.8.9)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (2.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from scikit-learn>=0.24->flaml[ray]) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from scikit-learn>=0.24->flaml[ray]) (1.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from jsonschema->ray[tune]~=1.13->flaml[ray]) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from jsonschema->ray[tune]~=1.13->flaml[ray]) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema->ray[tune]~=1.13->flaml[ray]) (3.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (2021.10.8)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from virtualenv->ray[tune]~=1.13->flaml[ray]) (2.4.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from virtualenv->ray[tune]~=1.13->flaml[ray]) (0.3.3)\n",
      "Requirement already satisfied: backports.entry-points-selectable>=1.0.4 in /home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages (from virtualenv->ray[tune]~=1.13->flaml[ray]) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flaml[ray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:51:18,917]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "2022-08-15 01:51:20,536\tINFO services.py:1470 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-08-15 01:51:22,642\tWARNING function_runner.py:603 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-15 01:51:26 (running for 00:00:01.91)<br>Memory usage on this node: 2.2/7.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/3.71 GiB heap, 0.0/1.85 GiB objects<br>Result logdir: /home/ec2-user/ray_results/evaluate_config_2022-08-15_01-51-22<br>Number of trials: 1/infinite (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial evaluate_config_c536df28 reported score=7224490009.00 with parameters={'x': 3, 'y': 13184}.\n",
      "Trial evaluate_config_c536df28 completed. Last result: score=7224490008.999772,evaluation_cost=3e-05,constraint_metric=39552\n",
      "Trial evaluate_config_c65a5d62 reported score=6219845953.05 with parameters={'x': 6134, 'y': 2076}.\n",
      "Trial evaluate_config_c65a5d62 completed. Last result: score=6219845953.0452795,evaluation_cost=0.06134,constraint_metric=12734184\n",
      "Trial evaluate_config_c65dd12c reported score=7031996448.98 with parameters={'x': 1143, 'y': 74880}.\n",
      "Trial evaluate_config_c65dd12c completed. Last result: score=7031996448.9847355,evaluation_cost=0.01143,constraint_metric=85587840\n",
      "Trial evaluate_config_c669f7d6 reported score=7187648399.99 with parameters={'x': 220, 'y': 22480}.\n",
      "Trial evaluate_config_c669f7d6 completed. Last result: score=7187648399.990213,evaluation_cost=0.0022,constraint_metric=4945600\n",
      "Trial evaluate_config_c66cf5f8 reported score=6314044982.00 with parameters={'x': 5539, 'y': 1}.\n",
      "Trial evaluate_config_c66cf5f8 completed. Last result: score=6314044982.0,evaluation_cost=0.05539,constraint_metric=5539\n",
      "Trial evaluate_config_c66f1acc reported score=7223980036.00 with parameters={'x': 6, 'y': 76053}.\n",
      "Trial evaluate_config_c66f1acc completed. Last result: score=7223980035.999921,evaluation_cost=6e-05,constraint_metric=456318\n",
      "Trial evaluate_config_c679a672 reported score=7224320016.00 with parameters={'x': 4, 'y': 8834}.\n",
      "Trial evaluate_config_c679a672 completed. Last result: score=7224320015.999547,evaluation_cost=4e-05,constraint_metric=35336\n",
      "Trial evaluate_config_c67c90c6 reported score=6116334848.58 with parameters={'x': 6793, 'y': 16190}.\n",
      "Trial evaluate_config_c67c90c6 completed. Last result: score=6116334848.58042,evaluation_cost=0.06793,constraint_metric=109978670\n",
      "Trial evaluate_config_c67eddea reported score=6864453903.98 with parameters={'x': 2148, 'y': 95339}.\n",
      "Trial evaluate_config_c67eddea completed. Last result: score=6864453903.977469,evaluation_cost=0.02148,constraint_metric=204788172\n",
      "Trial evaluate_config_c68b508e reported score=7224830001.00 with parameters={'x': 1, 'y': 51219}.\n",
      "Trial evaluate_config_c68b508e completed. Last result: score=7224830000.999981,evaluation_cost=1e-05,constraint_metric=51219\n",
      "Trial evaluate_config_c69081e4 reported score=5601774024.83 with parameters={'x': 10155, 'y': 61252}.\n",
      "Trial evaluate_config_c69081e4 completed. Last result: score=5601774024.834209,evaluation_cost=0.10155,constraint_metric=622014060\n",
      "Trial evaluate_config_c6931206 reported score=6666722499.89 with parameters={'x': 3350, 'y': 29188}.\n",
      "Trial evaluate_config_c6931206 completed. Last result: score=6666722499.885227,evaluation_cost=0.0335,constraint_metric=97779800\n",
      "Trial evaluate_config_c6a7011c reported score=1110755583.16 with parameters={'x': 51672, 'y': 61799}.\n",
      "Trial evaluate_config_c6a7011c completed. Last result: score=1110755583.1638699,evaluation_cost=0.51672,constraint_metric=3193277928\n",
      "Trial evaluate_config_c6ae386a reported score=6889664015.97 with parameters={'x': 1996, 'y': 60705}.\n",
      "Trial evaluate_config_c6ae386a completed. Last result: score=6889664015.967119,evaluation_cost=0.01996,constraint_metric=121167180\n",
      "Trial evaluate_config_c6ffc7de reported score=4434627648.75 with parameters={'x': 18407, 'y': 72736}.\n",
      "Trial evaluate_config_c6ffc7de completed. Last result: score=4434627648.746934,evaluation_cost=0.18407,constraint_metric=1338851552\n",
      "Trial evaluate_config_c7051590 reported score=224969999.03 with parameters={'x': 99999, 'y': 50862}.\n",
      "Trial evaluate_config_c7051590 completed. Last result: score=224969999.0339153,evaluation_cost=0.99999,constraint_metric=5086149138\n",
      "Trial evaluate_config_c72326fc reported score=224969999.00 with parameters={'x': 99999, 'y': 50037}.\n",
      "Trial evaluate_config_c72326fc completed. Last result: score=224969999.00149888,evaluation_cost=0.99999,constraint_metric=5003649963\n",
      "Trial evaluate_config_c7bdcc34 reported score=1980784035.35 with parameters={'x': 40494, 'y': 62624}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-15 01:51:30 (running for 00:00:05.67)<br>Memory usage on this node: 2.1/7.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 0/0 GPUs, 0.0/3.71 GiB heap, 0.0/1.85 GiB objects<br>Current best trial: c72326fc with score=224969999.00149888 and parameters={'x': 99999, 'y': 50037}<br>Result logdir: /home/ec2-user/ray_results/evaluate_config_2022-08-15_01-51-22<br>Number of trials: 19/infinite (1 PENDING, 1 RUNNING, 17 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial evaluate_config_c7bdcc34 completed. Last result: score=1980784035.353379,evaluation_cost=0.40494,constraint_metric=2535896256\n",
      "Trial evaluate_config_c858d602 reported score=594628223.34 with parameters={'x': 60615, 'y': 36580}.\n",
      "Trial evaluate_config_c858d602 completed. Last result: score=594628223.342947,evaluation_cost=0.60615,constraint_metric=2217296700\n",
      "Trial evaluate_config_c89b5202 reported score=224969999.43 with parameters={'x': 99999, 'y': 63494}.\n",
      "Trial evaluate_config_c89b5202 completed. Last result: score=224969999.4250638,evaluation_cost=0.99999,constraint_metric=6349336506\n",
      "Trial evaluate_config_c8fa5338 reported score=2230483983.38 with parameters={'x': 37772, 'y': 61372}.\n",
      "Trial evaluate_config_c8fa5338 completed. Last result: score=2230483983.38454,evaluation_cost=0.37772,constraint_metric=2318143184\n",
      "Trial evaluate_config_c9957e08 reported score=224969998.42 with parameters={'x': 99999, 'y': 38702}.\n",
      "Trial evaluate_config_c9957e08 completed. Last result: score=224969998.41618004,evaluation_cost=0.99999,constraint_metric=3870161298\n",
      "Trial evaluate_config_c9d1a2ac reported score=2337335715.49 with parameters={'x': 36654, 'y': 71457}.\n",
      "Trial evaluate_config_c9d1a2ac completed. Last result: score=2337335715.487048,evaluation_cost=0.36654,constraint_metric=2619184878\n",
      "Trial evaluate_config_ca6d7c04 reported score=545456023.68 with parameters={'x': 61645, 'y': 46806}.\n",
      "Trial evaluate_config_ca6d7c04 completed. Last result: score=545456023.682968,evaluation_cost=0.61645,constraint_metric=2885355870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 01:51:35,841\tINFO stopper.py:363 -- Reached timeout of 10 seconds. Stopping all trials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial evaluate_config_caa8a734 reported score=224969997.73 with parameters={'x': 99999, 'y': 30598}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-15 01:51:35 (running for 00:00:10.83)<br>Memory usage on this node: 2.1/7.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/3.71 GiB heap, 0.0/1.85 GiB objects<br>Current best trial: caa8a734 with score=224969997.73184523 and parameters={'x': 99999, 'y': 30598}<br>Result logdir: /home/ec2-user/ray_results/evaluate_config_2022-08-15_01-51-22<br>Number of trials: 26/infinite (26 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-15 01:51:35 (running for 00:00:10.84)<br>Memory usage on this node: 2.1/7.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/3.71 GiB heap, 0.0/1.85 GiB objects<br>Current best trial: caa8a734 with score=224969997.73184523 and parameters={'x': 99999, 'y': 30598}<br>Result logdir: /home/ec2-user/ray_results/evaluate_config_2022-08-15_01-51-22<br>Number of trials: 26/infinite (26 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    x</th><th style=\"text-align: right;\">    y</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      score</th><th style=\"text-align: right;\">  evaluation_cost</th><th style=\"text-align: right;\">  constraint_metric</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>evaluate_config_c536df28</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">    3</td><td style=\"text-align: right;\">13184</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00030303 </td><td style=\"text-align: right;\">7.22449e+09</td><td style=\"text-align: right;\">          3e-05  </td><td style=\"text-align: right;\">              39552</td></tr>\n",
       "<tr><td>evaluate_config_c65a5d62</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\"> 6134</td><td style=\"text-align: right;\"> 2076</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0616226  </td><td style=\"text-align: right;\">6.21985e+09</td><td style=\"text-align: right;\">          0.06134</td><td style=\"text-align: right;\">           12734184</td></tr>\n",
       "<tr><td>evaluate_config_c65dd12c</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\"> 1143</td><td style=\"text-align: right;\">74880</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0119874  </td><td style=\"text-align: right;\">7.032e+09  </td><td style=\"text-align: right;\">          0.01143</td><td style=\"text-align: right;\">           85587840</td></tr>\n",
       "<tr><td>evaluate_config_c669f7d6</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">  220</td><td style=\"text-align: right;\">22480</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00262713 </td><td style=\"text-align: right;\">7.18765e+09</td><td style=\"text-align: right;\">          0.0022 </td><td style=\"text-align: right;\">            4945600</td></tr>\n",
       "<tr><td>evaluate_config_c66cf5f8</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\"> 5539</td><td style=\"text-align: right;\">    1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0560231  </td><td style=\"text-align: right;\">6.31404e+09</td><td style=\"text-align: right;\">          0.05539</td><td style=\"text-align: right;\">               5539</td></tr>\n",
       "<tr><td>evaluate_config_c66f1acc</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">    6</td><td style=\"text-align: right;\">76053</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00024724 </td><td style=\"text-align: right;\">7.22398e+09</td><td style=\"text-align: right;\">          6e-05  </td><td style=\"text-align: right;\">             456318</td></tr>\n",
       "<tr><td>evaluate_config_c679a672</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">    4</td><td style=\"text-align: right;\"> 8834</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000227928</td><td style=\"text-align: right;\">7.22432e+09</td><td style=\"text-align: right;\">          4e-05  </td><td style=\"text-align: right;\">              35336</td></tr>\n",
       "<tr><td>evaluate_config_c67c90c6</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\"> 6793</td><td style=\"text-align: right;\">16190</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0681667  </td><td style=\"text-align: right;\">6.11633e+09</td><td style=\"text-align: right;\">          0.06793</td><td style=\"text-align: right;\">          109978670</td></tr>\n",
       "<tr><td>evaluate_config_c67eddea</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\"> 2148</td><td style=\"text-align: right;\">95339</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.021688   </td><td style=\"text-align: right;\">6.86445e+09</td><td style=\"text-align: right;\">          0.02148</td><td style=\"text-align: right;\">          204788172</td></tr>\n",
       "<tr><td>evaluate_config_c68b508e</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">    1</td><td style=\"text-align: right;\">51219</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000186205</td><td style=\"text-align: right;\">7.22483e+09</td><td style=\"text-align: right;\">          1e-05  </td><td style=\"text-align: right;\">              51219</td></tr>\n",
       "<tr><td>evaluate_config_c69081e4</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">10155</td><td style=\"text-align: right;\">61252</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.101838   </td><td style=\"text-align: right;\">5.60177e+09</td><td style=\"text-align: right;\">          0.10155</td><td style=\"text-align: right;\">          622014060</td></tr>\n",
       "<tr><td>evaluate_config_c6931206</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\"> 3350</td><td style=\"text-align: right;\">29188</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0337539  </td><td style=\"text-align: right;\">6.66672e+09</td><td style=\"text-align: right;\">          0.0335 </td><td style=\"text-align: right;\">           97779800</td></tr>\n",
       "<tr><td>evaluate_config_c6a7011c</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">51672</td><td style=\"text-align: right;\">61799</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.517449   </td><td style=\"text-align: right;\">1.11076e+09</td><td style=\"text-align: right;\">          0.51672</td><td style=\"text-align: right;\">         3193277928</td></tr>\n",
       "<tr><td>evaluate_config_c6ae386a</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\"> 1996</td><td style=\"text-align: right;\">60705</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0201714  </td><td style=\"text-align: right;\">6.88966e+09</td><td style=\"text-align: right;\">          0.01996</td><td style=\"text-align: right;\">          121167180</td></tr>\n",
       "<tr><td>evaluate_config_c6ffc7de</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">18407</td><td style=\"text-align: right;\">72736</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.184582   </td><td style=\"text-align: right;\">4.43463e+09</td><td style=\"text-align: right;\">          0.18407</td><td style=\"text-align: right;\">         1338851552</td></tr>\n",
       "<tr><td>evaluate_config_c7051590</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">99999</td><td style=\"text-align: right;\">50862</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     1.00135    </td><td style=\"text-align: right;\">2.2497e+08 </td><td style=\"text-align: right;\">          0.99999</td><td style=\"text-align: right;\">         5086149138</td></tr>\n",
       "<tr><td>evaluate_config_c72326fc</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">99999</td><td style=\"text-align: right;\">50037</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     1.00037    </td><td style=\"text-align: right;\">2.2497e+08 </td><td style=\"text-align: right;\">          0.99999</td><td style=\"text-align: right;\">         5003649963</td></tr>\n",
       "<tr><td>evaluate_config_c7bdcc34</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">40494</td><td style=\"text-align: right;\">62624</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.406778   </td><td style=\"text-align: right;\">1.98078e+09</td><td style=\"text-align: right;\">          0.40494</td><td style=\"text-align: right;\">         2535896256</td></tr>\n",
       "<tr><td>evaluate_config_c858d602</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">60615</td><td style=\"text-align: right;\">36580</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.609233   </td><td style=\"text-align: right;\">5.94628e+08</td><td style=\"text-align: right;\">          0.60615</td><td style=\"text-align: right;\">         2217296700</td></tr>\n",
       "<tr><td>evaluate_config_c89b5202</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">99999</td><td style=\"text-align: right;\">63494</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     1.00081    </td><td style=\"text-align: right;\">2.2497e+08 </td><td style=\"text-align: right;\">          0.99999</td><td style=\"text-align: right;\">         6349336506</td></tr>\n",
       "<tr><td>evaluate_config_c8fa5338</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">37772</td><td style=\"text-align: right;\">61372</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.37827    </td><td style=\"text-align: right;\">2.23048e+09</td><td style=\"text-align: right;\">          0.37772</td><td style=\"text-align: right;\">         2318143184</td></tr>\n",
       "<tr><td>evaluate_config_c9957e08</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">99999</td><td style=\"text-align: right;\">38702</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     1.00128    </td><td style=\"text-align: right;\">2.2497e+08 </td><td style=\"text-align: right;\">          0.99999</td><td style=\"text-align: right;\">         3870161298</td></tr>\n",
       "<tr><td>evaluate_config_c9d1a2ac</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">36654</td><td style=\"text-align: right;\">71457</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.372607   </td><td style=\"text-align: right;\">2.33734e+09</td><td style=\"text-align: right;\">          0.36654</td><td style=\"text-align: right;\">         2619184878</td></tr>\n",
       "<tr><td>evaluate_config_ca6d7c04</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">61645</td><td style=\"text-align: right;\">46806</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.617233   </td><td style=\"text-align: right;\">5.45456e+08</td><td style=\"text-align: right;\">          0.61645</td><td style=\"text-align: right;\">         2885355870</td></tr>\n",
       "<tr><td>evaluate_config_caa8a734</td><td>TERMINATED</td><td>172.31.46.142:32472</td><td style=\"text-align: right;\">99999</td><td style=\"text-align: right;\">30598</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     1.00039    </td><td style=\"text-align: right;\">2.2497e+08 </td><td style=\"text-align: right;\">          0.99999</td><td style=\"text-align: right;\">         3059769402</td></tr>\n",
       "<tr><td>evaluate_config_cb098aae</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">99999</td><td style=\"text-align: right;\">40382</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 01:51:36,123\tINFO tune.py:747 -- Total run time: 13.49 seconds (10.84 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 224969997.73184523, 'evaluation_cost': 0.99999, 'constraint_metric': 3059769402, 'time_this_iter_s': 1.0003881454467773, 'done': False, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 1, 'trial_id': 'caa8a734', 'experiment_id': '6b71e8bc54c0420c8c26b0c528a6d9b6', 'date': '2022-08-15_01-51-35', 'timestamp': 1660528295, 'time_total_s': 1.0003881454467773, 'pid': 32472, 'hostname': 'ip-172-31-46-142.us-east-2.compute.internal', 'node_ip': '172.31.46.142', 'config': {'x': 99999, 'y': 30598}, 'time_since_restore': 1.0003881454467773, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.0036840438842773438, 'experiment_tag': '25_x=99999,y=30598'}\n",
      "{'x': 99999, 'y': 30598}\n"
     ]
    }
   ],
   "source": [
    "# require: pip install flaml[ray]\n",
    "analysis = tune.run(\n",
    "    evaluate_config,  # the function to evaluate a config\n",
    "    config=config_search_space,  # the search space defined\n",
    "    metric=\"score\",\n",
    "    mode=\"min\",  # the optimization mode, \"min\" or \"max\"\n",
    "    num_samples=-1,  # the maximal number of configs to try, -1 means infinite\n",
    "    time_budget_s=10,  # the time budget in seconds\n",
    "    use_ray=True,\n",
    "    resources_per_trial={\"cpu\": 2}  # limit resources allocated per trial\n",
    ")\n",
    "print(analysis.best_trial.last_result)  # the best trial's result\n",
    "print(analysis.best_config)  # the best config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 01:51:37,165\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:51:37,168]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "/home/ec2-user/miniconda3/envs/myflaml/lib/python3.8/site-packages/ray/tune/suggest/optuna.py:561: ExperimentalWarning: create_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
      "  trial = ot.trial.create_trial(\n",
      "2022-08-15 01:51:37,171\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:51:37,173]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 1 config: {'b': 0.8, 'a': 3.0}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 2 config: {'b': 0.8, 'a': 2.0}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 3 config: {'a': 0.7636074368340785, 'b': 0.0622558480782045}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 4 config: {'a': 0.6273117525770127, 'b': 2.246411647615836}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 5 config: {'a': 0.4935219421795645, 'b': 0.674389936592543}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 6 config: {'a': 0.19608223611202774, 'b': 2.2815921365968763}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 7 config: {'a': 0.1674197281969101, 'b': 0.2650194425220308}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 8 config: {'a': 0.6785062201841192, 'b': 2.8601800385848097}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 9 config: {'a': 0.003908783664635307, 'b': 1.53657679015733}\n",
      "[flaml.tune.tune: 08-15 01:51:37] {506} INFO - trial 10 config: {'a': 0.8044947520355924, 'b': 1.8375782004881644}\n"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "\n",
    "config_search_space = {\n",
    "    \"a\": tune.uniform(lower=0, upper=0.99),\n",
    "    \"b\": tune.uniform(lower=0, upper=3),\n",
    "}\n",
    "\n",
    "def simple_obj(config):\n",
    "    return config[\"a\"] + config[\"b\"]\n",
    "\n",
    "points_to_evaluate = [\n",
    "    {\"b\": .99, \"a\": 3},\n",
    "    {\"b\": .99, \"a\": 2},\n",
    "    {\"b\": .80, \"a\": 3},\n",
    "    {\"b\": .80, \"a\": 2},\n",
    "]\n",
    "evaluated_rewards = [3.99, 2.99]\n",
    "\n",
    "analysis = tune.run(\n",
    "    simple_obj,\n",
    "    config=config_search_space,\n",
    "    mode=\"max\",\n",
    "    points_to_evaluate=points_to_evaluate,\n",
    "    evaluated_rewards=evaluated_rewards,\n",
    "    num_samples=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial scheduling\n",
    "\n",
    "###  An authentic scheduler implemented in FLAML (`scheduler='flaml'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "        \"n_estimators\": tune.lograndint(lower=4, upper=32768),\n",
    "        \"max_leaves\": tune.lograndint(lower=4, upper=32768),\n",
    "        \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set a evaluation function with resource dimension'''\n",
    "def obj_from_resource_attr(resource_attr, X_train, X_test, y_train, y_test, config):\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # in this example sample size is our resource dimension\n",
    "    resource = int(config[resource_attr])\n",
    "    sampled_X_train = X_train.iloc[:resource]\n",
    "    sampled_y_train = y_train[:resource]\n",
    "\n",
    "    # construct a LGBM model from the config\n",
    "    # note that you need to first remove the resource_attr field\n",
    "    # from the config as it is not part of the original search space\n",
    "    model_config = config.copy()\n",
    "    del model_config[resource_attr]\n",
    "    model = LGBMClassifier(**model_config)\n",
    "\n",
    "    model.fit(sampled_X_train, sampled_y_train)\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    test_loss = 1.0 - accuracy_score(y_test, y_test_predict)\n",
    "    return {\"loss\": test_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 01:51:39,897\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-08-15 01:51:39,900]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 08-15 01:51:39] {506} INFO - trial 1 config: {'n_estimators': 9, 'max_leaves': 1364, 'learning_rate': 0.012074374674294664, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download dataset from openml\n",
      "X_train.shape: (43957, 14), y_train.shape: (43957,),\n",
      "X_test.shape: (4885, 14), y_test.shape: (4885,)\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/home/ec2-user/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "from functools import partial\n",
    "from flaml.data import load_openml_task\n",
    "    \n",
    "X_train, X_test, y_train, y_test = load_openml_task(task_id=7592, data_dir=\"\")\n",
    "max_resource = len(y_train)\n",
    "resource_attr = \"sample_size\"\n",
    "min_resource = 1000\n",
    "analysis = tune.run(\n",
    "    partial(\n",
    "        obj_from_resource_attr, resource_attr, X_train, X_test, y_train, y_test\n",
    "    ),\n",
    "    config=search_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    resource_attr=resource_attr,\n",
    "    scheduler=\"flaml\",\n",
    "    max_resource=max_resource,\n",
    "    min_resource=min_resource,\n",
    "    reduction_factor=2,\n",
    "    time_budget_s=10,\n",
    "    num_samples=-1,\n",
    ")\n",
    "print(\"best result w/ flaml scheduler (in 10s): \", analysis.best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ASHA scheduler (`scheduler='asha'`) or a custom scheduler of the  [`TrialScheduler`](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-schedulers) class from `ray.tune`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_w_intermediate_report(\n",
    "        resource_attr,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        min_resource,\n",
    "        max_resource,\n",
    "        config,\n",
    "    ):\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # a customized schedule to perform the evaluation\n",
    "    eval_schedule = [res for res in range(min_resource, max_resource, 5000)] + [\n",
    "        max_resource\n",
    "    ]\n",
    "    for resource in eval_schedule:\n",
    "        sampled_X_train = X_train.iloc[:resource]\n",
    "        sampled_y_train = y_train[:resource]\n",
    "\n",
    "        # construct a LGBM model from the config\n",
    "        model = LGBMClassifier(**config)\n",
    "\n",
    "        model.fit(sampled_X_train, sampled_y_train)\n",
    "        y_test_predict = model.predict(X_test)\n",
    "        test_loss = 1.0 - accuracy_score(y_test, y_test_predict)\n",
    "        # need to report the resource attribute used and the corresponding intermediate results\n",
    "        try:\n",
    "            tune.report(sample_size=resource, loss=test_loss)\n",
    "        except StopIteration:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 04:19:32,319\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-07-07 04:19:32,321]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 07-07 04:19:32] {506} INFO - trial 1 config: {'n_estimators': 9, 'max_leaves': 1364, 'learning_rate': 0.012074374674294664}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from openml_task7592.pkl\n",
      "X_train.shape: (43957, 14), y_train.shape: (43957,),\n",
      "X_test.shape: (4885, 14), y_test.shape: (4885,)\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 07-07 04:19:33] {506} INFO - trial 2 config: {'n_estimators': 4048, 'max_leaves': 4, 'learning_rate': 0.07891713267442702}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "best result w/ asha scheduler (in 10s):  {'sample_size': 43957, 'loss': 0.13920163766632554, 'training_iteration': 9, 'config': {'n_estimators': 4048, 'max_leaves': 4, 'learning_rate': 0.07891713267442702}, 'config/n_estimators': 4048, 'config/max_leaves': 4, 'config/learning_rate': 0.07891713267442702, 'experiment_tag': 'exp', 'time_total_s': 66.68751931190491}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_openml_task(task_id=7592, data_dir=\"\")\n",
    "resource_attr = \"sample_size\"\n",
    "min_resource = 1000\n",
    "max_resource = len(y_train)\n",
    "analysis = tune.run(\n",
    "    partial(\n",
    "        obj_w_intermediate_report,\n",
    "        resource_attr,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        min_resource,\n",
    "        max_resource,\n",
    "    ),\n",
    "    config=search_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    resource_attr=resource_attr,\n",
    "    scheduler=\"asha\",\n",
    "    max_resource=max_resource,\n",
    "    min_resource=min_resource,\n",
    "    reduction_factor=2,\n",
    "    time_budget_s=10,\n",
    "    num_samples=-1,\n",
    ")\n",
    "print(\"best result w/ asha scheduler (in 10s): \", analysis.best_result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61dacc07bdd67a6bd133ad154042152699ffea5858044733f84b495e7a4b9e6a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myflaml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
