{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook includes toy examples to demonstrate how to tune User Defined Functions with `flaml.tune`.\n",
    "\n",
    "FLAML requires `Python>=3.7`. To run this notebook example, please install flaml with the `notebook` option:\n",
    "```bash\n",
    "pip install flaml[notebook]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flaml[notebook]\n",
    "# from v0.6.6, catboost is made an optional dependency to build conda package.\n",
    "# to install catboost without installing the notebook option, you can run:\n",
    "# %pip install flaml[catboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tuning procedure\n",
    "## 1. A basic tuning example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set a search space'''\n",
    "from flaml import tune\n",
    "config_search_space = {\n",
    "    \"x\": tune.lograndint(lower=1, upper=100000),\n",
    "    \"y\": tune.randint(lower=1, upper=100000)\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a evaluation function'''\n",
    "import time\n",
    "def evaluate_config(config: dict):\n",
    "    \"\"\"evaluate a hyperparameter configuration\"\"\"\n",
    "    score = (config[\"x\"] - 85000) ** 2 - config[\"x\"] / config[\"y\"]\n",
    "    # usually the evaluation takes an non-neglible cost\n",
    "    # and the cost could be related to certain hyperparameters\n",
    "    # here we simulate this cost by calling the time.sleep() function\n",
    "    # here we assume the cost is proportional to x\n",
    "    faked_evaluation_cost = config[\"x\"] / 100000\n",
    "    time.sleep(faked_evaluation_cost)\n",
    "    # we can return a single float as a score on the input config:\n",
    "    # return score\n",
    "    # or, we can return a dictionary that maps metric name to metric value:\n",
    "    return {\"score\": score, \"evaluation_cost\": faked_evaluation_cost, \"constraint_metric\": config[\"x\"] * config[\"y\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:33:55] {486} INFO - Using search algorithm type.\n",
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:33:55,739]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 1 config: {'x': 3, 'y': 13184}\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 2 config: {'x': 6134, 'y': 2076}\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 3 config: {'x': 1143, 'y': 74880}\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 4 config: {'x': 5539, 'y': 1}\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 5 config: {'x': 6793, 'y': 16190}\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 6 config: {'x': 220, 'y': 22480}\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 7 config: {'x': 6, 'y': 76053}\n",
      "[flaml.tune.tune: 12-08 10:33:55] {636} INFO - trial 8 config: {'x': 4, 'y': 8834}\n",
      "[flaml.tune.tune: 12-08 10:33:56] {636} INFO - trial 9 config: {'x': 2148, 'y': 95339}\n",
      "[flaml.tune.tune: 12-08 10:33:56] {636} INFO - trial 10 config: {'x': 1, 'y': 51219}\n",
      "[flaml.tune.tune: 12-08 10:33:56] {636} INFO - trial 11 config: {'x': 10155, 'y': 61252}\n",
      "[flaml.tune.tune: 12-08 10:33:56] {636} INFO - trial 12 config: {'x': 51672, 'y': 61799}\n",
      "[flaml.tune.tune: 12-08 10:33:56] {636} INFO - trial 13 config: {'x': 18407, 'y': 72736}\n",
      "[flaml.tune.tune: 12-08 10:33:56] {636} INFO - trial 14 config: {'x': 99999, 'y': 50862}\n",
      "[flaml.tune.tune: 12-08 10:33:57] {636} INFO - trial 15 config: {'x': 2, 'y': 372}\n",
      "[flaml.tune.tune: 12-08 10:33:57] {636} INFO - trial 16 config: {'x': 99999, 'y': 39100}\n",
      "[flaml.tune.tune: 12-08 10:33:58] {636} INFO - trial 17 config: {'x': 40494, 'y': 50862}\n",
      "[flaml.tune.tune: 12-08 10:33:59] {636} INFO - trial 18 config: {'x': 60615, 'y': 25643}\n",
      "[flaml.tune.tune: 12-08 10:33:59] {636} INFO - trial 19 config: {'x': 99999, 'y': 52557}\n",
      "[flaml.tune.tune: 12-08 10:34:00] {636} INFO - trial 20 config: {'x': 3350, 'y': 29188}\n",
      "[flaml.tune.tune: 12-08 10:34:00] {636} INFO - trial 21 config: {'x': 6, 'y': 25996}\n",
      "[flaml.tune.tune: 12-08 10:34:00] {636} INFO - trial 22 config: {'x': 36654, 'y': 71457}\n",
      "[flaml.tune.tune: 12-08 10:34:01] {636} INFO - trial 23 config: {'x': 376, 'y': 14217}\n",
      "[flaml.tune.tune: 12-08 10:34:01] {636} INFO - trial 24 config: {'x': 99999, 'y': 64368}\n",
      "[flaml.tune.tune: 12-08 10:34:02] {636} INFO - trial 25 config: {'x': 51439, 'y': 97709}\n",
      "[flaml.tune.tune: 12-08 10:34:02] {636} INFO - trial 26 config: {'x': 24442, 'y': 71457}\n",
      "[flaml.tune.tune: 12-08 10:34:03] {636} INFO - trial 27 config: {'x': 60949, 'y': 50896}\n",
      "[flaml.tune.tune: 12-08 10:34:03] {636} INFO - trial 28 config: {'x': 73238, 'y': 99999}\n",
      "[flaml.tune.tune: 12-08 10:34:04] {636} INFO - trial 29 config: {'x': 51439, 'y': 86194}\n",
      "[flaml.tune.tune: 12-08 10:34:04] {636} INFO - trial 30 config: {'x': 99999, 'y': 77840}\n"
     ]
    }
   ],
   "source": [
    "''''Performs tuning'''\n",
    "# require: pip install flaml[blendsearch]\n",
    "analysis = tune.run(\n",
    "    evaluate_config,  # the function to evaluate a config\n",
    "    config=config_search_space,  # the search space defined\n",
    "    metric=\"score\",\n",
    "    mode=\"min\",  # the optimization mode, \"min\" or \"max\"\n",
    "    num_samples=-1,  # the maximal number of configs to try, -1 means infinite\n",
    "    time_budget_s=10,  # the time budget in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 138344643.26761267, 'evaluation_cost': 0.73238, 'constraint_metric': 7323726762, 'training_iteration': 0, 'config': {'x': 73238, 'y': 99999}, 'config/x': 73238, 'config/y': 99999, 'experiment_tag': 'exp', 'time_total_s': 0.7345480918884277}\n"
     ]
    }
   ],
   "source": [
    "'''Investigate results'''\n",
    "print(analysis.best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(analysis.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical search space \n",
    "Hierarchical search space is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set a hierarchical search space'''\n",
    "from flaml import tune\n",
    "gbtree_hp_space = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"n_estimators\": tune.lograndint(lower=4, upper=64),\n",
    "        \"max_leaves\": tune.lograndint(lower=4, upper=64),\n",
    "        \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),\n",
    "    }\n",
    "gblinear_hp_space = {\n",
    "    \"booster\": \"gblinear\",\n",
    "    \"lambda\": tune.uniform(0, 1),\n",
    "    \"alpha\": tune.loguniform(0.0001, 1),\n",
    "}\n",
    "\n",
    "full_space = {\n",
    "    \"xgb_config\": tune.choice([gbtree_hp_space, gblinear_hp_space]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a evaluation function'''\n",
    "import xgboost as xgb\n",
    "def xgb_obj(X_train, X_test, y_train, y_test, config):\n",
    "    config = config[\"xgb_config\"]\n",
    "    params = config2params(config)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    booster_type = config.get(\"booster\")\n",
    "\n",
    "    if booster_type == \"gblinear\":\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "        )\n",
    "    else:\n",
    "        _n_estimators = params.pop(\"n_estimators\")\n",
    "        model = xgb.train(params, dtrain, _n_estimators)\n",
    "\n",
    "    # get validation loss\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_test_predict = model.predict(dtest)\n",
    "    test_loss = 1.0 - r2_score(y_test, y_test_predict)\n",
    "    return {\"loss\": test_loss}\n",
    "\n",
    "def config2params(config: dict) -> dict:\n",
    "    params = config.copy()\n",
    "    max_depth = params[\"max_depth\"] = params.get(\"max_depth\", 0)\n",
    "    if max_depth == 0:\n",
    "        params[\"grow_policy\"] = params.get(\"grow_policy\", \"lossguide\")\n",
    "        params[\"tree_method\"] = params.get(\"tree_method\", \"hist\")\n",
    "    # params[\"booster\"] = params.get(\"booster\", \"gbtree\")\n",
    "    params[\"use_label_encoder\"] = params.get(\"use_label_encoder\", False)\n",
    "    if \"n_jobs\" in config:\n",
    "        params[\"nthread\"] = params.pop(\"n_jobs\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tune xgb_obj with configs from the hierarchical search space'''\n",
    "from flaml.data import load_openml_dataset\n",
    "from functools import partial\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(\n",
    "    dataset_id=537, data_dir=\"./\"\n",
    ")\n",
    "analysis = tune.run(\n",
    "    partial(xgb_obj, X_train, X_test, y_train, y_test),\n",
    "    config=full_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=5,\n",
    ")\n",
    "print(\"analysis\", analysis.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tuning Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constraints on the tuning\n",
    "\n",
    "1. A user can specify constraints on the configurations to be satisfied via the argument `config_constraints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:07] {486} INFO - Using search algorithm type.\n",
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:07,631]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 1 config: {'width': 1, 'height': 132, 'length': 647}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 2 config: {'width': 2, 'height': 760, 'length': 169}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 3 config: {'width': 1, 'height': 685, 'length': 953}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 4 config: {'width': 1, 'height': 512, 'length': 812}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 5 config: {'width': 1, 'height': 373, 'length': 674}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'volume': 85404, 'training_iteration': 0, 'config': {'width': 1, 'height': 132, 'length': 647}, 'config/width': 1, 'config/height': 132, 'config/length': 647, 'experiment_tag': 'exp', 'time_total_s': 0.0012857913970947266}\n"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "def area(config):\n",
    "    return config[\"width\"] * config[\"length\"]\n",
    "\n",
    "cube_search_space = {\n",
    "    \"width\": tune.lograndint(lower=1, upper=1000),\n",
    "    \"height\": tune.randint(lower=1, upper=1000),\n",
    "    \"length\": tune.randint(lower=1, upper=1000),\n",
    "}\n",
    "\n",
    "def cube_volume(config: dict):\n",
    "    \"\"\"evaluate a hyperparameter configuration\"\"\"\n",
    "    score = config[\"width\"] * config[\"height\"] * config[\"length\"]\n",
    "    return {\"volume\": score}\n",
    "\n",
    "analysis = tune.run(evaluation_function=cube_volume,\n",
    "         mode=\"min\",\n",
    "         metric=\"volume\",\n",
    "         config=cube_search_space,\n",
    "         config_constraints=[(area, \"<=\", 1000)],\n",
    "         num_samples=5,\n",
    "        )\n",
    "print(analysis.best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  You can also specify a list of metric constraints to be satisfied via the argument `metric_constraints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:07] {486} INFO - Using search algorithm type.\n",
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:07,831]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 1 config: {'x': 3, 'y': 13184}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 2 config: {'x': 6134, 'y': 2076}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 3 config: {'x': 1143, 'y': 74880}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 4 config: {'x': 5539, 'y': 1}\n",
      "[flaml.tune.tune: 12-08 10:34:07] {636} INFO - trial 5 config: {'x': 6793, 'y': 16190}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flaml.tune.tune.ExperimentAnalysis at 0x7fb08cffefa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flaml import tune\n",
    "tune.run(evaluation_function=evaluate_config,\n",
    "         mode=\"min\",\n",
    "         metric=\"score\",\n",
    "         config=config_search_space,\n",
    "         metric_constraints=[(\"evaluation_cost\", \"<=\", 0.1)],\n",
    "         num_samples=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config_constraints vs metric_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:08] {486} INFO - Using search algorithm type.\n",
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:08,193]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 12-08 10:34:08] {636} INFO - trial 1 config: {'n_estimators': 7, 'max_leaves': 5, 'learning_rate': 0.6111947006764871}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from ./openml_ds537.pkl\n",
      "Dataset name: houses\n",
      "X_train.shape: (15480, 8), y_train.shape: (15480,);\n",
      "X_test.shape: (5160, 8), y_test.shape: (5160,)\n",
      "[10:34:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:08] {636} INFO - trial 2 config: {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "Received additional result for trial e6bcd752, but it already finished. Result: {'loss': 0.8238569381974518, 'training_cost': 0.021718502044677734, 'training_iteration': 0, 'config': {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}, 'config/n_estimators': 6, 'config/max_leaves': 5, 'config/learning_rate': 0.18074443349590638, 'experiment_tag': 'exp', 'time_total_s': 0.03700089454650879, 'loss_lagrange': 0.8238569381974518}\n",
      "Received additional completion for trial e6bcd752, but it already finished. Result: {'loss': 0.8238569381974518, 'training_cost': 0.021718502044677734, 'training_iteration': 0, 'config': {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}, 'config/n_estimators': 6, 'config/max_leaves': 5, 'config/learning_rate': 0.18074443349590638, 'experiment_tag': 'exp', 'time_total_s': 0.03700089454650879, 'loss_lagrange': 0.8238569381974518}\n",
      "[flaml.tune.tune: 12-08 10:34:09] {636} INFO - trial 3 config: {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.0050708287994836255}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:34:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:09] {636} INFO - trial 4 config: {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:34:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received additional result for trial e6bcd76b, but it already finished. Result: {'loss': 0.8420746064984069, 'training_cost': 0.19157624244689941, 'training_iteration': 0, 'config': {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}, 'config/n_estimators': 4, 'config/max_leaves': 8, 'config/learning_rate': 0.24184523333348865, 'experiment_tag': 'exp', 'time_total_s': 0.20658111572265625, 'loss_lagrange': 0.8420746064984069}\n",
      "Received additional completion for trial e6bcd76b, but it already finished. Result: {'loss': 0.8420746064984069, 'training_cost': 0.19157624244689941, 'training_iteration': 0, 'config': {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}, 'config/n_estimators': 4, 'config/max_leaves': 8, 'config/learning_rate': 0.24184523333348865, 'experiment_tag': 'exp', 'time_total_s': 0.20658111572265625, 'loss_lagrange': 0.8420746064984069}\n",
      "[flaml.tune.tune: 12-08 10:34:09] {636} INFO - trial 5 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.6006787986201269}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:09] {636} INFO - trial 6 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5793237833265791}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:09] {636} INFO - trial 7 config: {'n_estimators': 8, 'max_leaves': 5, 'learning_rate': 0.6139350165706452}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:34:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:09] {636} INFO - trial 8 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5823407285827188}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 9 config: {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.6153425666105765}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 10 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.0058671903833274665}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 11 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005472055643063294}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 12 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005482932727232923}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:34:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 13 config: {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.00451003340688015}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 14 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.004559264656132888}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 15 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005491692419583242}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 16 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005400819560603306}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 17 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.0056842152617478025}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 18 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005370232296413302}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 19 config: {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005273310814672741}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[flaml.tune.tune: 12-08 10:34:10] {636} INFO - trial 20 config: {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.003752175266028254}\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/xgboost/data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "analysis {'e6bcd6fe': {'loss': 0.3552178044511305, 'training_cost': 0.09059309959411621, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 5, 'learning_rate': 0.6111947006764871}, 'config/n_estimators': 7, 'config/max_leaves': 5, 'config/learning_rate': 0.6111947006764871, 'experiment_tag': 'exp', 'time_total_s': 0.11991572380065918, 'loss_lagrange': 0.3552178044511305}, 'e6bcd752': {'loss': 0.8238569381974518, 'training_cost': 0.021718502044677734, 'training_iteration': 0, 'config': {'n_estimators': 6, 'max_leaves': 5, 'learning_rate': 0.18074443349590638}, 'config/n_estimators': 6, 'config/max_leaves': 5, 'config/learning_rate': 0.18074443349590638, 'experiment_tag': 'exp', 'time_total_s': 0.03700089454650879, 'loss_lagrange': 0.8238569381974518}, 'e6bcd768': {'loss': 3.9320434124731167, 'training_cost': 0.01953291893005371, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.0050708287994836255}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/learning_rate': 0.0050708287994836255, 'experiment_tag': 'exp', 'time_total_s': 0.04684638977050781, 'loss_lagrange': 3.9320434124731167}, 'e6bcd76b': {'loss': 0.8420746064984069, 'training_cost': 0.19157624244689941, 'training_iteration': 0, 'config': {'n_estimators': 4, 'max_leaves': 8, 'learning_rate': 0.24184523333348865}, 'config/n_estimators': 4, 'config/max_leaves': 8, 'config/learning_rate': 0.24184523333348865, 'experiment_tag': 'exp', 'time_total_s': 0.20658111572265625, 'loss_lagrange': 0.8420746064984069}, 'e6bcd799': {'loss': 0.3562011380932295, 'training_cost': 0.022757768630981445, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.6006787986201269}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.6006787986201269, 'experiment_tag': 'exp', 'time_total_s': 0.03645753860473633, 'loss_lagrange': 0.3562011380932295}, 'e6bcd79a': {'loss': 0.3557370996374538, 'training_cost': 0.01763319969177246, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5793237833265791}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.5793237833265791, 'experiment_tag': 'exp', 'time_total_s': 0.025971174240112305, 'loss_lagrange': 0.3557370996374538}, 'e6bcd79c': {'loss': 0.3186249025051877, 'training_cost': 0.11423397064208984, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 5, 'learning_rate': 0.6139350165706452}, 'config/n_estimators': 8, 'config/max_leaves': 5, 'config/learning_rate': 0.6139350165706452, 'experiment_tag': 'exp', 'time_total_s': 0.16222643852233887, 'loss_lagrange': 0.3186249025051877}, 'e6bcd79d': {'loss': 0.3555395919501293, 'training_cost': 0.019733667373657227, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.5823407285827188}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.5823407285827188, 'experiment_tag': 'exp', 'time_total_s': 0.037145137786865234, 'loss_lagrange': 0.3555395919501293}, 'e7d2a608': {'loss': 0.3736141330578333, 'training_cost': 0.025461673736572266, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.6153425666105765}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/learning_rate': 0.6153425666105765, 'experiment_tag': 'exp', 'time_total_s': 0.03709578514099121, 'loss_lagrange': 0.3736141330578333}, 'e7d2a609': {'loss': 3.7776502666152, 'training_cost': 0.019869565963745117, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.0058671903833274665}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.0058671903833274665, 'experiment_tag': 'exp', 'time_total_s': 0.02805638313293457, 'loss_lagrange': 3.7776502666152}, 'e7d2a60a': {'loss': 3.80352379937711, 'training_cost': 0.020340919494628906, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005472055643063294}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.005472055643063294, 'experiment_tag': 'exp', 'time_total_s': 0.02846550941467285, 'loss_lagrange': 3.80352379937711}, 'e7d2a60b': {'loss': 3.802808953642905, 'training_cost': 0.05766487121582031, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.005482932727232923}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.005482932727232923, 'experiment_tag': 'exp', 'time_total_s': 0.07246065139770508, 'loss_lagrange': 3.802808953642905}, 'e7d2a60c': {'loss': 3.8673322906832537, 'training_cost': 0.039414167404174805, 'training_iteration': 0, 'config': {'n_estimators': 10, 'max_leaves': 4, 'learning_rate': 0.00451003340688015}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/learning_rate': 0.00451003340688015, 'experiment_tag': 'exp', 'time_total_s': 0.04778552055358887, 'loss_lagrange': 3.8673322906832537}, 'e7d2a60d': {'loss': 3.925433802238373, 'training_cost': 0.017255306243896484, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.004559264656132888}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.004559264656132888, 'experiment_tag': 'exp', 'time_total_s': 0.02558112144470215, 'loss_lagrange': 3.925433802238373}, 'e7d2a60e': {'loss': 3.875029299919108, 'training_cost': 0.08225297927856445, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005491692419583242}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005491692419583242, 'experiment_tag': 'exp', 'time_total_s': 0.10463213920593262, 'loss_lagrange': 3.875029299919108}, 'e7d2a60f': {'loss': 3.8799155655214927, 'training_cost': 0.1710367202758789, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005400819560603306}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005400819560603306, 'experiment_tag': 'exp', 'time_total_s': 0.19387125968933105, 'loss_lagrange': 3.8799155655214927}, 'e7d2a610': {'loss': 3.864699473593886, 'training_cost': 0.018401622772216797, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.0056842152617478025}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.0056842152617478025, 'experiment_tag': 'exp', 'time_total_s': 0.02675008773803711, 'loss_lagrange': 3.864699473593886}, 'e7d2a611': {'loss': 3.8815617684345805, 'training_cost': 0.030604124069213867, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005370232296413302}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005370232296413302, 'experiment_tag': 'exp', 'time_total_s': 0.04322457313537598, 'loss_lagrange': 3.8815617684345805}, 'e7d2a612': {'loss': 3.8867830670109877, 'training_cost': 0.017615079879760742, 'training_iteration': 0, 'config': {'n_estimators': 8, 'max_leaves': 4, 'learning_rate': 0.005273310814672741}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/learning_rate': 0.005273310814672741, 'experiment_tag': 'exp', 'time_total_s': 0.025716543197631836, 'loss_lagrange': 3.8867830670109877}, 'e7d2a613': {'loss': 3.9956338775935913, 'training_cost': 0.25075483322143555, 'training_iteration': 0, 'config': {'n_estimators': 7, 'max_leaves': 4, 'learning_rate': 0.003752175266028254}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/learning_rate': 0.003752175266028254, 'experiment_tag': 'exp', 'time_total_s': 0.25925326347351074, 'loss_lagrange': 3.9956338775935913}}\n"
     ]
    }
   ],
   "source": [
    "'''Write a evaluation function'''\n",
    "import xgboost as xgb\n",
    "from flaml import tune\n",
    "import time\n",
    "def xgb_simple_obj(X_train, X_test, y_train, y_test, config):\n",
    "    params = config2params(config)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    start_time = time.time()\n",
    "    _n_estimators = params.pop(\"n_estimators\")\n",
    "    model = xgb.train(params, dtrain, _n_estimators)\n",
    "    end_time = time.time()\n",
    "    # get validation loss\n",
    "    from sklearn.metrics import r2_score\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_test_predict = model.predict(dtest)\n",
    "    test_loss = 1.0 - r2_score(y_test, y_test_predict)\n",
    "    return {\"loss\": test_loss, \"training_cost\": end_time-start_time}\n",
    "\n",
    "def config2params(config: dict) -> dict:\n",
    "    params = config.copy()\n",
    "    max_depth = params[\"max_depth\"] = params.get(\"max_depth\", 0)\n",
    "    if max_depth == 0:\n",
    "        params[\"grow_policy\"] = params.get(\"grow_policy\", \"lossguide\")\n",
    "        params[\"tree_method\"] = params.get(\"tree_method\", \"hist\")\n",
    "    # params[\"booster\"] = params.get(\"booster\", \"gbtree\")\n",
    "    params[\"use_label_encoder\"] = params.get(\"use_label_encoder\", False)\n",
    "    if \"n_jobs\" in config:\n",
    "        params[\"nthread\"] = params.pop(\"n_jobs\")\n",
    "    return params\n",
    "\n",
    "def my_model_size(config):\n",
    "    return config[\"n_estimators\"] * config[\"max_leaves\"]\n",
    "\n",
    "'''Tune xgb_obj with configs from the hierarchical search space'''\n",
    "from flaml.data import load_openml_dataset\n",
    "from functools import partial\n",
    "\n",
    "xgb_space = {\n",
    "     \"n_estimators\": tune.randint(lower=4, upper=64),\n",
    "      \"max_leaves\": tune.randint(lower=4, upper=64),\n",
    "      \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),\n",
    "}\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(\n",
    "    dataset_id=537, data_dir=\"./\"\n",
    ")\n",
    "analysis = tune.run(\n",
    "    partial(xgb_simple_obj, X_train, X_test, y_train, y_test),\n",
    "    config=xgb_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    config_constraints = [(my_model_size, \"<=\", 40)],\n",
    "    metric_constraints = [(\"training_cost\", \"<=\", 1)],\n",
    "    num_samples=20,\n",
    ")\n",
    "print(\"analysis\", analysis.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flaml[ray] in /home/qxw5138/tutorial/MS/FLAML (1.0.8)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.22.3)\n",
      "Requirement already satisfied: lightgbm>=2.3.1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (3.3.2)\n",
      "Requirement already satisfied: xgboost>=0.90 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.5.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (0.24.2)\n",
      "Requirement already satisfied: ray[tune]~=1.13 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from flaml[ray]) (1.13.0)\n",
      "Requirement already satisfied: wheel in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from lightgbm>=2.3.1->flaml[ray]) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from pandas>=1.1.4->flaml[ray]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from pandas>=1.1.4->flaml[ray]) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.4->flaml[ray]) (1.16.0)\n",
      "Requirement already satisfied: pyyaml in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (6.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (3.19.4)\n",
      "Requirement already satisfied: attrs in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (21.4.0)\n",
      "Requirement already satisfied: virtualenv in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (20.13.3)\n",
      "Requirement already satisfied: jsonschema in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (4.4.0)\n",
      "Requirement already satisfied: aiosignal in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.2.0)\n",
      "Requirement already satisfied: filelock in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (3.6.0)\n",
      "Requirement already satisfied: frozenlist in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.3.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.0.3)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (1.43.0)\n",
      "Requirement already satisfied: requests in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (2.27.1)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (8.0.4)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (2.5)\n",
      "Requirement already satisfied: tabulate in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from ray[tune]~=1.13->flaml[ray]) (0.8.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from scikit-learn>=0.24->flaml[ray]) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from scikit-learn>=0.24->flaml[ray]) (3.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from jsonschema->ray[tune]~=1.13->flaml[ray]) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from jsonschema->ray[tune]~=1.13->flaml[ray]) (0.18.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema->ray[tune]~=1.13->flaml[ray]) (3.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from requests->ray[tune]~=1.13->flaml[ray]) (1.26.8)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from virtualenv->ray[tune]~=1.13->flaml[ray]) (2.5.1)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages (from virtualenv->ray[tune]~=1.13->flaml[ray]) (0.3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flaml[ray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:13,894]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "2022-12-08 10:34:17,455\tWARNING function_runner.py:603 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2022-12-08 10:34:19,727\tWARNING tune.py:668 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-08 10:34:21 (running for 00:00:01.84)<br>Memory usage on this node: 20.7/503.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/48 CPUs, 0/4 GPUs, 0.0/332.25 GiB heap, 0.0/146.39 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/qxw5138/ray_results/evaluate_config_2022-12-08_10-34-17<br>Number of trials: 1/infinite (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial evaluate_config_ed9cac64 reported score=7224490009.00 with parameters={'x': 3, 'y': 13184}.\n",
      "Trial evaluate_config_ed9cac64 completed. Last result: score=7224490008.999772,evaluation_cost=3e-05,constraint_metric=39552\n",
      "Trial evaluate_config_eeb6493f reported score=7031996448.98 with parameters={'x': 1143, 'y': 74880}.\n",
      "Trial evaluate_config_eeb6493f completed. Last result: score=7031996448.9847355,evaluation_cost=0.01143,constraint_metric=85587840\n",
      "Trial evaluate_config_eeb64941 reported score=7223980036.00 with parameters={'x': 6, 'y': 76053}.\n",
      "Trial evaluate_config_eeb64941 completed. Last result: score=7223980035.999921,evaluation_cost=6e-05,constraint_metric=456318\n",
      "Trial evaluate_config_eeb64943 reported score=6864453903.98 with parameters={'x': 2148, 'y': 95339}.\n",
      "Trial evaluate_config_eeb64943 completed. Last result: score=6864453903.977469,evaluation_cost=0.02148,constraint_metric=204788172\n",
      "Trial evaluate_config_eeb64946 reported score=6303883608.92 with parameters={'x': 5603, 'y': 71825}.\n",
      "Trial evaluate_config_eeb64946 completed. Last result: score=6303883608.921991,evaluation_cost=0.05603,constraint_metric=402435475\n",
      "Trial evaluate_config_eeb6494a reported score=6236260899.93 with parameters={'x': 6030, 'y': 84402}.\n",
      "Trial evaluate_config_eeb6494a completed. Last result: score=6236260899.928556,evaluation_cost=0.0603,constraint_metric=508944060\n",
      "Trial evaluate_config_eeb6494c reported score=6767201168.95 with parameters={'x': 2737, 'y': 59126}.\n",
      "Trial evaluate_config_eeb6494c completed. Last result: score=6767201168.953709,evaluation_cost=0.02737,constraint_metric=161827862\n",
      "Trial evaluate_config_eeb6494f reported score=6413607224.94 with parameters={'x': 4915, 'y': 78908}.\n",
      "Trial evaluate_config_eeb6494f completed. Last result: score=6413607224.937713,evaluation_cost=0.04915,constraint_metric=387832820\n",
      "Trial evaluate_config_eeb64953 reported score=7224830001.00 with parameters={'x': 1, 'y': 51219}.\n",
      "Trial evaluate_config_eeb64953 completed. Last result: score=7224830000.999981,evaluation_cost=1e-05,constraint_metric=51219\n",
      "Trial evaluate_config_eeb64955 reported score=6666722499.89 with parameters={'x': 3350, 'y': 29188}.\n",
      "Trial evaluate_config_eeb64955 completed. Last result: score=6666722499.885227,evaluation_cost=0.0335,constraint_metric=97779800\n",
      "Trial evaluate_config_eeb64958 reported score=7216842304.00 with parameters={'x': 48, 'y': 67413}.\n",
      "Trial evaluate_config_eeb64958 completed. Last result: score=7216842303.999288,evaluation_cost=0.00048,constraint_metric=3235824\n",
      "Trial evaluate_config_eeb6495a reported score=7065915480.98 with parameters={'x': 941, 'y': 51314}.\n",
      "Trial evaluate_config_eeb6495a completed. Last result: score=7065915480.981662,evaluation_cost=0.00941,constraint_metric=48286474\n",
      "Trial evaluate_config_eeb6495c reported score=5733669840.82 with parameters={'x': 9279, 'y': 52165}.\n",
      "Trial evaluate_config_eeb6495c completed. Last result: score=5733669840.822123,evaluation_cost=0.09279,constraint_metric=484039035\n",
      "Trial evaluate_config_eeb64960 reported score=7224830001.00 with parameters={'x': 1, 'y': 62629}.\n",
      "Trial evaluate_config_eeb64960 completed. Last result: score=7224830000.999984,evaluation_cost=1e-05,constraint_metric=62629\n",
      "Trial evaluate_config_eeb64962 reported score=6844583823.96 with parameters={'x': 2268, 'y': 59254}.\n",
      "Trial evaluate_config_eeb64962 completed. Last result: score=6844583823.961724,evaluation_cost=0.02268,constraint_metric=134388072\n",
      "Trial evaluate_config_eeb64964 reported score=4868690175.77 with parameters={'x': 15224, 'y': 65637}.\n",
      "Trial evaluate_config_eeb64964 completed. Last result: score=4868690175.768058,evaluation_cost=0.15224,constraint_metric=999257688\n",
      "Trial evaluate_config_eeb64965 reported score=7157160000.00 with parameters={'x': 400, 'y': 81928}.\n",
      "Trial evaluate_config_eeb64965 completed. Last result: score=7157159999.995117,evaluation_cost=0.004,constraint_metric=32771200\n",
      "Trial evaluate_config_eeb64966 reported score=5589506168.83 with parameters={'x': 10237, 'y': 61006}.\n",
      "Trial evaluate_config_eeb64966 completed. Last result: score=5589506168.832197,evaluation_cost=0.10237,constraint_metric=624518422\n",
      "Trial evaluate_config_eeb64940 reported score=7187648399.99 with parameters={'x': 220, 'y': 22480}.\n",
      "Trial evaluate_config_eeb64940 completed. Last result: score=7187648399.990213,evaluation_cost=0.0022,constraint_metric=4945600\n",
      "Trial evaluate_config_eeb6493e reported score=6219845953.05 with parameters={'x': 6134, 'y': 2076}.\n",
      "Trial evaluate_config_eeb6493e completed. Last result: score=6219845953.0452795,evaluation_cost=0.06134,constraint_metric=12734184\n",
      "Trial evaluate_config_eeb64967 reported score=3888769599.68 with parameters={'x': 22640, 'y': 70268}.\n",
      "Trial evaluate_config_eeb64967 completed. Last result: score=3888769599.677805,evaluation_cost=0.2264,constraint_metric=1590867520\n",
      "Trial evaluate_config_eeb64942 reported score=7224320016.00 with parameters={'x': 4, 'y': 8834}.\n",
      "Trial evaluate_config_eeb64944 reported score=7050625023.98 with parameters={'x': 1032, 'y': 60766}.\n",
      "Trial evaluate_config_eeb64942 completed. Last result: score=7224320015.999547,evaluation_cost=4e-05,constraint_metric=35336\n",
      "Trial evaluate_config_eeb64944 completed. Last result: score=7050625023.983017,evaluation_cost=0.01032,constraint_metric=62710512\n",
      "Trial evaluate_config_efa2fe3d reported score=5860055600.87 with parameters={'x': 8449, 'y': 62958}.\n",
      "Trial evaluate_config_efa2fe3d completed. Last result: score=5860055600.865799,evaluation_cost=0.08449,constraint_metric=531932142\n",
      "Trial evaluate_config_efa2fe3e reported score=5816655288.45 with parameters={'x': 8733, 'y': 15881}.\n",
      "Trial evaluate_config_efa2fe3e completed. Last result: score=5816655288.450098,evaluation_cost=0.08733,constraint_metric=138688773\n",
      "Trial evaluate_config_eeb64945 reported score=7011382755.99 with parameters={'x': 1266, 'y': 88994}.\n",
      "Trial evaluate_config_eeb64945 completed. Last result: score=7011382755.985774,evaluation_cost=0.01266,constraint_metric=112666404\n",
      "Trial evaluate_config_eeb64948 reported score=7153438084.00 with parameters={'x': 422, 'y': 94792}.\n",
      "Trial evaluate_config_efa2fe3c reported score=3314304899.60 with parameters={'x': 27430, 'y': 68316}.\n",
      "Trial evaluate_config_efa2fe3f reported score=5099102463.81 with parameters={'x': 13592, 'y': 70866}.\n",
      "Trial evaluate_config_eeb64948 completed. Last result: score=7153438083.995548,evaluation_cost=0.00422,constraint_metric=40002224\n",
      "Trial evaluate_config_efa2fe3f completed. Last result: score=5099102463.808202,evaluation_cost=0.13592,constraint_metric=963210672\n",
      "Trial evaluate_config_efa2fe3c completed. Last result: score=3314304899.5984836,evaluation_cost=0.2743,constraint_metric=1873907880\n",
      "Trial evaluate_config_efa2fe44 reported score=6838132248.70 with parameters={'x': 2307, 'y': 7739}.\n",
      "Trial evaluate_config_efa2fe44 completed. Last result: score=6838132248.7019,evaluation_cost=0.02307,constraint_metric=17853873\n",
      "Trial evaluate_config_eeb64949 reported score=7095535224.99 with parameters={'x': 765, 'y': 99999}.\n",
      "Trial evaluate_config_eeb64949 completed. Last result: score=7095535224.99235,evaluation_cost=0.00765,constraint_metric=76499235\n",
      "Trial evaluate_config_efa2fe42 reported score=4352964528.71 with parameters={'x': 19023, 'y': 66059}.\n",
      "Trial evaluate_config_efa2fe42 completed. Last result: score=4352964528.71203,evaluation_cost=0.19023,constraint_metric=1256640357\n",
      "Trial evaluate_config_eeb64947 reported score=5486364899.89 with parameters={'x': 10930, 'y': 95886}.\n",
      "Trial evaluate_config_efa2fe41 reported score=3370383024.64 with parameters={'x': 26945, 'y': 74477}.\n",
      "Trial evaluate_config_efa2fe4a reported score=6924236943.86 with parameters={'x': 1788, 'y': 12680}.\n",
      "Trial evaluate_config_eeb64947 completed. Last result: score=5486364899.88601,evaluation_cost=0.1093,constraint_metric=1048033980\n",
      "Trial evaluate_config_efa2fe47 reported score=4516243208.74 with parameters={'x': 17797, 'y': 67910}.\n",
      "Trial evaluate_config_efa2fe41 completed. Last result: score=3370383024.6382103,evaluation_cost=0.26945,constraint_metric=2006782765\n",
      "Trial evaluate_config_efa2fe40 reported score=2236249520.46 with parameters={'x': 37711, 'y': 69670}.\n",
      "Trial evaluate_config_efa2fe4a completed. Last result: score=6924236943.858991,evaluation_cost=0.01788,constraint_metric=22671840\n",
      "Trial evaluate_config_eeb6494d reported score=5818333283.89 with parameters={'x': 8722, 'y': 79600}.\n",
      "Trial evaluate_config_efa2fe40 completed. Last result: score=2236249520.4587197,evaluation_cost=0.37711,constraint_metric=2627325370\n",
      "Trial evaluate_config_efa2fe48 reported score=3951631043.69 with parameters={'x': 22138, 'y': 71605}.\n",
      "Trial evaluate_config_efa2fe47 completed. Last result: score=4516243208.737932,evaluation_cost=0.17797,constraint_metric=1208594270\n",
      "Trial evaluate_config_eeb64950 reported score=6022225608.92 with parameters={'x': 7397, 'y': 89896}.\n",
      "Trial evaluate_config_efa2fe4c reported score=5923765155.92 with parameters={'x': 8034, 'y': 99470}.\n",
      "Trial evaluate_config_efa2fe48 completed. Last result: score=3951631043.6908317,evaluation_cost=0.22138,constraint_metric=1585191490\n",
      "Trial evaluate_config_efa2fe46 reported score=1825169283.38 with parameters={'x': 42278, 'y': 68722}.\n",
      "Trial evaluate_config_efa2fe49 reported score=2602428195.48 with parameters={'x': 33986, 'y': 65027}.\n",
      "Trial evaluate_config_eeb6494e reported score=6533650560.95 with parameters={'x': 4169, 'y': 89204}.\n",
      "Trial evaluate_config_efa2fe4c completed. Last result: score=5923765155.919232,evaluation_cost=0.08034,constraint_metric=799141980\n",
      "Trial evaluate_config_eeb64957 reported score=7161221375.97 with parameters={'x': 376, 'y': 14217}.\n",
      "Trial evaluate_config_eeb64951 reported score=5406660899.86 with parameters={'x': 11470, 'y': 84524}.\n",
      "Trial evaluate_config_eeb6494e completed. Last result: score=6533650560.953264,evaluation_cost=0.04169,constraint_metric=371891476\n",
      "Trial evaluate_config_eeb64952 reported score=4777574399.74 with parameters={'x': 15880, 'y': 60957}.\n",
      "Trial evaluate_config_efa2fe46 completed. Last result: score=1825169283.3847966,evaluation_cost=0.42278,constraint_metric=2905428716\n",
      "Trial evaluate_config_eeb64954 reported score=5601774024.83 with parameters={'x': 10155, 'y': 61252}.\n",
      "Trial evaluate_config_efa2fe4d reported score=4918076640.84 with parameters={'x': 14871, 'y': 92302}.\n",
      "Trial evaluate_config_eeb64952 completed. Last result: score=4777574399.739489,evaluation_cost=0.1588,constraint_metric=967997160\n",
      "Trial evaluate_config_eeb64957 completed. Last result: score=7161221375.973553,evaluation_cost=0.00376,constraint_metric=5345592\n",
      "Trial evaluate_config_eeb64954 completed. Last result: score=5601774024.834209,evaluation_cost=0.10155,constraint_metric=622014060\n",
      "Trial evaluate_config_eeb6494d completed. Last result: score=5818333283.890428,evaluation_cost=0.08722,constraint_metric=694271200\n",
      "Trial evaluate_config_eeb6495b reported score=6988625603.98 with parameters={'x': 1402, 'y': 60104}.\n",
      "Trial evaluate_config_efa2fe4f reported score=3288907800.59 with parameters={'x': 27651, 'y': 67709}.\n",
      "Trial evaluate_config_eeb6495b completed. Last result: score=6988625603.976674,evaluation_cost=0.01402,constraint_metric=84265808\n",
      "Trial evaluate_config_eeb64959 reported score=7206312100.00 with parameters={'x': 110, 'y': 43401}.\n",
      "Trial evaluate_config_eeb6495e reported score=7224660004.00 with parameters={'x': 2, 'y': 30070}.\n",
      "Trial evaluate_config_eeb64950 completed. Last result: score=6022225608.917716,evaluation_cost=0.07397,constraint_metric=664960712\n",
      "Trial evaluate_config_eeb64959 completed. Last result: score=7206312099.997465,evaluation_cost=0.0011,constraint_metric=4774110\n",
      "Trial evaluate_config_efa2fe58 reported score=5535211200.83 with parameters={'x': 10601, 'y': 64103}.\n",
      "Trial evaluate_config_efa2fe4f completed. Last result: score=3288907800.59162,evaluation_cost=0.27651,constraint_metric=1872221559\n",
      "Trial evaluate_config_eeb64956 reported score=2337335715.49 with parameters={'x': 36654, 'y': 71457}.\n",
      "Trial evaluate_config_efa2fe51 reported score=2648955023.50 with parameters={'x': 33532, 'y': 66497}.\n",
      "Trial evaluate_config_efa2fe50 reported score=1813822920.42 with parameters={'x': 42411, 'y': 72843}.\n",
      "Trial evaluate_config_efa2fe4d completed. Last result: score=4918076640.838887,evaluation_cost=0.14871,constraint_metric=1372623042\n",
      "Trial evaluate_config_efa2fe49 completed. Last result: score=2602428195.4773555,evaluation_cost=0.33986,constraint_metric=2210007622\n",
      "Trial evaluate_config_eeb64951 completed. Last result: score=5406660899.864299,evaluation_cost=0.1147,constraint_metric=969490280\n",
      "Trial evaluate_config_eeb6495f reported score=7224660004.00 with parameters={'x': 2, 'y': 82868}.\n",
      "Trial evaluate_config_eeb64963 reported score=6295470335.85 with parameters={'x': 5656, 'y': 38693}.\n",
      "Trial evaluate_config_efa2fe5a reported score=5368053288.88 with parameters={'x': 11733, 'y': 96272}.\n",
      "Trial evaluate_config_efa2fe59 reported score=4625904195.70 with parameters={'x': 16986, 'y': 56279}.\n",
      "Trial evaluate_config_efa2fe57 reported score=3746786520.59 with parameters={'x': 23789, 'y': 57811}.\n",
      "Trial evaluate_config_efa2fe4e reported score=1126877760.28 with parameters={'x': 51431, 'y': 71631}.\n",
      "Trial evaluate_config_efa2fe55 reported score=3037222320.57 with parameters={'x': 29889, 'y': 68859}.\n",
      "Trial evaluate_config_efa2fe4e completed. Last result: score=1126877760.2820008,evaluation_cost=0.51431,constraint_metric=3684053961\n",
      "Trial evaluate_config_efa2fe52 reported score=2948272803.56 with parameters={'x': 30702, 'y': 69892}.\n",
      "Trial evaluate_config_efa2fe58 completed. Last result: score=5535211200.834625,evaluation_cost=0.10601,constraint_metric=679555903\n",
      "Trial evaluate_config_eeb6495d reported score=2725884098.97 with parameters={'x': 32790, 'y': 31924}.\n",
      "Trial evaluate_config_efa2fe5c reported score=3639950223.74 with parameters={'x': 24668, 'y': 93125}.\n",
      "Trial evaluate_config_eeb6495d completed. Last result: score=2725884098.972873,evaluation_cost=0.3279,constraint_metric=1046787960\n",
      "Trial evaluate_config_efa2fe53 reported score=717275523.14 with parameters={'x': 58218, 'y': 67552}.\n",
      "Trial evaluate_config_efa2fe59 completed. Last result: score=4625904195.698182,evaluation_cost=0.16986,constraint_metric=955955094\n",
      "Trial evaluate_config_efa2fe51 completed. Last result: score=2648955023.4957366,evaluation_cost=0.33532,constraint_metric=2229777404\n",
      "Trial evaluate_config_efa2fe53 completed. Last result: score=717275523.138175,evaluation_cost=0.58218,constraint_metric=3932742336\n",
      "Trial evaluate_config_eeb64956 completed. Last result: score=2337335715.487048,evaluation_cost=0.36654,constraint_metric=2619184878\n",
      "Trial evaluate_config_efa2fe5b reported score=4376087103.79 with parameters={'x': 18848, 'y': 88332}.\n",
      "Trial evaluate_config_efa2fe52 completed. Last result: score=2948272803.5607224,evaluation_cost=0.30702,constraint_metric=2145824184\n",
      "Trial evaluate_config_eeb6495e completed. Last result: score=7224660003.999933,evaluation_cost=2e-05,constraint_metric=60140\n",
      "Trial evaluate_config_efa2fe5a completed. Last result: score=5368053288.878126,evaluation_cost=0.11733,constraint_metric=1129559376\n",
      "Trial evaluate_config_efa2fe50 completed. Last result: score=1813822920.4177752,evaluation_cost=0.42411,constraint_metric=3089344473\n",
      "Trial evaluate_config_efa2fe55 completed. Last result: score=3037222320.565939,evaluation_cost=0.29889,constraint_metric=2058126651\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-08 10:34:25 (running for 00:00:05.34)<br>Memory usage on this node: 21.7/503.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 46.0/48 CPUs, 0/4 GPUs, 0.0/332.25 GiB heap, 0.0/146.39 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Current best trial: efa2fe53 with score=717275523.138175 and parameters={'x': 58218, 'y': 67552}<br>Result logdir: /home/qxw5138/ray_results/evaluate_config_2022-12-08_10-34-17<br>Number of trials: 88/infinite (1 PENDING, 23 RUNNING, 64 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial evaluate_config_efa2fe5d reported score=5781321224.90 with parameters={'x': 8965, 'y': 91479}.\n",
      "Trial evaluate_config_efa2fe4b reported score=16908541.34 with parameters={'x': 89112, 'y': 33454}.\n",
      "Trial evaluate_config_efa2fe57 completed. Last result: score=3746786520.588504,evaluation_cost=0.23789,constraint_metric=1375265879\n",
      "Trial evaluate_config_efa2fe4b completed. Last result: score=16908541.336282656,evaluation_cost=0.89112,constraint_metric=2981152848\n",
      "Trial evaluate_config_efa2fe5d completed. Last result: score=5781321224.901999,evaluation_cost=0.08965,constraint_metric=820109235\n",
      "Trial evaluate_config_efa2fe5f reported score=1929405624.41 with parameters={'x': 41075, 'y': 70084}.\n",
      "Trial evaluate_config_eeb6495f completed. Last result: score=7224660003.999976,evaluation_cost=2e-05,constraint_metric=165736\n",
      "Trial evaluate_config_efa2fe5f completed. Last result: score=1929405624.4139175,evaluation_cost=0.41075,constraint_metric=2878700300\n",
      "Trial evaluate_config_efa2fe54 reported score=634939203.13 with parameters={'x': 59802, 'y': 68585}.\n",
      "Trial evaluate_config_eeb64961 reported score=2212573443.16 with parameters={'x': 37962, 'y': 45076}.\n",
      "Trial evaluate_config_efa2fe45 reported score=2697659719.62 with parameters={'x': 33061, 'y': 24023}.\n",
      "Trial evaluate_config_efa2fe54 completed. Last result: score=634939203.1280601,evaluation_cost=0.59802,constraint_metric=4101520170\n",
      "Trial evaluate_config_eeb64961 completed. Last result: score=2212573443.157822,evaluation_cost=0.37962,constraint_metric=1711175112\n",
      "Trial evaluate_config_efa2fe5c completed. Last result: score=3639950223.735109,evaluation_cost=0.24668,constraint_metric=2297207500\n",
      "Trial evaluate_config_efa2fe63 reported score=1491040995.33 with parameters={'x': 46386, 'y': 69007}.\n",
      "Trial evaluate_config_efa2fe60 reported score=1815271235.44 with parameters={'x': 42394, 'y': 75616}.\n",
      "Trial evaluate_config_efa2fe62 reported score=782656575.23 with parameters={'x': 57024, 'y': 74255}.\n",
      "Trial evaluate_config_efa2fe5e reported score=1698181680.42 with parameters={'x': 43791, 'y': 75602}.\n",
      "Trial evaluate_config_efa2fe5b completed. Last result: score=4376087103.786623,evaluation_cost=0.18848,constraint_metric=1664881536\n",
      "Trial evaluate_config_efa2fe45 completed. Last result: score=2697659719.6237774,evaluation_cost=0.33061,constraint_metric=794224403\n",
      "Trial evaluate_config_efa2fe63 completed. Last result: score=1491040995.3278074,evaluation_cost=0.46386,constraint_metric=3200958702\n",
      "Trial evaluate_config_eeb64963 completed. Last result: score=6295470335.853824,evaluation_cost=0.05656,constraint_metric=218847608\n",
      "Trial evaluate_config_efa2fe60 completed. Last result: score=1815271235.4393516,evaluation_cost=0.42394,constraint_metric=3205664704\n",
      "Trial evaluate_config_efa2fe5e completed. Last result: score=1698181680.4207692,evaluation_cost=0.43791,constraint_metric=3310687182\n",
      "Trial evaluate_config_efa2fe64 reported score=1359323160.31 with parameters={'x': 48131, 'y': 69779}.\n",
      "Trial evaluate_config_efa2fe68 reported score=1483174142.70 with parameters={'x': 46488, 'y': 35624}.\n",
      "Trial evaluate_config_efa2fe66 reported score=103428898.86 with parameters={'x': 74830, 'y': 65838}.\n",
      "Trial evaluate_config_efa2fe67 reported score=1576566435.35 with parameters={'x': 45294, 'y': 69266}.\n",
      "Trial evaluate_config_efa2fe62 completed. Last result: score=782656575.2320517,evaluation_cost=0.57024,constraint_metric=4234317120\n",
      "Trial evaluate_config_f0c474c7 reported score=554885135.38 with parameters={'x': 61444, 'y': 99016}.\n",
      "Trial evaluate_config_efa2fe66 completed. Last result: score=103428898.86342233,evaluation_cost=0.7483,constraint_metric=4926657540\n",
      "Trial evaluate_config_efa2fe68 completed. Last result: score=1483174142.6950371,evaluation_cost=0.46488,constraint_metric=1656088512\n",
      "Trial evaluate_config_efa2fe61 reported score=1125468303.25 with parameters={'x': 51452, 'y': 68858}.\n",
      "Trial evaluate_config_efa2fe65 reported score=212605559.92 with parameters={'x': 70419, 'y': 65325}.\n",
      "Trial evaluate_config_efa2fe65 completed. Last result: score=212605559.92202067,evaluation_cost=0.70419,constraint_metric=4600121175\n",
      "Trial evaluate_config_f0c474c6 reported score=37970242.06 with parameters={'x': 78838, 'y': 40668}.\n",
      "Trial evaluate_config_efa2fe61 completed. Last result: score=1125468303.2527812,evaluation_cost=0.51452,constraint_metric=3542881816\n",
      "Trial evaluate_config_efa2fe64 completed. Last result: score=1359323160.3102367,evaluation_cost=0.48131,constraint_metric=3358533049\n",
      "Trial evaluate_config_efa2fe69 reported score=11068926.88 with parameters={'x': 88327, 'y': 41660}.\n",
      "Trial evaluate_config_f0c474c7 completed. Last result: score=554885135.3794538,evaluation_cost=0.61444,constraint_metric=6083939104\n",
      "Trial evaluate_config_efa2fe69 completed. Last result: score=11068926.87981277,evaluation_cost=0.88327,constraint_metric=3679702820\n",
      "Trial evaluate_config_f0c474cc reported score=184117759.16 with parameters={'x': 71431, 'y': 38833}.\n",
      "Trial evaluate_config_f0c474ca reported score=14730243.18 with parameters={'x': 81162, 'y': 99223}.\n",
      "Trial evaluate_config_efa2fe67 completed. Last result: score=1576566435.346086,evaluation_cost=0.45294,constraint_metric=3137334204\n",
      "Trial evaluate_config_f0c474cf reported score=292307407.32 with parameters={'x': 67903, 'y': 40361}.\n",
      "Trial evaluate_config_f0c474c9 reported score=25280.15 with parameters={'x': 84841, 'y': 99590}.\n",
      "Trial evaluate_config_f0c474cf completed. Last result: score=292307407.3176086,evaluation_cost=0.67903,constraint_metric=2740632983\n",
      "Trial evaluate_config_f0c474c8 reported score=196812839.92 with parameters={'x': 99029, 'y': 91919}.\n",
      "Trial evaluate_config_f0c474c9 completed. Last result: score=25280.148097198515,evaluation_cost=0.84841,constraint_metric=8449315190\n",
      "Trial evaluate_config_f0c474cc completed. Last result: score=184117759.16055933,evaluation_cost=0.71431,constraint_metric=2773880023\n",
      "Trial evaluate_config_f0c474c6 completed. Last result: score=37970242.06142422,evaluation_cost=0.78838,constraint_metric=3206183784\n",
      "Trial evaluate_config_f0c474d9 reported score=2388276899.13 with parameters={'x': 36130, 'y': 41713}.\n",
      "Trial evaluate_config_f0c474c8 completed. Last result: score=196812839.9226493,evaluation_cost=0.99029,constraint_metric=9102646651\n",
      "Trial evaluate_config_f0c474ca completed. Last result: score=14730243.18202433,evaluation_cost=0.81162,constraint_metric=8053137126\n",
      "Trial evaluate_config_f0c474cb reported score=93392894.98 with parameters={'x': 94664, 'y': 92883}.\n",
      "Trial evaluate_config_f0c474cd reported score=39476086.73 with parameters={'x': 91283, 'y': 40217}.\n",
      "Trial evaluate_config_f0c474cd completed. Last result: score=39476086.73023846,evaluation_cost=0.91283,constraint_metric=3671128411\n",
      "Trial evaluate_config_f0c474d9 completed. Last result: score=2388276899.133843,evaluation_cost=0.3613,constraint_metric=1507090690\n",
      "Trial evaluate_config_f0c474d5 reported score=288422287.12 with parameters={'x': 68017, 'y': 36173}.\n",
      "Trial evaluate_config_f0c474cb completed. Last result: score=93392894.98082533,evaluation_cost=0.94664,constraint_metric=8792676312\n",
      "Trial evaluate_config_f0c474e1 reported score=4623592008.29 with parameters={'x': 17003, 'y': 24030}.\n",
      "Trial evaluate_config_f0c474d1 reported score=76090726.62 with parameters={'x': 93723, 'y': 39359}.\n",
      "Trial evaluate_config_f0c474d2 reported score=321556622.21 with parameters={'x': 67068, 'y': 37525}.\n",
      "Trial evaluate_config_f0c474e4 reported score=4717766594.98 with parameters={'x': 16314, 'y': 15944}.\n",
      "Trial evaluate_config_f0c474ce reported score=135443041.45 with parameters={'x': 96638, 'y': 37895}.\n",
      "Trial evaluate_config_f0c474e4 completed. Last result: score=4717766594.976794,evaluation_cost=0.16314,constraint_metric=260110416\n",
      "Trial evaluate_config_f0c474ce completed. Last result: score=135443041.44984826,evaluation_cost=0.96638,constraint_metric=3662097010\n",
      "Trial evaluate_config_f0c474d2 completed. Last result: score=321556622.2127115,evaluation_cost=0.67068,constraint_metric=2516726700\n",
      "Trial evaluate_config_f0c474d7 reported score=8838726.97 with parameters={'x': 82027, 'y': 40434}.\n",
      "Trial evaluate_config_f0c474e2 reported score=3777577442.82 with parameters={'x': 23538, 'y': 19975}.\n",
      "Trial evaluate_config_f0c474d0 reported score=112466022.66 with parameters={'x': 95605, 'y': 40881}.\n",
      "Trial evaluate_config_f0c474e2 completed. Last result: score=3777577442.821627,evaluation_cost=0.23538,constraint_metric=470171550\n",
      "Trial evaluate_config_f0c474dd reported score=671846398.50 with parameters={'x': 59080, 'y': 39289}.\n",
      "Trial evaluate_config_f0c474d3 reported score=31764493.81 with parameters={'x': 79364, 'y': 36213}.\n",
      "Trial evaluate_config_f0c474e6 reported score=4808590335.82 with parameters={'x': 15656, 'y': 88184}.\n",
      "Trial evaluate_config_f0c474d5 completed. Last result: score=288422287.1196749,evaluation_cost=0.68017,constraint_metric=2460378941\n",
      "Trial evaluate_config_f0c474d1 completed. Last result: score=76090726.61876573,evaluation_cost=0.93723,constraint_metric=3688843557\n",
      "Trial evaluate_config_f0c474e6 completed. Last result: score=4808590335.822462,evaluation_cost=0.15656,constraint_metric=1380608704\n",
      "Trial evaluate_config_f0c474d3 completed. Last result: score=31764493.808411345,evaluation_cost=0.79364,constraint_metric=2874008532\n",
      "Trial evaluate_config_f0c474ed reported score=6162721008.93 with parameters={'x': 6497, 'y': 87464}.\n",
      "Trial evaluate_config_f0c474d6 reported score=18455613.58 with parameters={'x': 89296, 'y': 36948}.\n",
      "Trial evaluate_config_f0c474e3 reported score=4698142848.23 with parameters={'x': 16457, 'y': 21333}.\n",
      "Trial evaluate_config_f0c474ed completed. Last result: score=6162721008.925718,evaluation_cost=0.06497,constraint_metric=568253608\n",
      "Trial evaluate_config_f0c474e8 reported score=4753275134.87 with parameters={'x': 16056, 'y': 14216}.\n",
      "Trial evaluate_config_f0c474e7 reported score=5888874120.61 with parameters={'x': 8261, 'y': 21200}.\n",
      "Trial evaluate_config_f0c474ef reported score=6692712480.40 with parameters={'x': 3191, 'y': 5285}.\n",
      "Trial evaluate_config_f0c474d4 reported score=6651238.85 with parameters={'x': 82421, 'y': 38352}.\n",
      "Trial evaluate_config_f0c474e8 completed. Last result: score=4753275134.870568,evaluation_cost=0.16056,constraint_metric=228252096\n",
      "Trial evaluate_config_f0c474d4 completed. Last result: score=6651238.850933459,evaluation_cost=0.82421,constraint_metric=3161010192\n",
      "Trial evaluate_config_f0c474dd completed. Last result: score=671846398.4962713,evaluation_cost=0.5908,constraint_metric=2321194120\n",
      "Trial evaluate_config_f0c474da reported score=29571841.81 with parameters={'x': 90438, 'y': 41375}.\n",
      "Trial evaluate_config_f0c474ec reported score=6348743039.78 with parameters={'x': 5321, 'y': 4372}.\n",
      "Trial evaluate_config_f0c474ec completed. Last result: score=6348743039.782937,evaluation_cost=0.05321,constraint_metric=23263412\n",
      "Trial evaluate_config_f0c474db reported score=113209597.74 with parameters={'x': 95640, 'y': 42265}.\n",
      "Trial evaluate_config_f0c474ea reported score=4160637008.12 with parameters={'x': 20497, 'y': 23274}.\n",
      "Trial evaluate_config_f0c474e1 completed. Last result: score=4623592008.292426,evaluation_cost=0.17003,constraint_metric=408582090\n",
      "Trial evaluate_config_f0c474d7 completed. Last result: score=8838726.971336005,evaluation_cost=0.82027,constraint_metric=3316679718\n",
      "Trial evaluate_config_f0c474f2 reported score=7221600400.00 with parameters={'x': 20, 'y': 24597}.\n",
      "Trial evaluate_config_f0c474d6 completed. Last result: score=18455613.583198007,evaluation_cost=0.89296,constraint_metric=3299308608\n",
      "Trial evaluate_config_f0c474df reported score=112296406.78 with parameters={'x': 95597, 'y': 43139}.\n",
      "Trial evaluate_config_f0c474ea completed. Last result: score=4160637008.1193175,evaluation_cost=0.20497,constraint_metric=477047178\n",
      "Trial evaluate_config_f0c474db completed. Last result: score=113209597.73713474,evaluation_cost=0.9564,constraint_metric=4042224600\n",
      "Trial evaluate_config_f0c474d8 reported score=390378562.33 with parameters={'x': 65242, 'y': 39045}.\n",
      "Trial evaluate_config_f0c474da completed. Last result: score=29571841.81418731,evaluation_cost=0.90438,constraint_metric=3741872250\n",
      "Trial evaluate_config_f0c474d0 completed. Last result: score=112466022.66138303,evaluation_cost=0.95605,constraint_metric=3908428005\n",
      "Trial evaluate_config_f0c474ee reported score=6625959999.40 with parameters={'x': 3600, 'y': 5967}.\n",
      "Trial evaluate_config_f0c474dc reported score=2368518.95 with parameters={'x': 83461, 'y': 40623}.\n",
      "Trial evaluate_config_f1f8015b reported score=6635568680.84 with parameters={'x': 3541, 'y': 22662}.\n",
      "Trial evaluate_config_f0c474d8 completed. Last result: score=390378562.3290562,evaluation_cost=0.65242,constraint_metric=2547373890\n",
      "Trial evaluate_config_f1f8015b completed. Last result: score=6635568680.843747,evaluation_cost=0.03541,constraint_metric=80246142\n",
      "Trial evaluate_config_f0c474de reported score=60933633.73 with parameters={'x': 92806, 'y': 40936}.\n",
      "Trial evaluate_config_f0c474dc completed. Last result: score=2368518.9454742386,evaluation_cost=0.83461,constraint_metric=3390436203\n",
      "Trial evaluate_config_f0c474e0 reported score=44555622.81 with parameters={'x': 91675, 'y': 41884}.\n",
      "Trial evaluate_config_f0c474e9 reported score=3962576599.96 with parameters={'x': 22051, 'y': 21305}.\n",
      "Trial evaluate_config_f0c474e0 completed. Last result: score=44555622.8112167,evaluation_cost=0.91675,constraint_metric=3839715700\n",
      "Trial evaluate_config_f0c474e9 completed. Last result: score=3962576599.964985,evaluation_cost=0.22051,constraint_metric=469796555\n",
      "Trial evaluate_config_f0c474f2 completed. Last result: score=7221600399.9991865,evaluation_cost=0.0002,constraint_metric=491940\n",
      "Trial evaluate_config_f0c474de completed. Last result: score=60933633.732900135,evaluation_cost=0.92806,constraint_metric=3799106416\n",
      "Trial evaluate_config_f1f8015a reported score=3568389694.98 with parameters={'x': 25264, 'y': 24823}.\n",
      "Trial evaluate_config_f1f80163 reported score=7222110289.00 with parameters={'x': 17, 'y': 47043}.\n",
      "Trial evaluate_config_f1f8015c reported score=7222280256.00 with parameters={'x': 16, 'y': 22871}.\n",
      "Trial evaluate_config_f0c474f0 reported score=6393921443.81 with parameters={'x': 5038, 'y': 26841}.\n",
      "Trial evaluate_config_f0c474e7 completed. Last result: score=5888874120.610331,evaluation_cost=0.08261,constraint_metric=175133200\n",
      "Trial evaluate_config_f1f80161 reported score=2860859168.33 with parameters={'x': 31513, 'y': 47078}.\n",
      "Trial evaluate_config_f1f8015a completed. Last result: score=3568389694.982234,evaluation_cost=0.25264,constraint_metric=627128272\n",
      "Trial evaluate_config_f0c474ef completed. Last result: score=6692712480.396215,evaluation_cost=0.03191,constraint_metric=16864435\n",
      "Trial evaluate_config_f0c474df completed. Last result: score=112296406.78397737,evaluation_cost=0.95597,constraint_metric=4123958983\n",
      "Trial evaluate_config_f1f80163 completed. Last result: score=7222110288.999639,evaluation_cost=0.00017,constraint_metric=799731\n",
      "Trial evaluate_config_f1f80160 reported score=7218031681.00 with parameters={'x': 41, 'y': 45993}.\n",
      "Trial evaluate_config_f1f80162 reported score=2427828528.26 with parameters={'x': 35727, 'y': 48537}.\n",
      "Trial evaluate_config_f0c474f0 completed. Last result: score=6393921443.812302,evaluation_cost=0.05038,constraint_metric=135224958\n",
      "Trial evaluate_config_f0c474f1 reported score=6456443903.81 with parameters={'x': 4648, 'y': 24910}.\n",
      "Trial evaluate_config_f1f80162 completed. Last result: score=2427828528.263922,evaluation_cost=0.35727,constraint_metric=1734081399\n",
      "Trial evaluate_config_f1f80164 reported score=7223300100.00 with parameters={'x': 10, 'y': 47977}.\n",
      "Trial evaluate_config_f1f80160 completed. Last result: score=7218031680.999108,evaluation_cost=0.00041,constraint_metric=1885713\n",
      "Trial evaluate_config_f0c474e5 reported score=4737431240.81 with parameters={'x': 16171, 'y': 83352}.\n",
      "Trial evaluate_config_f1f8015f reported score=3072484899.37 with parameters={'x': 29570, 'y': 47224}.\n",
      "Trial evaluate_config_f1f8015e reported score=7220920576.00 with parameters={'x': 24, 'y': 55505}.\n",
      "Trial evaluate_config_f0c474e5 completed. Last result: score=4737431240.805991,evaluation_cost=0.16171,constraint_metric=1347885192\n",
      "Trial evaluate_config_f0c474ee completed. Last result: score=6625959999.396682,evaluation_cost=0.036,constraint_metric=21481200\n",
      "Trial evaluate_config_f1f8015e completed. Last result: score=7220920575.999568,evaluation_cost=0.00024,constraint_metric=1332120\n",
      "Trial evaluate_config_f1f8015f completed. Last result: score=3072484899.3738356,evaluation_cost=0.2957,constraint_metric=1396413680\n",
      "Trial evaluate_config_f1f80164 completed. Last result: score=7223300099.999791,evaluation_cost=0.0001,constraint_metric=479770\n",
      "Trial evaluate_config_f1f80165 reported score=2909415720.34 with parameters={'x': 31061, 'y': 47124}.\n",
      "Trial evaluate_config_f1f80161 completed. Last result: score=2860859168.3306217,evaluation_cost=0.31513,constraint_metric=1483569014\n",
      "Trial evaluate_config_f0c474eb reported score=4667622399.26 with parameters={'x': 16680, 'y': 22445}.\n",
      "Trial evaluate_config_f1f8016a reported score=2282928399.22 with parameters={'x': 37220, 'y': 47596}.\n",
      "Trial evaluate_config_f1f8016f reported score=2276816655.33 with parameters={'x': 37284, 'y': 55677}.\n",
      "Trial evaluate_config_f1f8016e reported score=1931338808.14 with parameters={'x': 41053, 'y': 47569}.\n",
      "Trial evaluate_config_f1f8015c completed. Last result: score=7222280255.9993,evaluation_cost=0.00016,constraint_metric=365936\n",
      "Trial evaluate_config_f1f8016a completed. Last result: score=2282928399.2180014,evaluation_cost=0.3722,constraint_metric=1771523120\n",
      "Trial evaluate_config_f1f80169 reported score=2265855200.20 with parameters={'x': 37399, 'y': 46873}.\n",
      "Trial evaluate_config_f1f8016f completed. Last result: score=2276816655.330352,evaluation_cost=0.37284,constraint_metric=2075861268\n",
      "Trial evaluate_config_f1f8016c reported score=2202988095.18 with parameters={'x': 38064, 'y': 46369}.\n",
      "Trial evaluate_config_f1f80165 completed. Last result: score=2909415720.3408666,evaluation_cost=0.31061,constraint_metric=1463718564\n",
      "Trial evaluate_config_f1f8015d reported score=3149678883.48 with parameters={'x': 28878, 'y': 56067}.\n",
      "Trial evaluate_config_f1f80167 reported score=3004903488.35 with parameters={'x': 30183, 'y': 46563}.\n",
      "Trial evaluate_config_f1f80170 reported score=1905846335.13 with parameters={'x': 41344, 'y': 47331}.\n",
      "Trial evaluate_config_f1f80169 completed. Last result: score=2265855200.202121,evaluation_cost=0.37399,constraint_metric=1753003327\n",
      "Trial evaluate_config_f1f8015d completed. Last result: score=3149678883.4849377,evaluation_cost=0.28878,constraint_metric=1619102826\n",
      "Trial evaluate_config_f1f8016d reported score=1842813183.12 with parameters={'x': 42072, 'y': 48023}.\n",
      "Trial evaluate_config_f1f80170 completed. Last result: score=1905846335.1264923,evaluation_cost=0.41344,constraint_metric=1956852864\n",
      "Trial evaluate_config_f1f80167 completed. Last result: score=3004903488.3517814,evaluation_cost=0.30183,constraint_metric=1405411029\n",
      "Trial evaluate_config_f1f80174 reported score=2219634768.30 with parameters={'x': 37887, 'y': 54027}.\n",
      "Trial evaluate_config_f0c474f1 completed. Last result: score=6456443903.813408,evaluation_cost=0.04648,constraint_metric=115781680\n",
      "Trial evaluate_config_f1f8016d completed. Last result: score=1842813183.1239197,evaluation_cost=0.42072,constraint_metric=2020423656\n",
      "Trial evaluate_config_f1f80173 reported score=1676574915.23 with parameters={'x': 44054, 'y': 57120}.\n",
      "Trial evaluate_config_f0c474e3 completed. Last result: score=4698142848.228566,evaluation_cost=0.16457,constraint_metric=351077181\n",
      "Trial evaluate_config_f1f8016b reported score=2475560024.26 with parameters={'x': 35245, 'y': 47536}.\n",
      "Trial evaluate_config_f1f80166 reported score=2529486435.27 with parameters={'x': 34706, 'y': 47676}.\n",
      "Trial evaluate_config_f1f80173 completed. Last result: score=1676574915.2287464,evaluation_cost=0.44054,constraint_metric=2516364480\n",
      "Trial evaluate_config_f1f80168 reported score=2253685728.22 with parameters={'x': 37527, 'y': 47995}.\n",
      "Trial evaluate_config_f0c474eb completed. Last result: score=4667622399.25685,evaluation_cost=0.1668,constraint_metric=374382600\n",
      "Trial evaluate_config_f1f80172 reported score=1866585614.68 with parameters={'x': 41796, 'y': 31676}.\n",
      "Trial evaluate_config_f1f80178 reported score=1618211527.53 with parameters={'x': 44773, 'y': 30536}.\n",
      "Trial evaluate_config_f1f80179 reported score=1296432034.55 with parameters={'x': 48994, 'y': 33673}.\n",
      "Trial evaluate_config_f1f80168 completed. Last result: score=2253685728.2181063,evaluation_cost=0.37527,constraint_metric=1801108365\n",
      "Trial evaluate_config_f1f8016c completed. Last result: score=2202988095.1791067,evaluation_cost=0.38064,constraint_metric=1764989616\n",
      "Trial evaluate_config_f1f8017c reported score=1401154622.48 with parameters={'x': 47568, 'y': 31266}.\n",
      "Trial evaluate_config_f1f8016e completed. Last result: score=1931338808.13698,evaluation_cost=0.41053,constraint_metric=1952850157\n",
      "Trial evaluate_config_f1f8017a reported score=1419556327.54 with parameters={'x': 47323, 'y': 32435}.\n",
      "Trial evaluate_config_f1f8017c completed. Last result: score=1401154622.478603,evaluation_cost=0.47568,constraint_metric=1487261088\n",
      "Trial evaluate_config_f1f8017a completed. Last result: score=1419556327.5409896,evaluation_cost=0.47323,constraint_metric=1534921505\n",
      "Trial evaluate_config_f1f8017d reported score=1255426622.52 with parameters={'x': 49568, 'y': 33382}.\n",
      "Trial evaluate_config_f1f80176 reported score=1538600624.15 with parameters={'x': 45775, 'y': 53747}.\n",
      "Trial evaluate_config_f1f80179 completed. Last result: score=1296432034.5450063,evaluation_cost=0.48994,constraint_metric=1649774962\n",
      "Trial evaluate_config_f1f8017b reported score=1222201598.48 with parameters={'x': 50040, 'y': 32867}.\n",
      "Trial evaluate_config_f1f8016b completed. Last result: score=2475560024.258562,evaluation_cost=0.35245,constraint_metric=1675406320\n",
      "Trial evaluate_config_f1f80175 reported score=1633210567.70 with parameters={'x': 44587, 'y': 34169}.\n",
      "Trial evaluate_config_f1f8017b completed. Last result: score=1222201598.4775002,evaluation_cost=0.5004,constraint_metric=1644664680\n",
      "Trial evaluate_config_f1f80172 completed. Last result: score=1866585614.6805153,evaluation_cost=0.41796,constraint_metric=1323930096\n",
      "Trial evaluate_config_f1f8017d completed. Last result: score=1255426622.515128,evaluation_cost=0.49568,constraint_metric=1654678976\n",
      "Trial evaluate_config_f1f80171 reported score=1887207362.70 with parameters={'x': 41558, 'y': 31970}.\n",
      "Trial evaluate_config_f1f80180 reported score=1103568398.44 with parameters={'x': 51780, 'y': 33283}.\n",
      "Trial evaluate_config_f1f80178 completed. Last result: score=1618211527.5337634,evaluation_cost=0.44773,constraint_metric=1367188328\n",
      "Trial evaluate_config_f1f80177 reported score=1661622167.68 with parameters={'x': 44237, 'y': 33604}.\n",
      "Trial evaluate_config_f1f80166 completed. Last result: score=2529486435.2720447,evaluation_cost=0.34706,constraint_metric=1654643256\n",
      "Trial evaluate_config_f1f80182 reported score=1060674622.42 with parameters={'x': 52432, 'y': 33179}.\n",
      "Trial evaluate_config_f1f8017f reported score=914336642.35 with parameters={'x': 54762, 'y': 33246}.\n",
      "Trial evaluate_config_f1f80177 completed. Last result: score=1661622167.6835794,evaluation_cost=0.44237,constraint_metric=1486540148\n",
      "Trial evaluate_config_f1f80176 completed. Last result: score=1538600624.1483245,evaluation_cost=0.45775,constraint_metric=2460268925\n",
      "Trial evaluate_config_f1f80185 reported score=1018248098.42 with parameters={'x': 53090, 'y': 33568}.\n",
      "Trial evaluate_config_f1f8017e reported score=1205408959.43 with parameters={'x': 50281, 'y': 32089}.\n",
      "Trial evaluate_config_f1f8017f completed. Last result: score=914336642.3528244,evaluation_cost=0.54762,constraint_metric=1820617452\n",
      "Trial evaluate_config_f1f80171 completed. Last result: score=1887207362.7000937,evaluation_cost=0.41558,constraint_metric=1328609260\n",
      "Trial evaluate_config_f32cfd2b reported score=1099519279.47 with parameters={'x': 51841, 'y': 33881}.\n",
      "Trial evaluate_config_f1f80181 reported score=1089660098.37 with parameters={'x': 51990, 'y': 31913}.\n",
      "Trial evaluate_config_f32cfd28 reported score=1008253007.36 with parameters={'x': 53247, 'y': 32521}.\n",
      "Trial evaluate_config_f32cfd2a reported score=879715598.33 with parameters={'x': 55340, 'y': 33105}.\n",
      "Trial evaluate_config_f1f80175 completed. Last result: score=1633210567.6951036,evaluation_cost=0.44587,constraint_metric=1523493203\n",
      "Trial evaluate_config_f1f80181 completed. Last result: score=1089660098.3708832,evaluation_cost=0.5199,constraint_metric=1659156870\n",
      "Trial evaluate_config_f1f80182 completed. Last result: score=1060674622.4197233,evaluation_cost=0.52432,constraint_metric=1739641328\n",
      "Trial evaluate_config_f1f8017e completed. Last result: score=1205408959.4330769,evaluation_cost=0.50281,constraint_metric=1613467009\n",
      "Trial evaluate_config_f1f80174 completed. Last result: score=2219634768.2987394,evaluation_cost=0.37887,constraint_metric=2046920949\n",
      "Trial evaluate_config_f1f80185 completed. Last result: score=1018248098.4184343,evaluation_cost=0.5309,constraint_metric=1782125120\n",
      "Trial evaluate_config_f32cfd2b completed. Last result: score=1099519279.4699094,evaluation_cost=0.51841,constraint_metric=1756424921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 10:34:30,080\tINFO stopper.py:363 -- Reached timeout of 10 seconds. Stopping all trials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial evaluate_config_f1f80183 reported score=955366279.22 with parameters={'x': 54091, 'y': 30467}.\n",
      "Trial evaluate_config_f1f80180 completed. Last result: score=1103568398.4442508,evaluation_cost=0.5178,constraint_metric=1723393740\n",
      "Trial evaluate_config_f32cfd2e reported score=466344023.17 with parameters={'x': 63405, 'y': 34639}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-08 10:34:30 (running for 00:00:10.62)<br>Memory usage on this node: 20.8/503.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/48 CPUs, 0/4 GPUs, 0.0/332.25 GiB heap, 0.0/146.39 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Current best trial: f0c474c9 with score=25280.148097198515 and parameters={'x': 84841, 'y': 99590}<br>Result logdir: /home/qxw5138/ray_results/evaluate_config_2022-12-08_10-34-17<br>Number of trials: 199/infinite (199 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-08 10:34:30 (running for 00:00:10.63)<br>Memory usage on this node: 20.8/503.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/48 CPUs, 0/4 GPUs, 0.0/332.25 GiB heap, 0.0/146.39 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Current best trial: f0c474c9 with score=25280.148097198515 and parameters={'x': 84841, 'y': 99590}<br>Result logdir: /home/qxw5138/ray_results/evaluate_config_2022-12-08_10-34-17<br>Number of trials: 199/infinite (199 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                    </th><th style=\"text-align: right;\">    x</th><th style=\"text-align: right;\">    y</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          score</th><th style=\"text-align: right;\">  evaluation_cost</th><th style=\"text-align: right;\">  constraint_metric</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>evaluate_config_ed9cac64</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">    3</td><td style=\"text-align: right;\">13184</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000496149</td><td style=\"text-align: right;\">    7.22449e+09</td><td style=\"text-align: right;\">          3e-05  </td><td style=\"text-align: right;\">              39552</td></tr>\n",
       "<tr><td>evaluate_config_eeb6493e</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\"> 6134</td><td style=\"text-align: right;\"> 2076</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.061902   </td><td style=\"text-align: right;\">    6.21985e+09</td><td style=\"text-align: right;\">          0.06134</td><td style=\"text-align: right;\">           12734184</td></tr>\n",
       "<tr><td>evaluate_config_eeb6493f</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 1143</td><td style=\"text-align: right;\">74880</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0118706  </td><td style=\"text-align: right;\">    7.032e+09  </td><td style=\"text-align: right;\">          0.01143</td><td style=\"text-align: right;\">           85587840</td></tr>\n",
       "<tr><td>evaluate_config_eeb64940</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">  220</td><td style=\"text-align: right;\">22480</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00257039 </td><td style=\"text-align: right;\">    7.18765e+09</td><td style=\"text-align: right;\">          0.0022 </td><td style=\"text-align: right;\">            4945600</td></tr>\n",
       "<tr><td>evaluate_config_eeb64941</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">    6</td><td style=\"text-align: right;\">76053</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00046587 </td><td style=\"text-align: right;\">    7.22398e+09</td><td style=\"text-align: right;\">          6e-05  </td><td style=\"text-align: right;\">             456318</td></tr>\n",
       "<tr><td>evaluate_config_eeb64942</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">    4</td><td style=\"text-align: right;\"> 8834</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000368118</td><td style=\"text-align: right;\">    7.22432e+09</td><td style=\"text-align: right;\">          4e-05  </td><td style=\"text-align: right;\">              35336</td></tr>\n",
       "<tr><td>evaluate_config_eeb64943</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 2148</td><td style=\"text-align: right;\">95339</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0218968  </td><td style=\"text-align: right;\">    6.86445e+09</td><td style=\"text-align: right;\">          0.02148</td><td style=\"text-align: right;\">          204788172</td></tr>\n",
       "<tr><td>evaluate_config_eeb64944</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\"> 1032</td><td style=\"text-align: right;\">60766</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0106943  </td><td style=\"text-align: right;\">    7.05063e+09</td><td style=\"text-align: right;\">          0.01032</td><td style=\"text-align: right;\">           62710512</td></tr>\n",
       "<tr><td>evaluate_config_eeb64945</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\"> 1266</td><td style=\"text-align: right;\">88994</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0130966  </td><td style=\"text-align: right;\">    7.01138e+09</td><td style=\"text-align: right;\">          0.01266</td><td style=\"text-align: right;\">          112666404</td></tr>\n",
       "<tr><td>evaluate_config_eeb64946</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 5603</td><td style=\"text-align: right;\">71825</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.056505   </td><td style=\"text-align: right;\">    6.30388e+09</td><td style=\"text-align: right;\">          0.05603</td><td style=\"text-align: right;\">          402435475</td></tr>\n",
       "<tr><td>evaluate_config_eeb64947</td><td>TERMINATED</td><td>130.203.136.143:2091499</td><td style=\"text-align: right;\">10930</td><td style=\"text-align: right;\">95886</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.109825   </td><td style=\"text-align: right;\">    5.48636e+09</td><td style=\"text-align: right;\">          0.1093 </td><td style=\"text-align: right;\">         1048033980</td></tr>\n",
       "<tr><td>evaluate_config_eeb64948</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\">  422</td><td style=\"text-align: right;\">94792</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00478292 </td><td style=\"text-align: right;\">    7.15344e+09</td><td style=\"text-align: right;\">          0.00422</td><td style=\"text-align: right;\">           40002224</td></tr>\n",
       "<tr><td>evaluate_config_eeb64949</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\">  765</td><td style=\"text-align: right;\">99999</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0080018  </td><td style=\"text-align: right;\">    7.09554e+09</td><td style=\"text-align: right;\">          0.00765</td><td style=\"text-align: right;\">           76499235</td></tr>\n",
       "<tr><td>evaluate_config_eeb6494a</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 6030</td><td style=\"text-align: right;\">84402</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.060806   </td><td style=\"text-align: right;\">    6.23626e+09</td><td style=\"text-align: right;\">          0.0603 </td><td style=\"text-align: right;\">          508944060</td></tr>\n",
       "<tr><td>evaluate_config_eeb6494c</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 2737</td><td style=\"text-align: right;\">59126</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0278509  </td><td style=\"text-align: right;\">    6.7672e+09 </td><td style=\"text-align: right;\">          0.02737</td><td style=\"text-align: right;\">          161827862</td></tr>\n",
       "<tr><td>evaluate_config_eeb6494d</td><td>TERMINATED</td><td>130.203.136.143:2091518</td><td style=\"text-align: right;\"> 8722</td><td style=\"text-align: right;\">79600</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0877728  </td><td style=\"text-align: right;\">    5.81833e+09</td><td style=\"text-align: right;\">          0.08722</td><td style=\"text-align: right;\">          694271200</td></tr>\n",
       "<tr><td>evaluate_config_eeb6494e</td><td>TERMINATED</td><td>130.203.136.143:2091523</td><td style=\"text-align: right;\"> 4169</td><td style=\"text-align: right;\">89204</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0421426  </td><td style=\"text-align: right;\">    6.53365e+09</td><td style=\"text-align: right;\">          0.04169</td><td style=\"text-align: right;\">          371891476</td></tr>\n",
       "<tr><td>evaluate_config_eeb6494f</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 4915</td><td style=\"text-align: right;\">78908</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0496564  </td><td style=\"text-align: right;\">    6.41361e+09</td><td style=\"text-align: right;\">          0.04915</td><td style=\"text-align: right;\">          387832820</td></tr>\n",
       "<tr><td>evaluate_config_eeb64950</td><td>TERMINATED</td><td>130.203.136.143:2091529</td><td style=\"text-align: right;\"> 7397</td><td style=\"text-align: right;\">89896</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0745356  </td><td style=\"text-align: right;\">    6.02223e+09</td><td style=\"text-align: right;\">          0.07397</td><td style=\"text-align: right;\">          664960712</td></tr>\n",
       "<tr><td>evaluate_config_eeb64951</td><td>TERMINATED</td><td>130.203.136.143:2091532</td><td style=\"text-align: right;\">11470</td><td style=\"text-align: right;\">84524</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.115089   </td><td style=\"text-align: right;\">    5.40666e+09</td><td style=\"text-align: right;\">          0.1147 </td><td style=\"text-align: right;\">          969490280</td></tr>\n",
       "<tr><td>evaluate_config_eeb64952</td><td>TERMINATED</td><td>130.203.136.143:2091537</td><td style=\"text-align: right;\">15880</td><td style=\"text-align: right;\">60957</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.1594     </td><td style=\"text-align: right;\">    4.77757e+09</td><td style=\"text-align: right;\">          0.1588 </td><td style=\"text-align: right;\">          967997160</td></tr>\n",
       "<tr><td>evaluate_config_eeb64953</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">    1</td><td style=\"text-align: right;\">51219</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000448942</td><td style=\"text-align: right;\">    7.22483e+09</td><td style=\"text-align: right;\">          1e-05  </td><td style=\"text-align: right;\">              51219</td></tr>\n",
       "<tr><td>evaluate_config_eeb64954</td><td>TERMINATED</td><td>130.203.136.143:2091546</td><td style=\"text-align: right;\">10155</td><td style=\"text-align: right;\">61252</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.102103   </td><td style=\"text-align: right;\">    5.60177e+09</td><td style=\"text-align: right;\">          0.10155</td><td style=\"text-align: right;\">          622014060</td></tr>\n",
       "<tr><td>evaluate_config_eeb64955</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 3350</td><td style=\"text-align: right;\">29188</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0339797  </td><td style=\"text-align: right;\">    6.66672e+09</td><td style=\"text-align: right;\">          0.0335 </td><td style=\"text-align: right;\">           97779800</td></tr>\n",
       "<tr><td>evaluate_config_eeb64956</td><td>TERMINATED</td><td>130.203.136.143:2091553</td><td style=\"text-align: right;\">36654</td><td style=\"text-align: right;\">71457</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.367415   </td><td style=\"text-align: right;\">    2.33734e+09</td><td style=\"text-align: right;\">          0.36654</td><td style=\"text-align: right;\">         2619184878</td></tr>\n",
       "<tr><td>evaluate_config_eeb64957</td><td>TERMINATED</td><td>130.203.136.143:2091560</td><td style=\"text-align: right;\">  376</td><td style=\"text-align: right;\">14217</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00432038 </td><td style=\"text-align: right;\">    7.16122e+09</td><td style=\"text-align: right;\">          0.00376</td><td style=\"text-align: right;\">            5345592</td></tr>\n",
       "<tr><td>evaluate_config_eeb64958</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">   48</td><td style=\"text-align: right;\">67413</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00100708 </td><td style=\"text-align: right;\">    7.21684e+09</td><td style=\"text-align: right;\">          0.00048</td><td style=\"text-align: right;\">            3235824</td></tr>\n",
       "<tr><td>evaluate_config_eeb64959</td><td>TERMINATED</td><td>130.203.136.143:2091582</td><td style=\"text-align: right;\">  110</td><td style=\"text-align: right;\">43401</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00166988 </td><td style=\"text-align: right;\">    7.20631e+09</td><td style=\"text-align: right;\">          0.0011 </td><td style=\"text-align: right;\">            4774110</td></tr>\n",
       "<tr><td>evaluate_config_eeb6495a</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">  941</td><td style=\"text-align: right;\">51314</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00989461 </td><td style=\"text-align: right;\">    7.06592e+09</td><td style=\"text-align: right;\">          0.00941</td><td style=\"text-align: right;\">           48286474</td></tr>\n",
       "<tr><td>evaluate_config_eeb6495b</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\"> 1402</td><td style=\"text-align: right;\">60104</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0144963  </td><td style=\"text-align: right;\">    6.98863e+09</td><td style=\"text-align: right;\">          0.01402</td><td style=\"text-align: right;\">           84265808</td></tr>\n",
       "<tr><td>evaluate_config_eeb6495c</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 9279</td><td style=\"text-align: right;\">52165</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0932992  </td><td style=\"text-align: right;\">    5.73367e+09</td><td style=\"text-align: right;\">          0.09279</td><td style=\"text-align: right;\">          484039035</td></tr>\n",
       "<tr><td>evaluate_config_eeb6495d</td><td>TERMINATED</td><td>130.203.136.143:2091628</td><td style=\"text-align: right;\">32790</td><td style=\"text-align: right;\">31924</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.328639   </td><td style=\"text-align: right;\">    2.72588e+09</td><td style=\"text-align: right;\">          0.3279 </td><td style=\"text-align: right;\">         1046787960</td></tr>\n",
       "<tr><td>evaluate_config_eeb6495e</td><td>TERMINATED</td><td>130.203.136.143:2091652</td><td style=\"text-align: right;\">    2</td><td style=\"text-align: right;\">30070</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000456095</td><td style=\"text-align: right;\">    7.22466e+09</td><td style=\"text-align: right;\">          2e-05  </td><td style=\"text-align: right;\">              60140</td></tr>\n",
       "<tr><td>evaluate_config_eeb6495f</td><td>TERMINATED</td><td>130.203.136.143:2091660</td><td style=\"text-align: right;\">    2</td><td style=\"text-align: right;\">82868</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000626564</td><td style=\"text-align: right;\">    7.22466e+09</td><td style=\"text-align: right;\">          2e-05  </td><td style=\"text-align: right;\">             165736</td></tr>\n",
       "<tr><td>evaluate_config_eeb64960</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">    1</td><td style=\"text-align: right;\">62629</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000496387</td><td style=\"text-align: right;\">    7.22483e+09</td><td style=\"text-align: right;\">          1e-05  </td><td style=\"text-align: right;\">              62629</td></tr>\n",
       "<tr><td>evaluate_config_eeb64961</td><td>TERMINATED</td><td>130.203.136.143:2091700</td><td style=\"text-align: right;\">37962</td><td style=\"text-align: right;\">45076</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.380512   </td><td style=\"text-align: right;\">    2.21257e+09</td><td style=\"text-align: right;\">          0.37962</td><td style=\"text-align: right;\">         1711175112</td></tr>\n",
       "<tr><td>evaluate_config_eeb64962</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 2268</td><td style=\"text-align: right;\">59254</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0231752  </td><td style=\"text-align: right;\">    6.84458e+09</td><td style=\"text-align: right;\">          0.02268</td><td style=\"text-align: right;\">          134388072</td></tr>\n",
       "<tr><td>evaluate_config_eeb64963</td><td>TERMINATED</td><td>130.203.136.143:2091716</td><td style=\"text-align: right;\"> 5656</td><td style=\"text-align: right;\">38693</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0570891  </td><td style=\"text-align: right;\">    6.29547e+09</td><td style=\"text-align: right;\">          0.05656</td><td style=\"text-align: right;\">          218847608</td></tr>\n",
       "<tr><td>evaluate_config_eeb64964</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">15224</td><td style=\"text-align: right;\">65637</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.152771   </td><td style=\"text-align: right;\">    4.86869e+09</td><td style=\"text-align: right;\">          0.15224</td><td style=\"text-align: right;\">          999257688</td></tr>\n",
       "<tr><td>evaluate_config_eeb64965</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">  400</td><td style=\"text-align: right;\">81928</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.00431013 </td><td style=\"text-align: right;\">    7.15716e+09</td><td style=\"text-align: right;\">          0.004  </td><td style=\"text-align: right;\">           32771200</td></tr>\n",
       "<tr><td>evaluate_config_eeb64966</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">10237</td><td style=\"text-align: right;\">61006</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.102747   </td><td style=\"text-align: right;\">    5.58951e+09</td><td style=\"text-align: right;\">          0.10237</td><td style=\"text-align: right;\">          624518422</td></tr>\n",
       "<tr><td>evaluate_config_eeb64967</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">22640</td><td style=\"text-align: right;\">70268</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.226907   </td><td style=\"text-align: right;\">    3.88877e+09</td><td style=\"text-align: right;\">          0.2264 </td><td style=\"text-align: right;\">         1590867520</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe3c</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">27430</td><td style=\"text-align: right;\">68316</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.274947   </td><td style=\"text-align: right;\">    3.3143e+09 </td><td style=\"text-align: right;\">          0.2743 </td><td style=\"text-align: right;\">         1873907880</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe3d</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\"> 8449</td><td style=\"text-align: right;\">62958</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0849419  </td><td style=\"text-align: right;\">    5.86006e+09</td><td style=\"text-align: right;\">          0.08449</td><td style=\"text-align: right;\">          531932142</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe3e</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 8733</td><td style=\"text-align: right;\">15881</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0877531  </td><td style=\"text-align: right;\">    5.81666e+09</td><td style=\"text-align: right;\">          0.08733</td><td style=\"text-align: right;\">          138688773</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe3f</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">13592</td><td style=\"text-align: right;\">70866</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.136537   </td><td style=\"text-align: right;\">    5.0991e+09 </td><td style=\"text-align: right;\">          0.13592</td><td style=\"text-align: right;\">          963210672</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe40</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">37711</td><td style=\"text-align: right;\">69670</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.377706   </td><td style=\"text-align: right;\">    2.23625e+09</td><td style=\"text-align: right;\">          0.37711</td><td style=\"text-align: right;\">         2627325370</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe41</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\">26945</td><td style=\"text-align: right;\">74477</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.270188   </td><td style=\"text-align: right;\">    3.37038e+09</td><td style=\"text-align: right;\">          0.26945</td><td style=\"text-align: right;\">         2006782765</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe42</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">19023</td><td style=\"text-align: right;\">66059</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.19068    </td><td style=\"text-align: right;\">    4.35296e+09</td><td style=\"text-align: right;\">          0.19023</td><td style=\"text-align: right;\">         1256640357</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe44</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\"> 2307</td><td style=\"text-align: right;\"> 7739</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0235672  </td><td style=\"text-align: right;\">    6.83813e+09</td><td style=\"text-align: right;\">          0.02307</td><td style=\"text-align: right;\">           17853873</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe45</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\">33061</td><td style=\"text-align: right;\">24023</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.331111   </td><td style=\"text-align: right;\">    2.69766e+09</td><td style=\"text-align: right;\">          0.33061</td><td style=\"text-align: right;\">          794224403</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe46</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">42278</td><td style=\"text-align: right;\">68722</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.423344   </td><td style=\"text-align: right;\">    1.82517e+09</td><td style=\"text-align: right;\">          0.42278</td><td style=\"text-align: right;\">         2905428716</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe47</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">17797</td><td style=\"text-align: right;\">67910</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.178431   </td><td style=\"text-align: right;\">    4.51624e+09</td><td style=\"text-align: right;\">          0.17797</td><td style=\"text-align: right;\">         1208594270</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe48</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\">22138</td><td style=\"text-align: right;\">71605</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.222027   </td><td style=\"text-align: right;\">    3.95163e+09</td><td style=\"text-align: right;\">          0.22138</td><td style=\"text-align: right;\">         1585191490</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe49</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\">33986</td><td style=\"text-align: right;\">65027</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.34045    </td><td style=\"text-align: right;\">    2.60243e+09</td><td style=\"text-align: right;\">          0.33986</td><td style=\"text-align: right;\">         2210007622</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe4a</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 1788</td><td style=\"text-align: right;\">12680</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0182195  </td><td style=\"text-align: right;\">    6.92424e+09</td><td style=\"text-align: right;\">          0.01788</td><td style=\"text-align: right;\">           22671840</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe4b</td><td>TERMINATED</td><td>130.203.136.143:2091499</td><td style=\"text-align: right;\">89112</td><td style=\"text-align: right;\">33454</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.892296   </td><td style=\"text-align: right;\">    1.69085e+07</td><td style=\"text-align: right;\">          0.89112</td><td style=\"text-align: right;\">         2981152848</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe4c</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\"> 8034</td><td style=\"text-align: right;\">99470</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0808222  </td><td style=\"text-align: right;\">    5.92377e+09</td><td style=\"text-align: right;\">          0.08034</td><td style=\"text-align: right;\">          799141980</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe4d</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">14871</td><td style=\"text-align: right;\">92302</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.149266   </td><td style=\"text-align: right;\">    4.91808e+09</td><td style=\"text-align: right;\">          0.14871</td><td style=\"text-align: right;\">         1372623042</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe4e</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">51431</td><td style=\"text-align: right;\">71631</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.515241   </td><td style=\"text-align: right;\">    1.12688e+09</td><td style=\"text-align: right;\">          0.51431</td><td style=\"text-align: right;\">         3684053961</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe4f</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">27651</td><td style=\"text-align: right;\">67709</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.277176   </td><td style=\"text-align: right;\">    3.28891e+09</td><td style=\"text-align: right;\">          0.27651</td><td style=\"text-align: right;\">         1872221559</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe50</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\">42411</td><td style=\"text-align: right;\">72843</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.424887   </td><td style=\"text-align: right;\">    1.81382e+09</td><td style=\"text-align: right;\">          0.42411</td><td style=\"text-align: right;\">         3089344473</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe51</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\">33532</td><td style=\"text-align: right;\">66497</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.335996   </td><td style=\"text-align: right;\">    2.64896e+09</td><td style=\"text-align: right;\">          0.33532</td><td style=\"text-align: right;\">         2229777404</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe52</td><td>TERMINATED</td><td>130.203.136.143:2091523</td><td style=\"text-align: right;\">30702</td><td style=\"text-align: right;\">69892</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.307724   </td><td style=\"text-align: right;\">    2.94827e+09</td><td style=\"text-align: right;\">          0.30702</td><td style=\"text-align: right;\">         2145824184</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe53</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">58218</td><td style=\"text-align: right;\">67552</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.583167   </td><td style=\"text-align: right;\">    7.17276e+08</td><td style=\"text-align: right;\">          0.58218</td><td style=\"text-align: right;\">         3932742336</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe54</td><td>TERMINATED</td><td>130.203.136.143:2091537</td><td style=\"text-align: right;\">59802</td><td style=\"text-align: right;\">68585</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.59901    </td><td style=\"text-align: right;\">    6.34939e+08</td><td style=\"text-align: right;\">          0.59802</td><td style=\"text-align: right;\">         4101520170</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe55</td><td>TERMINATED</td><td>130.203.136.143:2091560</td><td style=\"text-align: right;\">29889</td><td style=\"text-align: right;\">68859</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.299563   </td><td style=\"text-align: right;\">    3.03722e+09</td><td style=\"text-align: right;\">          0.29889</td><td style=\"text-align: right;\">         2058126651</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe57</td><td>TERMINATED</td><td>130.203.136.143:2091546</td><td style=\"text-align: right;\">23789</td><td style=\"text-align: right;\">57811</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.238566   </td><td style=\"text-align: right;\">    3.74679e+09</td><td style=\"text-align: right;\">          0.23789</td><td style=\"text-align: right;\">         1375265879</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe58</td><td>TERMINATED</td><td>130.203.136.143:2091518</td><td style=\"text-align: right;\">10601</td><td style=\"text-align: right;\">64103</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.106516   </td><td style=\"text-align: right;\">    5.53521e+09</td><td style=\"text-align: right;\">          0.10601</td><td style=\"text-align: right;\">          679555903</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe59</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\">16986</td><td style=\"text-align: right;\">56279</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.17048    </td><td style=\"text-align: right;\">    4.6259e+09 </td><td style=\"text-align: right;\">          0.16986</td><td style=\"text-align: right;\">          955955094</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe5a</td><td>TERMINATED</td><td>130.203.136.143:2091529</td><td style=\"text-align: right;\">11733</td><td style=\"text-align: right;\">96272</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.117931   </td><td style=\"text-align: right;\">    5.36805e+09</td><td style=\"text-align: right;\">          0.11733</td><td style=\"text-align: right;\">         1129559376</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe5b</td><td>TERMINATED</td><td>130.203.136.143:2091582</td><td style=\"text-align: right;\">18848</td><td style=\"text-align: right;\">88332</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.189116   </td><td style=\"text-align: right;\">    4.37609e+09</td><td style=\"text-align: right;\">          0.18848</td><td style=\"text-align: right;\">         1664881536</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe5c</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">24668</td><td style=\"text-align: right;\">93125</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.247361   </td><td style=\"text-align: right;\">    3.63995e+09</td><td style=\"text-align: right;\">          0.24668</td><td style=\"text-align: right;\">         2297207500</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe5d</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 8965</td><td style=\"text-align: right;\">91479</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0900934  </td><td style=\"text-align: right;\">    5.78132e+09</td><td style=\"text-align: right;\">          0.08965</td><td style=\"text-align: right;\">          820109235</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe5e</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\">43791</td><td style=\"text-align: right;\">75602</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.438753   </td><td style=\"text-align: right;\">    1.69818e+09</td><td style=\"text-align: right;\">          0.43791</td><td style=\"text-align: right;\">         3310687182</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe5f</td><td>TERMINATED</td><td>130.203.136.143:2091532</td><td style=\"text-align: right;\">41075</td><td style=\"text-align: right;\">70084</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.411485   </td><td style=\"text-align: right;\">    1.92941e+09</td><td style=\"text-align: right;\">          0.41075</td><td style=\"text-align: right;\">         2878700300</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe60</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">42394</td><td style=\"text-align: right;\">75616</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.424695   </td><td style=\"text-align: right;\">    1.81527e+09</td><td style=\"text-align: right;\">          0.42394</td><td style=\"text-align: right;\">         3205664704</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe61</td><td>TERMINATED</td><td>130.203.136.143:2091518</td><td style=\"text-align: right;\">51452</td><td style=\"text-align: right;\">68858</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.515352   </td><td style=\"text-align: right;\">    1.12547e+09</td><td style=\"text-align: right;\">          0.51452</td><td style=\"text-align: right;\">         3542881816</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe62</td><td>TERMINATED</td><td>130.203.136.143:2091628</td><td style=\"text-align: right;\">57024</td><td style=\"text-align: right;\">74255</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.571166   </td><td style=\"text-align: right;\">    7.82657e+08</td><td style=\"text-align: right;\">          0.57024</td><td style=\"text-align: right;\">         4234317120</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe63</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\">46386</td><td style=\"text-align: right;\">69007</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.464752   </td><td style=\"text-align: right;\">    1.49104e+09</td><td style=\"text-align: right;\">          0.46386</td><td style=\"text-align: right;\">         3200958702</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe64</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\">48131</td><td style=\"text-align: right;\">69779</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.482181   </td><td style=\"text-align: right;\">    1.35932e+09</td><td style=\"text-align: right;\">          0.48131</td><td style=\"text-align: right;\">         3358533049</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe65</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">70419</td><td style=\"text-align: right;\">65325</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.705266   </td><td style=\"text-align: right;\">    2.12606e+08</td><td style=\"text-align: right;\">          0.70419</td><td style=\"text-align: right;\">         4600121175</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe66</td><td>TERMINATED</td><td>130.203.136.143:2091553</td><td style=\"text-align: right;\">74830</td><td style=\"text-align: right;\">65838</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.749436   </td><td style=\"text-align: right;\">    1.03429e+08</td><td style=\"text-align: right;\">          0.7483 </td><td style=\"text-align: right;\">         4926657540</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe67</td><td>TERMINATED</td><td>130.203.136.143:2091523</td><td style=\"text-align: right;\">45294</td><td style=\"text-align: right;\">69266</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.453809   </td><td style=\"text-align: right;\">    1.57657e+09</td><td style=\"text-align: right;\">          0.45294</td><td style=\"text-align: right;\">         3137334204</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe68</td><td>TERMINATED</td><td>130.203.136.143:2091652</td><td style=\"text-align: right;\">46488</td><td style=\"text-align: right;\">35624</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.465717   </td><td style=\"text-align: right;\">    1.48317e+09</td><td style=\"text-align: right;\">          0.46488</td><td style=\"text-align: right;\">         1656088512</td></tr>\n",
       "<tr><td>evaluate_config_efa2fe69</td><td>TERMINATED</td><td>130.203.136.143:2091529</td><td style=\"text-align: right;\">88327</td><td style=\"text-align: right;\">41660</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.884525   </td><td style=\"text-align: right;\">    1.10689e+07</td><td style=\"text-align: right;\">          0.88327</td><td style=\"text-align: right;\">         3679702820</td></tr>\n",
       "<tr><td>evaluate_config_f0c474c6</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\">78838</td><td style=\"text-align: right;\">40668</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.789539   </td><td style=\"text-align: right;\">    3.79702e+07</td><td style=\"text-align: right;\">          0.78838</td><td style=\"text-align: right;\">         3206183784</td></tr>\n",
       "<tr><td>evaluate_config_f0c474c7</td><td>TERMINATED</td><td>130.203.136.143:2091560</td><td style=\"text-align: right;\">61444</td><td style=\"text-align: right;\">99016</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.615407   </td><td style=\"text-align: right;\">    5.54885e+08</td><td style=\"text-align: right;\">          0.61444</td><td style=\"text-align: right;\">         6083939104</td></tr>\n",
       "<tr><td>evaluate_config_f0c474c8</td><td>TERMINATED</td><td>130.203.136.143:2091546</td><td style=\"text-align: right;\">99029</td><td style=\"text-align: right;\">91919</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.991606   </td><td style=\"text-align: right;\">    1.96813e+08</td><td style=\"text-align: right;\">          0.99029</td><td style=\"text-align: right;\">         9102646651</td></tr>\n",
       "<tr><td>evaluate_config_f0c474c9</td><td>TERMINATED</td><td>130.203.136.143:2091499</td><td style=\"text-align: right;\">84841</td><td style=\"text-align: right;\">99590</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.849653   </td><td style=\"text-align: right;\">25280.1        </td><td style=\"text-align: right;\">          0.84841</td><td style=\"text-align: right;\">         8449315190</td></tr>\n",
       "<tr><td>evaluate_config_f0c474ca</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">81162</td><td style=\"text-align: right;\">99223</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.812806   </td><td style=\"text-align: right;\">    1.47302e+07</td><td style=\"text-align: right;\">          0.81162</td><td style=\"text-align: right;\">         8053137126</td></tr>\n",
       "<tr><td>evaluate_config_f0c474cb</td><td>TERMINATED</td><td>130.203.136.143:2091660</td><td style=\"text-align: right;\">94664</td><td style=\"text-align: right;\">92883</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.947983   </td><td style=\"text-align: right;\">    9.33929e+07</td><td style=\"text-align: right;\">          0.94664</td><td style=\"text-align: right;\">         8792676312</td></tr>\n",
       "<tr><td>evaluate_config_f0c474cc</td><td>TERMINATED</td><td>130.203.136.143:2091532</td><td style=\"text-align: right;\">71431</td><td style=\"text-align: right;\">38833</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.715069   </td><td style=\"text-align: right;\">    1.84118e+08</td><td style=\"text-align: right;\">          0.71431</td><td style=\"text-align: right;\">         2773880023</td></tr>\n",
       "<tr><td>evaluate_config_f0c474cd</td><td>TERMINATED</td><td>130.203.136.143:2091537</td><td style=\"text-align: right;\">91283</td><td style=\"text-align: right;\">40217</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.914129   </td><td style=\"text-align: right;\">    3.94761e+07</td><td style=\"text-align: right;\">          0.91283</td><td style=\"text-align: right;\">         3671128411</td></tr>\n",
       "<tr><td>evaluate_config_f0c474ce</td><td>TERMINATED</td><td>130.203.136.143:2091700</td><td style=\"text-align: right;\">96638</td><td style=\"text-align: right;\">37895</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.967729   </td><td style=\"text-align: right;\">    1.35443e+08</td><td style=\"text-align: right;\">          0.96638</td><td style=\"text-align: right;\">         3662097010</td></tr>\n",
       "<tr><td>evaluate_config_f0c474cf</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">67903</td><td style=\"text-align: right;\">40361</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.6801     </td><td style=\"text-align: right;\">    2.92307e+08</td><td style=\"text-align: right;\">          0.67903</td><td style=\"text-align: right;\">         2740632983</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d0</td><td>TERMINATED</td><td>130.203.136.143:2091582</td><td style=\"text-align: right;\">95605</td><td style=\"text-align: right;\">40881</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.956801   </td><td style=\"text-align: right;\">    1.12466e+08</td><td style=\"text-align: right;\">          0.95605</td><td style=\"text-align: right;\">         3908428005</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d1</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\">93723</td><td style=\"text-align: right;\">39359</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.938534   </td><td style=\"text-align: right;\">    7.60907e+07</td><td style=\"text-align: right;\">          0.93723</td><td style=\"text-align: right;\">         3688843557</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d2</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\">67068</td><td style=\"text-align: right;\">37525</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.671265   </td><td style=\"text-align: right;\">    3.21557e+08</td><td style=\"text-align: right;\">          0.67068</td><td style=\"text-align: right;\">         2516726700</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d3</td><td>TERMINATED</td><td>130.203.136.143:2091716</td><td style=\"text-align: right;\">79364</td><td style=\"text-align: right;\">36213</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.794806   </td><td style=\"text-align: right;\">    3.17645e+07</td><td style=\"text-align: right;\">          0.79364</td><td style=\"text-align: right;\">         2874008532</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d4</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">82421</td><td style=\"text-align: right;\">38352</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.825416   </td><td style=\"text-align: right;\">    6.65124e+06</td><td style=\"text-align: right;\">          0.82421</td><td style=\"text-align: right;\">         3161010192</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d5</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\">68017</td><td style=\"text-align: right;\">36173</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.681213   </td><td style=\"text-align: right;\">    2.88422e+08</td><td style=\"text-align: right;\">          0.68017</td><td style=\"text-align: right;\">         2460378941</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d6</td><td>TERMINATED</td><td>130.203.136.143:2091628</td><td style=\"text-align: right;\">89296</td><td style=\"text-align: right;\">36948</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.894267   </td><td style=\"text-align: right;\">    1.84556e+07</td><td style=\"text-align: right;\">          0.89296</td><td style=\"text-align: right;\">         3299308608</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d7</td><td>TERMINATED</td><td>130.203.136.143:2091553</td><td style=\"text-align: right;\">82027</td><td style=\"text-align: right;\">40434</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.821467   </td><td style=\"text-align: right;\">    8.83873e+06</td><td style=\"text-align: right;\">          0.82027</td><td style=\"text-align: right;\">         3316679718</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d8</td><td>TERMINATED</td><td>130.203.136.143:2091652</td><td style=\"text-align: right;\">65242</td><td style=\"text-align: right;\">39045</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.653039   </td><td style=\"text-align: right;\">    3.90379e+08</td><td style=\"text-align: right;\">          0.65242</td><td style=\"text-align: right;\">         2547373890</td></tr>\n",
       "<tr><td>evaluate_config_f0c474d9</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">36130</td><td style=\"text-align: right;\">41713</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.36202    </td><td style=\"text-align: right;\">    2.38828e+09</td><td style=\"text-align: right;\">          0.3613 </td><td style=\"text-align: right;\">         1507090690</td></tr>\n",
       "<tr><td>evaluate_config_f0c474da</td><td>TERMINATED</td><td>130.203.136.143:2091518</td><td style=\"text-align: right;\">90438</td><td style=\"text-align: right;\">41375</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.905649   </td><td style=\"text-align: right;\">    2.95718e+07</td><td style=\"text-align: right;\">          0.90438</td><td style=\"text-align: right;\">         3741872250</td></tr>\n",
       "<tr><td>evaluate_config_f0c474db</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\">95640</td><td style=\"text-align: right;\">42265</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.957753   </td><td style=\"text-align: right;\">    1.1321e+08 </td><td style=\"text-align: right;\">          0.9564 </td><td style=\"text-align: right;\">         4042224600</td></tr>\n",
       "<tr><td>evaluate_config_f0c474dc</td><td>TERMINATED</td><td>130.203.136.143:2091560</td><td style=\"text-align: right;\">83461</td><td style=\"text-align: right;\">40623</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.835858   </td><td style=\"text-align: right;\">    2.36852e+06</td><td style=\"text-align: right;\">          0.83461</td><td style=\"text-align: right;\">         3390436203</td></tr>\n",
       "<tr><td>evaluate_config_f0c474dd</td><td>TERMINATED</td><td>130.203.136.143:2091529</td><td style=\"text-align: right;\">59080</td><td style=\"text-align: right;\">39289</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.591742   </td><td style=\"text-align: right;\">    6.71846e+08</td><td style=\"text-align: right;\">          0.5908 </td><td style=\"text-align: right;\">         2321194120</td></tr>\n",
       "<tr><td>evaluate_config_f0c474de</td><td>TERMINATED</td><td>130.203.136.143:2091523</td><td style=\"text-align: right;\">92806</td><td style=\"text-align: right;\">40936</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.929367   </td><td style=\"text-align: right;\">    6.09336e+07</td><td style=\"text-align: right;\">          0.92806</td><td style=\"text-align: right;\">         3799106416</td></tr>\n",
       "<tr><td>evaluate_config_f0c474df</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">95597</td><td style=\"text-align: right;\">43139</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.957257   </td><td style=\"text-align: right;\">    1.12296e+08</td><td style=\"text-align: right;\">          0.95597</td><td style=\"text-align: right;\">         4123958983</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e0</td><td>TERMINATED</td><td>130.203.136.143:2091499</td><td style=\"text-align: right;\">91675</td><td style=\"text-align: right;\">41884</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.918067   </td><td style=\"text-align: right;\">    4.45556e+07</td><td style=\"text-align: right;\">          0.91675</td><td style=\"text-align: right;\">         3839715700</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e1</td><td>TERMINATED</td><td>130.203.136.143:2091532</td><td style=\"text-align: right;\">17003</td><td style=\"text-align: right;\">24030</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.170594   </td><td style=\"text-align: right;\">    4.62359e+09</td><td style=\"text-align: right;\">          0.17003</td><td style=\"text-align: right;\">          408582090</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e2</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\">23538</td><td style=\"text-align: right;\">19975</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.235999   </td><td style=\"text-align: right;\">    3.77758e+09</td><td style=\"text-align: right;\">          0.23538</td><td style=\"text-align: right;\">          470171550</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e3</td><td>TERMINATED</td><td>130.203.136.143:2091546</td><td style=\"text-align: right;\">16457</td><td style=\"text-align: right;\">21333</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.165074   </td><td style=\"text-align: right;\">    4.69814e+09</td><td style=\"text-align: right;\">          0.16457</td><td style=\"text-align: right;\">          351077181</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e4</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">16314</td><td style=\"text-align: right;\">15944</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.1637     </td><td style=\"text-align: right;\">    4.71777e+09</td><td style=\"text-align: right;\">          0.16314</td><td style=\"text-align: right;\">          260110416</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e5</td><td>TERMINATED</td><td>130.203.136.143:2091537</td><td style=\"text-align: right;\">16171</td><td style=\"text-align: right;\">83352</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.162279   </td><td style=\"text-align: right;\">    4.73743e+09</td><td style=\"text-align: right;\">          0.16171</td><td style=\"text-align: right;\">         1347885192</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e6</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">15656</td><td style=\"text-align: right;\">88184</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.157098   </td><td style=\"text-align: right;\">    4.80859e+09</td><td style=\"text-align: right;\">          0.15656</td><td style=\"text-align: right;\">         1380608704</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e7</td><td>TERMINATED</td><td>130.203.136.143:2091660</td><td style=\"text-align: right;\"> 8261</td><td style=\"text-align: right;\">21200</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0831182  </td><td style=\"text-align: right;\">    5.88887e+09</td><td style=\"text-align: right;\">          0.08261</td><td style=\"text-align: right;\">          175133200</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e8</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">16056</td><td style=\"text-align: right;\">14216</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.161107   </td><td style=\"text-align: right;\">    4.75328e+09</td><td style=\"text-align: right;\">          0.16056</td><td style=\"text-align: right;\">          228252096</td></tr>\n",
       "<tr><td>evaluate_config_f0c474e9</td><td>TERMINATED</td><td>130.203.136.143:2091700</td><td style=\"text-align: right;\">22051</td><td style=\"text-align: right;\">21305</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.221121   </td><td style=\"text-align: right;\">    3.96258e+09</td><td style=\"text-align: right;\">          0.22051</td><td style=\"text-align: right;\">          469796555</td></tr>\n",
       "<tr><td>evaluate_config_f0c474ea</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\">20497</td><td style=\"text-align: right;\">23274</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.205595   </td><td style=\"text-align: right;\">    4.16064e+09</td><td style=\"text-align: right;\">          0.20497</td><td style=\"text-align: right;\">          477047178</td></tr>\n",
       "<tr><td>evaluate_config_f0c474eb</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\">16680</td><td style=\"text-align: right;\">22445</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.167361   </td><td style=\"text-align: right;\">    4.66762e+09</td><td style=\"text-align: right;\">          0.1668 </td><td style=\"text-align: right;\">          374382600</td></tr>\n",
       "<tr><td>evaluate_config_f0c474ec</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\"> 5321</td><td style=\"text-align: right;\"> 4372</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0536213  </td><td style=\"text-align: right;\">    6.34874e+09</td><td style=\"text-align: right;\">          0.05321</td><td style=\"text-align: right;\">           23263412</td></tr>\n",
       "<tr><td>evaluate_config_f0c474ed</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\"> 6497</td><td style=\"text-align: right;\">87464</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0654261  </td><td style=\"text-align: right;\">    6.16272e+09</td><td style=\"text-align: right;\">          0.06497</td><td style=\"text-align: right;\">          568253608</td></tr>\n",
       "<tr><td>evaluate_config_f0c474ee</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\"> 3600</td><td style=\"text-align: right;\"> 5967</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0364316  </td><td style=\"text-align: right;\">    6.62596e+09</td><td style=\"text-align: right;\">          0.036  </td><td style=\"text-align: right;\">           21481200</td></tr>\n",
       "<tr><td>evaluate_config_f0c474ef</td><td>TERMINATED</td><td>130.203.136.143:2091716</td><td style=\"text-align: right;\"> 3191</td><td style=\"text-align: right;\"> 5285</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0323405  </td><td style=\"text-align: right;\">    6.69271e+09</td><td style=\"text-align: right;\">          0.03191</td><td style=\"text-align: right;\">           16864435</td></tr>\n",
       "<tr><td>evaluate_config_f0c474f0</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\"> 5038</td><td style=\"text-align: right;\">26841</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0507908  </td><td style=\"text-align: right;\">    6.39392e+09</td><td style=\"text-align: right;\">          0.05038</td><td style=\"text-align: right;\">          135224958</td></tr>\n",
       "<tr><td>evaluate_config_f0c474f1</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\"> 4648</td><td style=\"text-align: right;\">24910</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0469081  </td><td style=\"text-align: right;\">    6.45644e+09</td><td style=\"text-align: right;\">          0.04648</td><td style=\"text-align: right;\">          115781680</td></tr>\n",
       "<tr><td>evaluate_config_f0c474f2</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">   20</td><td style=\"text-align: right;\">24597</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000611305</td><td style=\"text-align: right;\">    7.2216e+09 </td><td style=\"text-align: right;\">          0.0002 </td><td style=\"text-align: right;\">             491940</td></tr>\n",
       "<tr><td>evaluate_config_f1f8015a</td><td>TERMINATED</td><td>130.203.136.143:2091529</td><td style=\"text-align: right;\">25264</td><td style=\"text-align: right;\">24823</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.253103   </td><td style=\"text-align: right;\">    3.56839e+09</td><td style=\"text-align: right;\">          0.25264</td><td style=\"text-align: right;\">          627128272</td></tr>\n",
       "<tr><td>evaluate_config_f1f8015b</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\"> 3541</td><td style=\"text-align: right;\">22662</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.0358584  </td><td style=\"text-align: right;\">    6.63557e+09</td><td style=\"text-align: right;\">          0.03541</td><td style=\"text-align: right;\">           80246142</td></tr>\n",
       "<tr><td>evaluate_config_f1f8015c</td><td>TERMINATED</td><td>130.203.136.143:2091532</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">22871</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000561476</td><td style=\"text-align: right;\">    7.22228e+09</td><td style=\"text-align: right;\">          0.00016</td><td style=\"text-align: right;\">             365936</td></tr>\n",
       "<tr><td>evaluate_config_f1f8015d</td><td>TERMINATED</td><td>130.203.136.143:2091553</td><td style=\"text-align: right;\">28878</td><td style=\"text-align: right;\">56067</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.289505   </td><td style=\"text-align: right;\">    3.14968e+09</td><td style=\"text-align: right;\">          0.28878</td><td style=\"text-align: right;\">         1619102826</td></tr>\n",
       "<tr><td>evaluate_config_f1f8015e</td><td>TERMINATED</td><td>130.203.136.143:2091628</td><td style=\"text-align: right;\">   24</td><td style=\"text-align: right;\">55505</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000733376</td><td style=\"text-align: right;\">    7.22092e+09</td><td style=\"text-align: right;\">          0.00024</td><td style=\"text-align: right;\">            1332120</td></tr>\n",
       "<tr><td>evaluate_config_f1f8015f</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\">29570</td><td style=\"text-align: right;\">47224</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.296398   </td><td style=\"text-align: right;\">    3.07248e+09</td><td style=\"text-align: right;\">          0.2957 </td><td style=\"text-align: right;\">         1396413680</td></tr>\n",
       "<tr><td>evaluate_config_f1f80160</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\">   41</td><td style=\"text-align: right;\">45993</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000775814</td><td style=\"text-align: right;\">    7.21803e+09</td><td style=\"text-align: right;\">          0.00041</td><td style=\"text-align: right;\">            1885713</td></tr>\n",
       "<tr><td>evaluate_config_f1f80161</td><td>TERMINATED</td><td>130.203.136.143:2091518</td><td style=\"text-align: right;\">31513</td><td style=\"text-align: right;\">47078</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.315859   </td><td style=\"text-align: right;\">    2.86086e+09</td><td style=\"text-align: right;\">          0.31513</td><td style=\"text-align: right;\">         1483569014</td></tr>\n",
       "<tr><td>evaluate_config_f1f80162</td><td>TERMINATED</td><td>130.203.136.143:2091582</td><td style=\"text-align: right;\">35727</td><td style=\"text-align: right;\">48537</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.358083   </td><td style=\"text-align: right;\">    2.42783e+09</td><td style=\"text-align: right;\">          0.35727</td><td style=\"text-align: right;\">         1734081399</td></tr>\n",
       "<tr><td>evaluate_config_f1f80163</td><td>TERMINATED</td><td>130.203.136.143:2091652</td><td style=\"text-align: right;\">   17</td><td style=\"text-align: right;\">47043</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000541449</td><td style=\"text-align: right;\">    7.22211e+09</td><td style=\"text-align: right;\">          0.00017</td><td style=\"text-align: right;\">             799731</td></tr>\n",
       "<tr><td>evaluate_config_f1f80164</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\">   10</td><td style=\"text-align: right;\">47977</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.000530958</td><td style=\"text-align: right;\">    7.2233e+09 </td><td style=\"text-align: right;\">          0.0001 </td><td style=\"text-align: right;\">             479770</td></tr>\n",
       "<tr><td>evaluate_config_f1f80165</td><td>TERMINATED</td><td>130.203.136.143:2091560</td><td style=\"text-align: right;\">31061</td><td style=\"text-align: right;\">47124</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.311034   </td><td style=\"text-align: right;\">    2.90942e+09</td><td style=\"text-align: right;\">          0.31061</td><td style=\"text-align: right;\">         1463718564</td></tr>\n",
       "<tr><td>evaluate_config_f1f80166</td><td>TERMINATED</td><td>130.203.136.143:2091499</td><td style=\"text-align: right;\">34706</td><td style=\"text-align: right;\">47676</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.3478     </td><td style=\"text-align: right;\">    2.52949e+09</td><td style=\"text-align: right;\">          0.34706</td><td style=\"text-align: right;\">         1654643256</td></tr>\n",
       "<tr><td>evaluate_config_f1f80167</td><td>TERMINATED</td><td>130.203.136.143:2091700</td><td style=\"text-align: right;\">30183</td><td style=\"text-align: right;\">46563</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.302553   </td><td style=\"text-align: right;\">    3.0049e+09 </td><td style=\"text-align: right;\">          0.30183</td><td style=\"text-align: right;\">         1405411029</td></tr>\n",
       "<tr><td>evaluate_config_f1f80168</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">37527</td><td style=\"text-align: right;\">47995</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.37598    </td><td style=\"text-align: right;\">    2.25369e+09</td><td style=\"text-align: right;\">          0.37527</td><td style=\"text-align: right;\">         1801108365</td></tr>\n",
       "<tr><td>evaluate_config_f1f80169</td><td>TERMINATED</td><td>130.203.136.143:2091523</td><td style=\"text-align: right;\">37399</td><td style=\"text-align: right;\">46873</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.37475    </td><td style=\"text-align: right;\">    2.26586e+09</td><td style=\"text-align: right;\">          0.37399</td><td style=\"text-align: right;\">         1753003327</td></tr>\n",
       "<tr><td>evaluate_config_f1f8016a</td><td>TERMINATED</td><td>130.203.136.143:2091660</td><td style=\"text-align: right;\">37220</td><td style=\"text-align: right;\">47596</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.373006   </td><td style=\"text-align: right;\">    2.28293e+09</td><td style=\"text-align: right;\">          0.3722 </td><td style=\"text-align: right;\">         1771523120</td></tr>\n",
       "<tr><td>evaluate_config_f1f8016b</td><td>TERMINATED</td><td>130.203.136.143:2091529</td><td style=\"text-align: right;\">35245</td><td style=\"text-align: right;\">47536</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.353185   </td><td style=\"text-align: right;\">    2.47556e+09</td><td style=\"text-align: right;\">          0.35245</td><td style=\"text-align: right;\">         1675406320</td></tr>\n",
       "<tr><td>evaluate_config_f1f8016c</td><td>TERMINATED</td><td>130.203.136.143:2091716</td><td style=\"text-align: right;\">38064</td><td style=\"text-align: right;\">46369</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.381413   </td><td style=\"text-align: right;\">    2.20299e+09</td><td style=\"text-align: right;\">          0.38064</td><td style=\"text-align: right;\">         1764989616</td></tr>\n",
       "<tr><td>evaluate_config_f1f8016d</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">42072</td><td style=\"text-align: right;\">48023</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.421622   </td><td style=\"text-align: right;\">    1.84281e+09</td><td style=\"text-align: right;\">          0.42072</td><td style=\"text-align: right;\">         2020423656</td></tr>\n",
       "<tr><td>evaluate_config_f1f8016e</td><td>TERMINATED</td><td>130.203.136.143:2091652</td><td style=\"text-align: right;\">41053</td><td style=\"text-align: right;\">47569</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.411349   </td><td style=\"text-align: right;\">    1.93134e+09</td><td style=\"text-align: right;\">          0.41053</td><td style=\"text-align: right;\">         1952850157</td></tr>\n",
       "<tr><td>evaluate_config_f1f8016f</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\">37284</td><td style=\"text-align: right;\">55677</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.3734     </td><td style=\"text-align: right;\">    2.27682e+09</td><td style=\"text-align: right;\">          0.37284</td><td style=\"text-align: right;\">         2075861268</td></tr>\n",
       "<tr><td>evaluate_config_f1f80170</td><td>TERMINATED</td><td>130.203.136.143:2091582</td><td style=\"text-align: right;\">41344</td><td style=\"text-align: right;\">47331</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.414247   </td><td style=\"text-align: right;\">    1.90585e+09</td><td style=\"text-align: right;\">          0.41344</td><td style=\"text-align: right;\">         1956852864</td></tr>\n",
       "<tr><td>evaluate_config_f1f80171</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\">41558</td><td style=\"text-align: right;\">31970</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.416375   </td><td style=\"text-align: right;\">    1.88721e+09</td><td style=\"text-align: right;\">          0.41558</td><td style=\"text-align: right;\">         1328609260</td></tr>\n",
       "<tr><td>evaluate_config_f1f80172</td><td>TERMINATED</td><td>130.203.136.143:2091537</td><td style=\"text-align: right;\">41796</td><td style=\"text-align: right;\">31676</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.418771   </td><td style=\"text-align: right;\">    1.86659e+09</td><td style=\"text-align: right;\">          0.41796</td><td style=\"text-align: right;\">         1323930096</td></tr>\n",
       "<tr><td>evaluate_config_f1f80173</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">44054</td><td style=\"text-align: right;\">57120</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.44135    </td><td style=\"text-align: right;\">    1.67657e+09</td><td style=\"text-align: right;\">          0.44054</td><td style=\"text-align: right;\">         2516364480</td></tr>\n",
       "<tr><td>evaluate_config_f1f80174</td><td>TERMINATED</td><td>130.203.136.143:2091628</td><td style=\"text-align: right;\">37887</td><td style=\"text-align: right;\">54027</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.379518   </td><td style=\"text-align: right;\">    2.21963e+09</td><td style=\"text-align: right;\">          0.37887</td><td style=\"text-align: right;\">         2046920949</td></tr>\n",
       "<tr><td>evaluate_config_f1f80175</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\">44587</td><td style=\"text-align: right;\">34169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.446659   </td><td style=\"text-align: right;\">    1.63321e+09</td><td style=\"text-align: right;\">          0.44587</td><td style=\"text-align: right;\">         1523493203</td></tr>\n",
       "<tr><td>evaluate_config_f1f80176</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\">45775</td><td style=\"text-align: right;\">53747</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.458561   </td><td style=\"text-align: right;\">    1.5386e+09 </td><td style=\"text-align: right;\">          0.45775</td><td style=\"text-align: right;\">         2460268925</td></tr>\n",
       "<tr><td>evaluate_config_f1f80177</td><td>TERMINATED</td><td>130.203.136.143:2091518</td><td style=\"text-align: right;\">44237</td><td style=\"text-align: right;\">33604</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.443225   </td><td style=\"text-align: right;\">    1.66162e+09</td><td style=\"text-align: right;\">          0.44237</td><td style=\"text-align: right;\">         1486540148</td></tr>\n",
       "<tr><td>evaluate_config_f1f80178</td><td>TERMINATED</td><td>130.203.136.143:2091532</td><td style=\"text-align: right;\">44773</td><td style=\"text-align: right;\">30536</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.448584   </td><td style=\"text-align: right;\">    1.61821e+09</td><td style=\"text-align: right;\">          0.44773</td><td style=\"text-align: right;\">         1367188328</td></tr>\n",
       "<tr><td>evaluate_config_f1f80179</td><td>TERMINATED</td><td>130.203.136.143:2091660</td><td style=\"text-align: right;\">48994</td><td style=\"text-align: right;\">33673</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.490803   </td><td style=\"text-align: right;\">    1.29643e+09</td><td style=\"text-align: right;\">          0.48994</td><td style=\"text-align: right;\">         1649774962</td></tr>\n",
       "<tr><td>evaluate_config_f1f8017a</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\">47323</td><td style=\"text-align: right;\">32435</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.474068   </td><td style=\"text-align: right;\">    1.41956e+09</td><td style=\"text-align: right;\">          0.47323</td><td style=\"text-align: right;\">         1534921505</td></tr>\n",
       "<tr><td>evaluate_config_f1f8017b</td><td>TERMINATED</td><td>130.203.136.143:2091560</td><td style=\"text-align: right;\">50040</td><td style=\"text-align: right;\">32867</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.501305   </td><td style=\"text-align: right;\">    1.2222e+09 </td><td style=\"text-align: right;\">          0.5004 </td><td style=\"text-align: right;\">         1644664680</td></tr>\n",
       "<tr><td>evaluate_config_f1f8017c</td><td>TERMINATED</td><td>130.203.136.143:2091523</td><td style=\"text-align: right;\">47568</td><td style=\"text-align: right;\">31266</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.476539   </td><td style=\"text-align: right;\">    1.40115e+09</td><td style=\"text-align: right;\">          0.47568</td><td style=\"text-align: right;\">         1487261088</td></tr>\n",
       "<tr><td>evaluate_config_f1f8017d</td><td>TERMINATED</td><td>130.203.136.143:2091553</td><td style=\"text-align: right;\">49568</td><td style=\"text-align: right;\">33382</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.496527   </td><td style=\"text-align: right;\">    1.25543e+09</td><td style=\"text-align: right;\">          0.49568</td><td style=\"text-align: right;\">         1654678976</td></tr>\n",
       "<tr><td>evaluate_config_f1f8017e</td><td>TERMINATED</td><td>130.203.136.143:2091582</td><td style=\"text-align: right;\">50281</td><td style=\"text-align: right;\">32089</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.503671   </td><td style=\"text-align: right;\">    1.20541e+09</td><td style=\"text-align: right;\">          0.50281</td><td style=\"text-align: right;\">         1613467009</td></tr>\n",
       "<tr><td>evaluate_config_f1f8017f</td><td>TERMINATED</td><td>130.203.136.143:2091700</td><td style=\"text-align: right;\">54762</td><td style=\"text-align: right;\">33246</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.548556   </td><td style=\"text-align: right;\">    9.14337e+08</td><td style=\"text-align: right;\">          0.54762</td><td style=\"text-align: right;\">         1820617452</td></tr>\n",
       "<tr><td>evaluate_config_f1f80180</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">51780</td><td style=\"text-align: right;\">33283</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.518706   </td><td style=\"text-align: right;\">    1.10357e+09</td><td style=\"text-align: right;\">          0.5178 </td><td style=\"text-align: right;\">         1723393740</td></tr>\n",
       "<tr><td>evaluate_config_f1f80181</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">51990</td><td style=\"text-align: right;\">31913</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.520627   </td><td style=\"text-align: right;\">    1.08966e+09</td><td style=\"text-align: right;\">          0.5199 </td><td style=\"text-align: right;\">         1659156870</td></tr>\n",
       "<tr><td>evaluate_config_f1f80182</td><td>TERMINATED</td><td>130.203.136.143:2091546</td><td style=\"text-align: right;\">52432</td><td style=\"text-align: right;\">33179</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.525188   </td><td style=\"text-align: right;\">    1.06067e+09</td><td style=\"text-align: right;\">          0.52432</td><td style=\"text-align: right;\">         1739641328</td></tr>\n",
       "<tr><td>evaluate_config_f1f80183</td><td>TERMINATED</td><td>130.203.136.143:2091484</td><td style=\"text-align: right;\">54091</td><td style=\"text-align: right;\">30467</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.541853   </td><td style=\"text-align: right;\">    9.55366e+08</td><td style=\"text-align: right;\">          0.54091</td><td style=\"text-align: right;\">         1647990497</td></tr>\n",
       "<tr><td>evaluate_config_f1f80184</td><td>TERMINATED</td><td>130.203.136.143:2091494</td><td style=\"text-align: right;\">59411</td><td style=\"text-align: right;\">32344</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f1f80185</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">53090</td><td style=\"text-align: right;\">33568</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.531825   </td><td style=\"text-align: right;\">    1.01825e+09</td><td style=\"text-align: right;\">          0.5309 </td><td style=\"text-align: right;\">         1782125120</td></tr>\n",
       "<tr><td>evaluate_config_f32cfd28</td><td>TERMINATED</td><td>130.203.136.143:2091716</td><td style=\"text-align: right;\">53247</td><td style=\"text-align: right;\">32521</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.532925   </td><td style=\"text-align: right;\">    1.00825e+09</td><td style=\"text-align: right;\">          0.53247</td><td style=\"text-align: right;\">         1731645687</td></tr>\n",
       "<tr><td>evaluate_config_f32cfd29</td><td>TERMINATED</td><td>130.203.136.143:2091652</td><td style=\"text-align: right;\">63510</td><td style=\"text-align: right;\">31444</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd2a</td><td>TERMINATED</td><td>130.203.136.143:2091523</td><td style=\"text-align: right;\">55340</td><td style=\"text-align: right;\">33105</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.554351   </td><td style=\"text-align: right;\">    8.79716e+08</td><td style=\"text-align: right;\">          0.5534 </td><td style=\"text-align: right;\">         1832030700</td></tr>\n",
       "<tr><td>evaluate_config_f32cfd2b</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\">51841</td><td style=\"text-align: right;\">33881</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.519313   </td><td style=\"text-align: right;\">    1.09952e+09</td><td style=\"text-align: right;\">          0.51841</td><td style=\"text-align: right;\">         1756424921</td></tr>\n",
       "<tr><td>evaluate_config_f32cfd2c</td><td>TERMINATED</td><td>130.203.136.143:2091660</td><td style=\"text-align: right;\">57620</td><td style=\"text-align: right;\">33966</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd2d</td><td>TERMINATED</td><td>130.203.136.143:2091529</td><td style=\"text-align: right;\">67753</td><td style=\"text-align: right;\">98773</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd2e</td><td>TERMINATED</td><td>130.203.136.143:2091560</td><td style=\"text-align: right;\">63405</td><td style=\"text-align: right;\">34639</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">     0.635086   </td><td style=\"text-align: right;\">    4.66344e+08</td><td style=\"text-align: right;\">          0.63405</td><td style=\"text-align: right;\">         2196285795</td></tr>\n",
       "<tr><td>evaluate_config_f32cfd2f</td><td>TERMINATED</td><td>130.203.136.143:2091537</td><td style=\"text-align: right;\">74269</td><td style=\"text-align: right;\">98008</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd30</td><td>TERMINATED</td><td>130.203.136.143:2091553</td><td style=\"text-align: right;\">71877</td><td style=\"text-align: right;\">35224</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd31</td><td>TERMINATED</td><td>130.203.136.143:2091532</td><td style=\"text-align: right;\">71613</td><td style=\"text-align: right;\">97854</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd32</td><td>TERMINATED</td><td>130.203.136.143:2091499</td><td style=\"text-align: right;\">80057</td><td style=\"text-align: right;\">99757</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd33</td><td>TERMINATED</td><td>130.203.136.143:2091518</td><td style=\"text-align: right;\">73258</td><td style=\"text-align: right;\">36012</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd34</td><td>TERMINATED</td><td>130.203.136.143:2091507</td><td style=\"text-align: right;\">74285</td><td style=\"text-align: right;\">98699</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd35</td><td>TERMINATED</td><td>130.203.136.143:2091700</td><td style=\"text-align: right;\">75344</td><td style=\"text-align: right;\">97633</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd36</td><td>TERMINATED</td><td>130.203.136.143:2091474</td><td style=\"text-align: right;\">74442</td><td style=\"text-align: right;\">98775</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd37</td><td>TERMINATED</td><td>130.203.136.143:2091595</td><td style=\"text-align: right;\">78401</td><td style=\"text-align: right;\">36093</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd38</td><td>TERMINATED</td><td>130.203.136.143:2091480</td><td style=\"text-align: right;\">79270</td><td style=\"text-align: right;\">37365</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd39</td><td>TERMINATED</td><td>130.203.136.143:2091546</td><td style=\"text-align: right;\">78600</td><td style=\"text-align: right;\">97722</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd3a</td><td>TERMINATED</td><td>130.203.136.143:2091582</td><td style=\"text-align: right;\">76559</td><td style=\"text-align: right;\">97551</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd3b</td><td>TERMINATED</td><td>130.203.136.143:2091628</td><td style=\"text-align: right;\">75912</td><td style=\"text-align: right;\">36346</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd3c</td><td>TERMINATED</td><td>130.203.136.143:2091491</td><td style=\"text-align: right;\">70334</td><td style=\"text-align: right;\">99233</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd3d</td><td>TERMINATED</td><td>130.203.136.143:2091502</td><td style=\"text-align: right;\">72536</td><td style=\"text-align: right;\">97613</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd3e</td><td>TERMINATED</td><td>130.203.136.143:2091440</td><td style=\"text-align: right;\">77172</td><td style=\"text-align: right;\">36256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "<tr><td>evaluate_config_f32cfd3f</td><td>TERMINATED</td><td>                       </td><td style=\"text-align: right;\">76457</td><td style=\"text-align: right;\">28480</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">               </td><td style=\"text-align: right;\">                 </td><td style=\"text-align: right;\">                   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 10:34:31,049\tINFO tune.py:747 -- Total run time: 13.60 seconds (10.62 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 25280.148097198515, 'evaluation_cost': 0.84841, 'constraint_metric': 8449315190, 'time_this_iter_s': 0.8496530055999756, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 1, 'trial_id': 'f0c474c9', 'experiment_id': '78aa09b8180c42ebba808cfd6754a5b7', 'date': '2022-12-08_10-34-26', 'timestamp': 1670524466, 'time_total_s': 0.8496530055999756, 'pid': 2091499, 'hostname': 'i4-l-qxw5138-01.ad.psu.edu', 'node_ip': '130.203.136.143', 'config': {'x': 84841, 'y': 99590}, 'time_since_restore': 0.8496530055999756, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.005667686462402344, 'experiment_tag': '90_x=84841,y=99590'}\n",
      "{'x': 84841, 'y': 99590}\n"
     ]
    }
   ],
   "source": [
    "# require: pip install flaml[ray]\n",
    "analysis = tune.run(\n",
    "    evaluate_config,  # the function to evaluate a config\n",
    "    config=config_search_space,  # the search space defined\n",
    "    metric=\"score\",\n",
    "    mode=\"min\",  # the optimization mode, \"min\" or \"max\"\n",
    "    num_samples=-1,  # the maximal number of configs to try, -1 means infinite\n",
    "    time_budget_s=10,  # the time budget in seconds\n",
    "    use_ray=True,\n",
    "    resources_per_trial={\"cpu\": 2}  # limit resources allocated per trial\n",
    ")\n",
    "print(analysis.best_trial.last_result)  # the best trial's result\n",
    "print(analysis.best_config)  # the best config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:31] {486} INFO - Using search algorithm type.\n",
      "2022-12-08 10:34:31,988\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:31,991]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "/home/qxw5138/miniconda3/envs/myflaml/lib/python3.8/site-packages/ray/tune/suggest/optuna.py:561: ExperimentalWarning: create_trial is experimental (supported from v2.0.0). The interface can change in the future.\n",
      "  trial = ot.trial.create_trial(\n",
      "2022-12-08 10:34:31,994\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:31,998]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 1 config: {'b': 0.8, 'a': 3.0}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 2 config: {'b': 0.8, 'a': 2.0}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 3 config: {'a': 0.7636074368340785, 'b': 0.0622558480782045}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 4 config: {'a': 0.6273117525770127, 'b': 2.246411647615836}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 5 config: {'a': 0.4935219421795645, 'b': 0.674389936592543}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 6 config: {'a': 0.19608223611202774, 'b': 2.2815921365968763}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 7 config: {'a': 0.1674197281969101, 'b': 0.2650194425220308}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 8 config: {'a': 0.6785062201841192, 'b': 2.8601800385848097}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 9 config: {'a': 0.003908783664635307, 'b': 1.53657679015733}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 10 config: {'a': 0.8044947520355924, 'b': 1.8375782004881644}\n"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "\n",
    "config_search_space = {\n",
    "    \"a\": tune.uniform(lower=0, upper=0.99),\n",
    "    \"b\": tune.uniform(lower=0, upper=3),\n",
    "}\n",
    "\n",
    "def simple_obj(config):\n",
    "    return config[\"a\"] + config[\"b\"]\n",
    "\n",
    "points_to_evaluate = [\n",
    "    {\"b\": .99, \"a\": 3},\n",
    "    {\"b\": .99, \"a\": 2},\n",
    "    {\"b\": .80, \"a\": 3},\n",
    "    {\"b\": .80, \"a\": 2},\n",
    "]\n",
    "evaluated_rewards = [3.99, 2.99]\n",
    "\n",
    "analysis = tune.run(\n",
    "    simple_obj,\n",
    "    config=config_search_space,\n",
    "    mode=\"max\",\n",
    "    points_to_evaluate=points_to_evaluate,\n",
    "    evaluated_rewards=evaluated_rewards,\n",
    "    num_samples=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial scheduling\n",
    "\n",
    "###  An authentic scheduler implemented in FLAML (`scheduler='flaml'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "        \"n_estimators\": tune.lograndint(lower=4, upper=32768),\n",
    "        \"max_leaves\": tune.lograndint(lower=4, upper=32768),\n",
    "        \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set a evaluation function with resource dimension'''\n",
    "def obj_from_resource_attr(resource_attr, X_train, X_test, y_train, y_test, config):\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # in this example sample size is our resource dimension\n",
    "    resource = int(config[resource_attr])\n",
    "    sampled_X_train = X_train.iloc[:resource]\n",
    "    sampled_y_train = y_train[:resource]\n",
    "\n",
    "    # construct a LGBM model from the config\n",
    "    # note that you need to first remove the resource_attr field\n",
    "    # from the config as it is not part of the original search space\n",
    "    model_config = config.copy()\n",
    "    del model_config[resource_attr]\n",
    "    model = LGBMClassifier(**model_config)\n",
    "\n",
    "    model.fit(sampled_X_train, sampled_y_train)\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    test_loss = 1.0 - accuracy_score(y_test, y_test_predict)\n",
    "    return {\"loss\": test_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:32] {486} INFO - Using search algorithm type.\n",
      "2022-12-08 10:34:32,611\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:32,612]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 1 config: {'n_estimators': 9, 'max_leaves': 1364, 'learning_rate': 0.012074374674294664, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 2 config: {'n_estimators': 4048, 'max_leaves': 4, 'learning_rate': 0.07891713267442702, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 3 config: {'n_estimators': 3295, 'max_leaves': 334, 'learning_rate': 0.004638797085780012, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 4 config: {'n_estimators': 21, 'max_leaves': 3668, 'learning_rate': 0.003153366048206083, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 5 config: {'n_estimators': 8, 'max_leaves': 1845, 'learning_rate': 0.7239356970260848, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 6 config: {'n_estimators': 4, 'max_leaves': 379, 'learning_rate': 0.2728556109672425, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from openml_task7592.pkl\n",
      "X_train.shape: (43957, 14), y_train.shape: (43957,),\n",
      "X_test.shape: (4885, 14), y_test.shape: (4885,)\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=334 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=3668 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1845 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=379 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 7 config: {'n_estimators': 948, 'max_leaves': 2573, 'learning_rate': 0.0073847289359894605, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 8 config: {'n_estimators': 15449, 'max_leaves': 2409, 'learning_rate': 0.04196829547317673, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 9 config: {'n_estimators': 13, 'max_leaves': 106, 'learning_rate': 0.10448271169801053, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 10 config: {'n_estimators': 199, 'max_leaves': 185, 'learning_rate': 0.07069097177435173, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:32] {636} INFO - trial 11 config: {'n_estimators': 382, 'max_leaves': 1340, 'learning_rate': 0.06295171682621144, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 12 config: {'n_estimators': 5520, 'max_leaves': 413, 'learning_rate': 0.5308914484951052, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2573 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2409 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=106 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=185 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1340 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 13 config: {'n_estimators': 3503, 'max_leaves': 7, 'learning_rate': 0.0010918443163764437, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 14 config: {'n_estimators': 54, 'max_leaves': 30204, 'learning_rate': 0.004360679238440181, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 15 config: {'n_estimators': 49, 'max_leaves': 23677, 'learning_rate': 0.0010641529214071718, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 16 config: {'n_estimators': 1534, 'max_leaves': 37, 'learning_rate': 0.006371183957388993, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 17 config: {'n_estimators': 29723, 'max_leaves': 9340, 'learning_rate': 0.0027991149620651954, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=413 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=7 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=30204 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=23677 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=37 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=9340 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 18 config: {'n_estimators': 70, 'max_leaves': 34, 'learning_rate': 0.011193803613244459, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 19 config: {'n_estimators': 7696, 'max_leaves': 6735, 'learning_rate': 0.0171883008494944, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 20 config: {'n_estimators': 24847, 'max_leaves': 13127, 'learning_rate': 0.0022947034584149804, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=34 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6735 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=13127 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 21 config: {'n_estimators': 167, 'max_leaves': 15, 'learning_rate': 0.01693528096911362, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 22 config: {'n_estimators': 10494, 'max_leaves': 746, 'learning_rate': 0.02385060065057464, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 23 config: {'n_estimators': 1934, 'max_leaves': 92, 'learning_rate': 0.0019836952742413195, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:33] {636} INFO - trial 24 config: {'n_estimators': 26875, 'max_leaves': 4, 'learning_rate': 0.0017068232742646725, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=15 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=746 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=92 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 25 config: {'n_estimators': 281, 'max_leaves': 688, 'learning_rate': 0.016552541070398875, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 26 config: {'n_estimators': 1778, 'max_leaves': 81, 'learning_rate': 0.03175152998663522, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 27 config: {'n_estimators': 688, 'max_leaves': 7, 'learning_rate': 0.001542522148885942, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 28 config: {'n_estimators': 3797, 'max_leaves': 791, 'learning_rate': 0.007264101943760989, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 29 config: {'n_estimators': 630, 'max_leaves': 177, 'learning_rate': 0.14424489344601685, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 30 config: {'n_estimators': 1174, 'max_leaves': 34, 'learning_rate': 0.03769197598985876, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=688 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=81 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=7 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=791 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=177 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=34 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 31 config: {'n_estimators': 3554, 'max_leaves': 12, 'learning_rate': 0.005723824290933867, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 32 config: {'n_estimators': 540, 'max_leaves': 970, 'learning_rate': 0.2234297355835903, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 33 config: {'n_estimators': 988, 'max_leaves': 207, 'learning_rate': 0.16430394221281475, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 34 config: {'n_estimators': 2835, 'max_leaves': 23, 'learning_rate': 0.004113773682186522, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 35 config: {'n_estimators': 480, 'max_leaves': 13, 'learning_rate': 0.3286382866833518, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 36 config: {'n_estimators': 5897, 'max_leaves': 254, 'learning_rate': 0.9259903592746499, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=12 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=970 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=207 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=23 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=13 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=254 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 37 config: {'n_estimators': 2526, 'max_leaves': 1332, 'learning_rate': 0.19284215718355668, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 38 config: {'n_estimators': 12671, 'max_leaves': 4, 'learning_rate': 0.0018253161972409146, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 39 config: {'n_estimators': 199, 'max_leaves': 439, 'learning_rate': 0.011304611020148745, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 40 config: {'n_estimators': 103, 'max_leaves': 88, 'learning_rate': 0.003539923584406662, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1332 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=439 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=88 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 41 config: {'n_estimators': 18778, 'max_leaves': 3702, 'learning_rate': 0.027602122008566297, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 42 config: {'n_estimators': 270, 'max_leaves': 62, 'learning_rate': 0.05507878569900855, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 43 config: {'n_estimators': 918, 'max_leaves': 564, 'learning_rate': 0.011450265080289271, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 44 config: {'n_estimators': 1817, 'max_leaves': 313, 'learning_rate': 0.008383218707494321, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=3702 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=62 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=564 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:34] {636} INFO - trial 45 config: {'n_estimators': 663, 'max_leaves': 178, 'learning_rate': 0.10140492706094423, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 46 config: {'n_estimators': 4785, 'max_leaves': 137, 'learning_rate': 0.0013296286140323664, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 47 config: {'n_estimators': 1109, 'max_leaves': 6, 'learning_rate': 0.09122606727247846, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 48 config: {'n_estimators': 778, 'max_leaves': 54, 'learning_rate': 0.04335186975552169, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 49 config: {'n_estimators': 3992, 'max_leaves': 14, 'learning_rate': 0.005902481455333757, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=313 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=178 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=137 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=54 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=14 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 50 config: {'n_estimators': 1332, 'max_leaves': 2032, 'learning_rate': 0.005114224731719409, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 51 config: {'n_estimators': 584, 'max_leaves': 39, 'learning_rate': 0.4763308566342319, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 52 config: {'n_estimators': 394, 'max_leaves': 1349, 'learning_rate': 0.15573324989369033, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 53 config: {'n_estimators': 1137, 'max_leaves': 22, 'learning_rate': 0.30657057312024644, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 54 config: {'n_estimators': 2881, 'max_leaves': 19, 'learning_rate': 0.003592649684962099, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 55 config: {'n_estimators': 424, 'max_leaves': 11, 'learning_rate': 0.2826008095299387, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2032 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=39 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1349 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=22 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=19 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=11 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 56 config: {'n_estimators': 7415, 'max_leaves': 9, 'learning_rate': 0.41144344654042997, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 57 config: {'n_estimators': 5525, 'max_leaves': 278, 'learning_rate': 0.9784854203265464, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 58 config: {'n_estimators': 2407, 'max_leaves': 257, 'learning_rate': 0.7924698570749448, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 59 config: {'n_estimators': 2586, 'max_leaves': 1133, 'learning_rate': 0.668218029499765, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=9 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=278 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=257 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1133 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 60 config: {'n_estimators': 5525, 'max_leaves': 443, 'learning_rate': 0.16378510208856667, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 61 config: {'n_estimators': 9220, 'max_leaves': 4, 'learning_rate': 0.36062762370731777, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 62 config: {'n_estimators': 13127, 'max_leaves': 2778, 'learning_rate': 0.00222017541525974, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=443 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2778 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 63 config: {'n_estimators': 30, 'max_leaves': 1436, 'learning_rate': 0.0026028422881987055, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 64 config: {'n_estimators': 201, 'max_leaves': 475, 'learning_rate': 0.011324773041374265, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:35] {636} INFO - trial 65 config: {'n_estimators': 18941, 'max_leaves': 4223, 'learning_rate': 0.025865362181268114, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 66 config: {'n_estimators': 146, 'max_leaves': 4821, 'learning_rate': 0.0034154784329390535, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1436 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=475 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4223 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 67 config: {'n_estimators': 101, 'max_leaves': 7778, 'learning_rate': 0.052572695809586165, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 68 config: {'n_estimators': 275, 'max_leaves': 2819, 'learning_rate': 0.07081648034307593, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 69 config: {'n_estimators': 110, 'max_leaves': 65, 'learning_rate': 0.012107530127793, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 70 config: {'n_estimators': 73, 'max_leaves': 587, 'learning_rate': 0.008061889806716868, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 71 config: {'n_estimators': 12, 'max_leaves': 134, 'learning_rate': 0.009322413215086983, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 72 config: {'n_estimators': 216, 'max_leaves': 122, 'learning_rate': 0.020857393898663115, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 73 config: {'n_estimators': 312, 'max_leaves': 186, 'learning_rate': 0.014177007227519758, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4821 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=7778 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2819 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=65 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=587 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=134 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=122 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 74 config: {'n_estimators': 1473, 'max_leaves': 20, 'learning_rate': 0.5282804782911118, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 75 config: {'n_estimators': 364, 'max_leaves': 23, 'learning_rate': 0.005958043358657801, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 76 config: {'n_estimators': 446, 'max_leaves': 43, 'learning_rate': 0.004775130134948399, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 77 config: {'n_estimators': 1219, 'max_leaves': 10, 'learning_rate': 0.3779589262687463, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 78 config: {'n_estimators': 1401, 'max_leaves': 10, 'learning_rate': 0.28038134184632646, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 79 config: {'n_estimators': 6628, 'max_leaves': 6, 'learning_rate': 0.47891449595724667, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=186 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=20 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=23 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=43 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=10 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=10 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 80 config: {'n_estimators': 8308, 'max_leaves': 30, 'learning_rate': 0.447042664449934, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 81 config: {'n_estimators': 2232, 'max_leaves': 302, 'learning_rate': 0.9413848630987627, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=30 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=302 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 82 config: {'n_estimators': 3008, 'max_leaves': 19, 'learning_rate': 0.6617776883804151, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 83 config: {'n_estimators': 4676, 'max_leaves': 831, 'learning_rate': 0.729098750582734, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 84 config: {'n_estimators': 9073, 'max_leaves': 7, 'learning_rate': 0.6999001097029152, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:36] {636} INFO - trial 85 config: {'n_estimators': 12097, 'max_leaves': 365, 'learning_rate': 0.23076401336141383, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=19 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=831 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=7 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=365 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 86 config: {'n_estimators': 6076, 'max_leaves': 15976, 'learning_rate': 0.3771793556881001, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 87 config: {'n_estimators': 4, 'max_leaves': 1072, 'learning_rate': 0.8133197504691136, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 88 config: {'n_estimators': 15851, 'max_leaves': 1532, 'learning_rate': 0.0026562675065994125, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=15976 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1072 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1532 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 89 config: {'n_estimators': 10539, 'max_leaves': 469, 'learning_rate': 0.0020422601196322786, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 90 config: {'n_estimators': 35, 'max_leaves': 2692, 'learning_rate': 0.0025090165260535893, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 91 config: {'n_estimators': 31, 'max_leaves': 3981, 'learning_rate': 0.0030253535104938114, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 92 config: {'n_estimators': 18188, 'max_leaves': 4341, 'learning_rate': 0.0013359200355607022, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=469 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2692 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=3981 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4341 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 93 config: {'n_estimators': 115, 'max_leaves': 6603, 'learning_rate': 0.021098867037444867, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 94 config: {'n_estimators': 153, 'max_leaves': 5730, 'learning_rate': 0.12902679796448774, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 95 config: {'n_estimators': 109, 'max_leaves': 2582, 'learning_rate': 0.07080984320878844, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 96 config: {'n_estimators': 86, 'max_leaves': 8478, 'learning_rate': 0.038435592425912324, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 97 config: {'n_estimators': 59, 'max_leaves': 5035, 'learning_rate': 0.007798167307370056, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 98 config: {'n_estimators': 16, 'max_leaves': 10730, 'learning_rate': 0.009894614910177672, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 99 config: {'n_estimators': 131, 'max_leaves': 127, 'learning_rate': 0.015268661873095089, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 100 config: {'n_estimators': 211, 'max_leaves': 57, 'learning_rate': 0.019426510247163038, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6603 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5730 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2582 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=8478 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5035 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=10730 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=127 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=57 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 101 config: {'n_estimators': 282, 'max_leaves': 127, 'learning_rate': 0.013917381016486404, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 102 config: {'n_estimators': 82, 'max_leaves': 213, 'learning_rate': 0.03215891352653318, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 103 config: {'n_estimators': 298, 'max_leaves': 73, 'learning_rate': 0.008842202108353148, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 104 config: {'n_estimators': 232, 'max_leaves': 102, 'learning_rate': 0.0066980336616668445, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 105 config: {'n_estimators': 392, 'max_leaves': 45, 'learning_rate': 0.004550516087330994, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 106 config: {'n_estimators': 8, 'max_leaves': 28, 'learning_rate': 0.013639473182140692, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 107 config: {'n_estimators': 1308, 'max_leaves': 71, 'learning_rate': 0.0050708287994836255, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=127 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=213 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=73 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=102 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=45 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=28 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=71 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 108 config: {'n_estimators': 879, 'max_leaves': 41, 'learning_rate': 0.006461510915459239, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:37] {636} INFO - trial 109 config: {'n_estimators': 1642, 'max_leaves': 5, 'learning_rate': 0.009620046518012432, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 110 config: {'n_estimators': 759, 'max_leaves': 10, 'learning_rate': 0.5582224182204506, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 111 config: {'n_estimators': 528, 'max_leaves': 16, 'learning_rate': 0.47482418104411844, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 112 config: {'n_estimators': 2055, 'max_leaves': 28, 'learning_rate': 0.5549732420567667, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 113 config: {'n_estimators': 2169, 'max_leaves': 19, 'learning_rate': 0.5973102809665856, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=41 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=10 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=16 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=28 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=19 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 114 config: {'n_estimators': 4191, 'max_leaves': 29, 'learning_rate': 0.45841248810094576, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 115 config: {'n_estimators': 3067, 'max_leaves': 8, 'learning_rate': 0.6950226593911881, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 116 config: {'n_estimators': 6417, 'max_leaves': 6, 'learning_rate': 0.8645386073451609, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 117 config: {'n_estimators': 8474, 'max_leaves': 5, 'learning_rate': 0.24803611166147882, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=29 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=8 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 118 config: {'n_estimators': 4634, 'max_leaves': 321, 'learning_rate': 0.34666942819830854, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 119 config: {'n_estimators': 6910, 'max_leaves': 1005, 'learning_rate': 0.38785398836958446, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 120 config: {'n_estimators': 3199, 'max_leaves': 6, 'learning_rate': 0.75166286138699, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 121 config: {'n_estimators': 11147, 'max_leaves': 23703, 'learning_rate': 0.6564634512210011, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=321 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1005 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=23703 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 122 config: {'n_estimators': 15171, 'max_leaves': 851, 'learning_rate': 0.9749384452988281, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 123 config: {'n_estimators': 9592, 'max_leaves': 382, 'learning_rate': 0.8134403889913343, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 124 config: {'n_estimators': 11708, 'max_leaves': 1687, 'learning_rate': 0.0022641547592865186, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=851 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=382 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:38] {636} INFO - trial 125 config: {'n_estimators': 29155, 'max_leaves': 653, 'learning_rate': 0.0009770550865846573, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1687 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=653 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 126 config: {'n_estimators': 22500, 'max_leaves': 2035, 'learning_rate': 0.00153183685334048, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 127 config: {'n_estimators': 29, 'max_leaves': 16840, 'learning_rate': 0.001964529732491673, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 128 config: {'n_estimators': 4, 'max_leaves': 3882, 'learning_rate': 0.001284644752246744, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2035 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=16840 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=3882 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 129 config: {'n_estimators': 22968, 'max_leaves': 6645, 'learning_rate': 0.21111485149822817, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 130 config: {'n_estimators': 15982, 'max_leaves': 5815, 'learning_rate': 0.12860984198671235, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6645 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5815 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 131 config: {'n_estimators': 44, 'max_leaves': 2998, 'learning_rate': 0.0030200234940365236, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 132 config: {'n_estimators': 39, 'max_leaves': 9202, 'learning_rate': 0.0811019584239895, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 133 config: {'n_estimators': 60, 'max_leaves': 5666, 'learning_rate': 0.004007829946037066, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 134 config: {'n_estimators': 20, 'max_leaves': 11870, 'learning_rate': 0.0027344474841943514, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 135 config: {'n_estimators': 153, 'max_leaves': 3318, 'learning_rate': 0.045335813727047994, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 136 config: {'n_estimators': 126, 'max_leaves': 4537, 'learning_rate': 0.0025274530893320225, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 137 config: {'n_estimators': 82, 'max_leaves': 2232, 'learning_rate': 0.035753038394389515, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 138 config: {'n_estimators': 167, 'max_leaves': 8771, 'learning_rate': 0.02146693016567059, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2998 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=9202 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5666 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=11870 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=3318 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4537 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=2232 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 139 config: {'n_estimators': 85, 'max_leaves': 5015, 'learning_rate': 0.01836949217228744, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:39] {636} INFO - trial 140 config: {'n_estimators': 55, 'max_leaves': 221, 'learning_rate': 0.02957686754386779, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 141 config: {'n_estimators': 110, 'max_leaves': 148, 'learning_rate': 0.015375135067511985, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 142 config: {'n_estimators': 230, 'max_leaves': 77, 'learning_rate': 0.009844835271589789, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 143 config: {'n_estimators': 238, 'max_leaves': 93, 'learning_rate': 0.0073902998306187104, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 144 config: {'n_estimators': 138, 'max_leaves': 52, 'learning_rate': 0.019378392365890264, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 145 config: {'n_estimators': 9, 'max_leaves': 113, 'learning_rate': 0.013815871105396057, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=8771 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5015 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=221 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=148 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=77 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=93 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=52 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 146 config: {'n_estimators': 181, 'max_leaves': 11664, 'learning_rate': 0.0052156503794167225, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 147 config: {'n_estimators': 343, 'max_leaves': 43, 'learning_rate': 0.006936450889135225, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 148 config: {'n_estimators': 6, 'max_leaves': 56, 'learning_rate': 0.012493396501243509, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 149 config: {'n_estimators': 1648, 'max_leaves': 149, 'learning_rate': 0.010012277560594009, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 150 config: {'n_estimators': 821, 'max_leaves': 35, 'learning_rate': 0.009159978735267736, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 151 config: {'n_estimators': 526, 'max_leaves': 16, 'learning_rate': 0.008275920668378171, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=113 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=11664 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=43 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=56 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=149 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=35 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=16 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 152 config: {'n_estimators': 816, 'max_leaves': 74, 'learning_rate': 0.006482067494868841, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 153 config: {'n_estimators': 1044, 'max_leaves': 99, 'learning_rate': 0.004397862236362672, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 154 config: {'n_estimators': 2148, 'max_leaves': 28, 'learning_rate': 0.005388498250955563, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 155 config: {'n_estimators': 464, 'max_leaves': 30, 'learning_rate': 0.4883386107761221, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 156 config: {'n_estimators': 637, 'max_leaves': 27, 'learning_rate': 0.6009989664388208, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 157 config: {'n_estimators': 1860, 'max_leaves': 15, 'learning_rate': 0.5409959333947137, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 158 config: {'n_estimators': 3492, 'max_leaves': 8, 'learning_rate': 0.5630385099257847, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=74 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=99 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=28 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=30 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=27 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=15 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=8 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 159 config: {'n_estimators': 4170, 'max_leaves': 5, 'learning_rate': 0.006205352481265516, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 160 config: {'n_estimators': 1349, 'max_leaves': 5, 'learning_rate': 0.004656096935804429, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 161 config: {'n_estimators': 2490, 'max_leaves': 12, 'learning_rate': 0.3501975907340783, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 162 config: {'n_estimators': 5056, 'max_leaves': 47, 'learning_rate': 0.0037448469596170035, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 163 config: {'n_estimators': 7706, 'max_leaves': 8, 'learning_rate': 0.4262802620058607, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=12 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=47 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=8 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 164 config: {'n_estimators': 3220, 'max_leaves': 5, 'learning_rate': 0.40615361388208804, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 165 config: {'n_estimators': 6878, 'max_leaves': 4, 'learning_rate': 0.6151997433753406, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 166 config: {'n_estimators': 4306, 'max_leaves': 17, 'learning_rate': 0.8844710716095144, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 167 config: {'n_estimators': 2898, 'max_leaves': 362, 'learning_rate': 0.7565061224429761, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:40] {636} INFO - trial 168 config: {'n_estimators': 10255, 'max_leaves': 22, 'learning_rate': 0.6688521012095087, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=5 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=17 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=362 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=22 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 169 config: {'n_estimators': 12928, 'max_leaves': 6, 'learning_rate': 0.8920365358541912, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 170 config: {'n_estimators': 5754, 'max_leaves': 830, 'learning_rate': 0.3053831719008097, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 171 config: {'n_estimators': 10672, 'max_leaves': 596, 'learning_rate': 0.9904703098611144, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=830 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=596 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 172 config: {'n_estimators': 28948, 'max_leaves': 644, 'learning_rate': 0.0010021800865329749, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 173 config: {'n_estimators': 22018, 'max_leaves': 27846, 'learning_rate': 0.001520413461764809, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=644 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=27846 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 174 config: {'n_estimators': 31314, 'max_leaves': 1677, 'learning_rate': 0.0013007529098430645, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 175 config: {'n_estimators': 23789, 'max_leaves': 19038, 'learning_rate': 0.24245421132864492, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1677 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=19038 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 176 config: {'n_estimators': 8483, 'max_leaves': 1016, 'learning_rate': 0.0016595298283853261, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:41] {636} INFO - trial 177 config: {'n_estimators': 15895, 'max_leaves': 1863, 'learning_rate': 0.001292415677672366, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 178 config: {'n_estimators': 19283, 'max_leaves': 1124, 'learning_rate': 0.21009062955579472, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1016 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1863 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1124 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 179 config: {'n_estimators': 15077, 'max_leaves': 4, 'learning_rate': 0.001116720666995303, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 180 config: {'n_estimators': 25744, 'max_leaves': 3066, 'learning_rate': 0.13462790568725672, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=3066 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 181 config: {'n_estimators': 14379, 'max_leaves': 21532, 'learning_rate': 0.11249756313060533, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 182 config: {'n_estimators': 43, 'max_leaves': 6487, 'learning_rate': 0.17296950883370182, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 183 config: {'n_estimators': 22, 'max_leaves': 7280, 'learning_rate': 0.258852421157622, 'sample_size': 1000}\n",
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 184 config: {'n_estimators': 45, 'max_leaves': 3571, 'learning_rate': 0.08732512289838663, 'sample_size': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=21532 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=6487 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=7280 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=3571 will be ignored. Current value: num_leaves=31\n",
      "best result w/ flaml scheduler (in 10s):  {'loss': 0.2393039918116684, 'training_iteration': 0, 'config': {'n_estimators': 9, 'max_leaves': 1364, 'learning_rate': 0.012074374674294664, 'sample_size': 1000}, 'config/n_estimators': 9, 'config/max_leaves': 1364, 'config/learning_rate': 0.012074374674294664, 'config/sample_size': 1000, 'experiment_tag': 'exp', 'time_total_s': 0.025426626205444336}\n"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "from functools import partial\n",
    "from flaml.data import load_openml_task\n",
    "    \n",
    "X_train, X_test, y_train, y_test = load_openml_task(task_id=7592, data_dir=\"\")\n",
    "max_resource = len(y_train)\n",
    "resource_attr = \"sample_size\"\n",
    "min_resource = 1000\n",
    "analysis = tune.run(\n",
    "    partial(\n",
    "        obj_from_resource_attr, resource_attr, X_train, X_test, y_train, y_test\n",
    "    ),\n",
    "    config=search_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    resource_attr=resource_attr,\n",
    "    scheduler=\"flaml\",\n",
    "    max_resource=max_resource,\n",
    "    min_resource=min_resource,\n",
    "    reduction_factor=2,\n",
    "    time_budget_s=10,\n",
    "    num_samples=-1,\n",
    ")\n",
    "print(\"best result w/ flaml scheduler (in 10s): \", analysis.best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ASHA scheduler (`scheduler='asha'`) or a custom scheduler of the  [`TrialScheduler`](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-schedulers) class from `ray.tune`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_w_intermediate_report(\n",
    "        resource_attr,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        min_resource,\n",
    "        max_resource,\n",
    "        config,\n",
    "    ):\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # a customized schedule to perform the evaluation\n",
    "    eval_schedule = [res for res in range(min_resource, max_resource, 5000)] + [\n",
    "        max_resource\n",
    "    ]\n",
    "    for resource in eval_schedule:\n",
    "        sampled_X_train = X_train.iloc[:resource]\n",
    "        sampled_y_train = y_train[:resource]\n",
    "\n",
    "        # construct a LGBM model from the config\n",
    "        model = LGBMClassifier(**config)\n",
    "\n",
    "        model.fit(sampled_X_train, sampled_y_train)\n",
    "        y_test_predict = model.predict(X_test)\n",
    "        test_loss = 1.0 - accuracy_score(y_test, y_test_predict)\n",
    "        # need to report the resource attribute used and the corresponding intermediate results\n",
    "        try:\n",
    "            tune.report(sample_size=resource, loss=test_loss)\n",
    "        except StopIteration:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:42] {486} INFO - Using search algorithm type.\n",
      "2022-12-08 10:34:42,987\tWARNING optuna.py:297 -- You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "\u001b[32m[I 2022-12-08 10:34:42,988]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "[flaml.tune.tune: 12-08 10:34:42] {636} INFO - trial 1 config: {'n_estimators': 9, 'max_leaves': 1364, 'learning_rate': 0.012074374674294664}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from openml_task7592.pkl\n",
      "X_train.shape: (43957, 14), y_train.shape: (43957,),\n",
      "X_test.shape: (4885, 14), y_test.shape: (4885,)\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=1364 will be ignored. Current value: num_leaves=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-08 10:34:43] {636} INFO - trial 2 config: {'n_estimators': 4048, 'max_leaves': 4, 'learning_rate': 0.07891713267442702}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "[LightGBM] [Warning] num_leaves is set=31, max_leaves=4 will be ignored. Current value: num_leaves=31\n",
      "best result w/ asha scheduler (in 10s):  {'sample_size': 43957, 'loss': 0.13920163766632554, 'training_iteration': 9, 'config': {'n_estimators': 4048, 'max_leaves': 4, 'learning_rate': 0.07891713267442702}, 'config/n_estimators': 4048, 'config/max_leaves': 4, 'config/learning_rate': 0.07891713267442702, 'experiment_tag': 'exp', 'time_total_s': 65.32875609397888}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_openml_task(task_id=7592, data_dir=\"\")\n",
    "resource_attr = \"sample_size\"\n",
    "min_resource = 1000\n",
    "max_resource = len(y_train)\n",
    "analysis = tune.run(\n",
    "    partial(\n",
    "        obj_w_intermediate_report,\n",
    "        resource_attr,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        min_resource,\n",
    "        max_resource,\n",
    "    ),\n",
    "    config=search_space,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    resource_attr=resource_attr,\n",
    "    scheduler=\"asha\",\n",
    "    max_resource=max_resource,\n",
    "    min_resource=min_resource,\n",
    "    reduction_factor=2,\n",
    "    time_budget_s=10,\n",
    "    num_samples=-1,\n",
    ")\n",
    "print(\"best result w/ asha scheduler (in 10s): \", analysis.best_result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61dacc07bdd67a6bd133ad154042152699ffea5858044733f84b495e7a4b9e6a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myflaml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
